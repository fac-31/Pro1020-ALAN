{
  "documents": [
    "How AI generated code compounds technical debt\n\nAI Coding Tools Are Driving Code Duplication and Technical Debt, GitClear Report Warns\nGitClear\u2019s 2025 AI Copilot Code Quality report reveals a troubling trend: as AI coding assistants like Cursor and GitHub Copilot become ubiquitous, code duplication is surging and long-term code quality is declining. \u201cI don\u2019t think I have ever seen so much technical debt being created in such a short period of time during my 35-year career in technology.\u201d\n\u2014 Kin Lane, API Evangelist\n\n\nKey Findings from 211 Million Lines of Code (2020\u20132024)\nAnalyzing anonymized private repos and 25 major open-source projects, GitClear found:\n\n8x increase in duplicated code blocks (5+ lines) in 2024 vs. prior years \u2014 10x higher than two years ago. 46% of code changes were new lines; copy-pasted lines outnumbered moved lines. \"Moved\" lines \u2014 GitClear\u2019s metric for refactoring and reuse \u2014 are in steep decline.",
    "\"Moved\" lines \u2014 GitClear\u2019s metric for refactoring and reuse \u2014 are in steep decline. \u201cRefactored systems, and moved code in particular, are the signature of code reuse.\u201d\n\u2014 Bill Harding, CEO of GitClear & Amplenote\n\nThis shift signals developers are less likely to consolidate logic into reusable modules \u2014 a direct violation of the DRY (Don\u2019t Repeat Yourself) principle. More Code \u2260 Better Code\nDespite perceived productivity gains, AI-generated code is creating hidden maintenance burdens:\n\nThe Harness State of Software Delivery 2025 report found developers spend more time debugging AI code and fixing security vulnerabilities. Google\u2019s 2024 DORA report: +25% AI usage speeds reviews and docs, but cuts delivery stability by 7.2%. \u201cIf developer productivity continues being measured by commit count or lines added, AI-driven maintainability decay will proliferate.\u201d\n\u2014 Bill Harding\n\n\nThe Real Cost of Code Cloning\nDuplicated code isn\u2019t just messy \u2014 it\u2019s expensive:",
    "\u201cIf developer productivity continues being measured by commit count or lines added, AI-driven maintainability decay will proliferate.\u201d\n\u2014 Bill Harding\n\n\nThe Real Cost of Code Cloning\nDuplicated code isn\u2019t just messy \u2014 it\u2019s expensive: The Real Cost of Code Cloning\nDuplicated code isn\u2019t just messy \u2014 it\u2019s expensive:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpactConsequenceStorageHigher cloud costsBugsDefects multiply across clonesTestingComplex, error-proneMaintenanceChanges must be made in multiple places\nA 2023 study from Central China Normal University confirms: code clones significantly increase defect rates. The Human Role in AI-Assisted Development\nAI excels at rapid generation, but its limited context window means it often fails to see the full codebase. Humans remain essential for:\n\nRefactoring repetitive logic\nConsolidating modules\nReusing microservices\nEnforcing architectural consistency\n\n\n\u201cThere is a lot of utility that AI provides, but the data from this year affirms why long-term-oriented devs might eye their \u2018tab\u2019 key with a faint sense of foreboding.\u201d\n\u2014 Bill Harding\n\n\nHow to Fight Back",
    "Humans remain essential for:\n\nRefactoring repetitive logic\nConsolidating modules\nReusing microservices\nEnforcing architectural consistency\n\n\n\u201cThere is a lot of utility that AI provides, but the data from this year affirms why long-term-oriented devs might eye their \u2018tab\u2019 key with a faint sense of foreboding.\u201d\n\u2014 Bill Harding\n\n\nHow to Fight Back How to Fight Back\n\nMeasure reuse, not just output (track \"moved\" lines, not LOC). Use AI to refactor \u2014 tools like Cursor can enforce consistency. Set explicit rules: \u201cNever duplicate \u2014 always modify existing code.\u201d\nAudit regularly for clones and redundant systems. Bottom Line\nAI coding tools are powerful accelerators \u2014 but without discipline, they risk turning software into infinite, unmaintainable sprawl. The future of sustainable development depends on using AI to refactor, not just generate.",
    "Monorepo vs Multi-repo vs Git submodule vs Git Subtree\n\nMonorepo vs Multi-repo vs Git submodule vs Git Subtree: A Complete Guide for Developers\nSubodh Shetty\nSubodh Shetty\n\nFollow\n5 min read\n\u00b7\n2 days ago\n44\n\n\n1\n\n\n\nPress enter or click to view image in full size\n\nMonoRepo vs MultiRepo vs Git Submodule vs Git Subtree\nAs projects grow, one question quietly becomes a big one:\n\n\u201cWhere should all this code live?\u201d\n\nSome teams prefer to keep everything in one place. Others prefer to split things into smaller, separate repositories. A few connect repositories using Git submodules or subtrees. In this guide, we\u2019ll walk through each strategy in plain English. You\u2019ll learn what each one does, when it helps, and what you trade off when you choose it. 1. Monorepo\nA monorepo means one big repository for all your projects. That includes backend, frontend, libraries, and sometimes even deployment scripts. Example\nImagine you are building an e-commerce system. Your structure might look like this:",
    "Your structure might look like this: Example\nImagine you are building an e-commerce system. Your structure might look like this:\n\n\nFolder structure\nDevelopers from multiple teams work inside this same repository.",
    "Your structure might look like this:\n\n\nFolder structure\nDevelopers from multiple teams work inside this same repository. Folder structure\nDevelopers from multiple teams work inside this same repository. Why teams use monorepos\nMonorepos make it easier to change related code together. Suppose your frontend and backend share a common API. You can update both sides in one pull request, and both will stay in sync. You also have one build pipeline, one testing setup, and one linting configuration. That makes consistency simple. What to keep in mind\nWhen a monorepo grows, builds can become slow if you do not optimize them. You also need a clear sense of code ownership. Without that, people can make changes in places they shouldn\u2019t. CODEOWNERS files help define who reviews which parts of the repository. When it fits\nMonorepos work best for small to medium teams where services are closely connected. They make shared tooling and cross-team collaboration easier. 2. Multi-repo (or Polyrepo)\nA multi-repo setup means each project has its own repository. It is the most common approach in the industry.",
    "It is the most common approach in the industry. Example\nYour company might have following Git repositories:\n\nPress enter or click to view image in full size\n\nEach repository has its own CI/CD setup, its own issue tracker, and its own release process.",
    "Example\nYour company might have following Git repositories:\n\nPress enter or click to view image in full size\n\nEach repository has its own CI/CD setup, its own issue tracker, and its own release process. Each repository has its own CI/CD setup, its own issue tracker, and its own release process. Why teams use multi-repos\nMulti-repos give teams freedom. Each team controls its own repo, deployment, and tools. The frontend team can use React while the backend uses Spring Boot or Node.js. Access control also becomes easier. Only members who work on a service need access to its repository. Real-world example\nThink of a company that has separate payments, catalog, and shipping teams. Each one builds its own service. They deploy independently and have different release schedules. That independence makes multi-repo a good fit. What to keep in mind\nWhen multiple repos depend on a shared library, keeping them consistent can be hard. If you fix a bug in that library, you need to update each repo using it. Cross-repo changes take longer and require coordination. When it fits\nMulti-repos are ideal for larger teams where services are mostly independent and ownership boundaries are clear.",
    "When it fits\nMulti-repos are ideal for larger teams where services are mostly independent and ownership boundaries are clear. 3. Git Submodules\nA Git submodule adds a reference to another repository at a specific commit. It does not copy the repository\u2019s content into your main repo\u2019s history. Instead, it just stores a pointer to the commit in the external repo. Example\nYou have:\n\nmain-app repository\nshared-logging repository\nYou add it like this:\n\ngit submodule add https://github.com/org/shared-logging.git libs/shared-logging\nYour main repository now contains the shared-logging code inside the libs/shared-logging folder. Updating a submodule\nIf the submodule has new commits, you need to pull them manually:\n\ncd libs/shared-logging\ngit checkout main\ngit pull origin main\ncd ../..\ngit add libs/shared-logging\ngit commit -m \"Update submodule to latest commit\"\nThat updates your main repo to point to the newer commit of the submodule. Cloning your main repo\nWhen someone clones your main repo, they have to run the following to download that external code. :",
    ": Cloning your main repo\nWhen someone clones your main repo, they have to run the following to download that external code. :\n\ngit submodule update --init --recursive\nOptional tracking\nIf you want your submodule to follow a specific branch:\n\ngit submodule add -b main https://github.com/org/shared-logging.git libs/shared-logging\nThen fetch updates using:",
    ":\n\ngit submodule update --init --recursive\nOptional tracking\nIf you want your submodule to follow a specific branch:\n\ngit submodule add -b main https://github.com/org/shared-logging.git libs/shared-logging\nThen fetch updates using: git submodule add -b main https://github.com/org/shared-logging.git libs/shared-logging\nThen fetch updates using:\n\ngit submodule update --remote\nWhy it exists\nSubmodules are designed for situations where you want to reuse code that lives in another repository. It\u2019s like having a dependency that you control. You decide when to upgrade to the next version by updating the pointer. What to keep in mind\nSubmodules need discipline. Submodules don\u2019t auto-update. You must manually pull updates. You have to remember to initialize them and update them when cloning. CI/CD pipelines must also handle them properly. 4. Git Subtree\nWhile submodules keep a pointer to another repository, a Git subtree actually copies the other repository\u2019s content into your own. It is like pulling another repository\u2019s code into a subfolder. Example\nYou can add a subtree like this:",
    "Example\nYou can add a subtree like this: Example\nYou can add a subtree like this:\n\ngit subtree add --prefix=libs/shared-logging https://github.com/org/shared-logging.git main --squash\nThis command copies all code from the shared-logging repository into the folder libs/shared-logging and adds it as part of your project. When the shared library gets updates, you can pull those updates later:\n\ngit subtree pull --prefix=libs/shared-logging https://github.com/org/shared-logging.git main --squash\nIf you make local changes and want to send them back upstream:",
    "When the shared library gets updates, you can pull those updates later:\n\ngit subtree pull --prefix=libs/shared-logging https://github.com/org/shared-logging.git main --squash\nIf you make local changes and want to send them back upstream: git subtree push --prefix=libs/shared-logging https://github.com/org/shared-logging.git main\nHow it works\nA subtree merges the other repository\u2019s history into yours. The files become a permanent part of your repository. Your project can now build without needing the external repository. Advantages\nNo extra setup for developers. Works with normal git clone. No special CI handling required. All code is available locally. What to keep in mind\nYour repository size grows because it includes the subtree\u2019s history. Managing many subtrees can become complex. You need to remember which folder maps to which external repository. When to use\nSubtrees are a good fit when you want to vendor another repository\u2019s code. That means you want to copy it inside your repo and update it occasionally. It\u2019s common for internal forks or open-source libraries that you customize. In Conclusion,",
    "In Conclusion, That means you want to copy it inside your repo and update it occasionally. It\u2019s common for internal forks or open-source libraries that you customize. In Conclusion,\nRepository structure influences how teams work, not just how code is stored. It affects how you test, deploy, and collaborate.",
    "It affects how you test, deploy, and collaborate. Before deciding, ask:\n\nHow independent are your services? How often do they change together? How do you want to manage shared code? A monorepo offers simplicity and consistency. A multi-repo offers independence and flexibility. A submodule links projects that evolve separately. A subtree embeds projects that need to live together. Each one solves a different kind of problem. Once you understand their behavior, choosing the right one becomes straightforward.",
    "Visualize FastAPI endpoints with FastAPI-Voyager\n\nLoading\u2026\nFastAPI Voyager\n{{ state.version }}\nscroll to zoom in/out\ndouble click node to view details. shift + click to see schema's dependencies without unrelated nodes. {{ tag.name }}\n{{ tag.routes.length }}\n{{ route.name }}\nNo routes\n{{ dumpJson }}\nImport core data JSON",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology How I spent two decades tracking down the creators of a 1987 USENET game and learned modern packaging tools in the process. The Discovery: A Digital Time Capsule from 1987\nPicture this: October 26, 1987. The Berlin Wall still stands, the World Wide Web is just text, and software is distributed through USENET newsgroups in text files split across multiple posts. On that day, Edward Barlow posted something special to comp.sources.games\n:\n\u201cconquest \u2013 middle earth multi-player game, Part01/05\u201d\nThat\u2019s how Ed Barlow announced it at the time, before quickly changed the name to Conquer. This was Conquer \u2013 a sophisticated multi-player strategy game that would influence countless others. Players controlled nations in Middle Earth, managing resources, armies, magic systems, and diplomatic relations. What made it remarkable wasn\u2019t just the gameplay, but how it was built and distributed in an era when \u201copen source\u201d wasn\u2019t even a term yet. Chapter 0: University Days.",
    "Chapter 0: University Days. Chapter 0: University Days. It was during these days, in the middle of the 90s, that my fellow students and I spent hours experimenting with terminals in the Computer Unix Labs, USENET, links, news, msgs, and of course: conquer. That game was a gem that required to be the leader of a country, and with a map representing as characters each player could control their elven kingdom, orcish empire, or human armies to fight each other while controlling all the details of the economy. But by 2006, this piece of computing history was trapped in legal limbo. Chapter 1: The Quest Begins (2006)\nAs a university student in Spain in the early \u201990s, I\u2019d encountered Conquer in the Unix labs. Fast forward to 2006, and I realized this pioneering game was at risk of being lost forever. The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution.",
    "The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution. I started what I thought would be a simple project: get permission from the original authors to relicense the code under GPL so it could be properly preserved and packaged for modern Linux distributions. Simple, right? Chapter 2: Digital Detective Work\nFinding Edward Barlow and Adam Bryant in 2006 was like archaeological work. Email addresses from the 1980s were long dead. USENET posts provided few clues. I scoured old university directories, googled fragments of names, and followed digital breadcrumbs across decades-old forums. The breakthrough came through pure persistence and a bit of luck. After months of searching, I managed to contact Ed Barlow. His response was refreshingly casual: \u201cYes i delegated it all to adam aeons ago. Im easy on it all\u2026. copyleft didnt exist when i wrote it and it was all for fun so\u2026\u201d\nBut there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)",
    "Chapter 3: The Long Wait (2006-2011) But there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)\nI documented everything on the Debian Legal mailing lists, created a GNU Savannah task (#5945), and even wrote blog posts hoping Adam would find them. The legal experts were clear: I needed explicit written permission from both copyright holders. Years passed. The project stalled. Then, on February 23, 2011, something magical happened. My phone buzzed with a contact form submission:\n\u201cI heard news of the request to release the code. I grant permission to release the code under GPL.\u201d \u2013 Adam Bryant\nHe had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)",
    "Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025) He had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)\nFast forward to 2025, and Stephen Smoogen contacts me about my relicesing efforts in 2006 and how he was particularly interested in reviving: Conquer Version 5 \u2013 a complete rewrite by Adam with advanced features like automatic data conversion, enhanced stability, and sophisticated administrative tools. This wasn\u2019t just an update; it was a complete reimagining of the game. But V5 had a different legal history. In the \u201990s, there had been commercial arrangements. Would Adam agree to GPL this version too? His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic",
    "His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic\nJust when I thought the story was complete, I discovered another contributor: MaF, who had created PostScript utilities for generating printable game maps \u2013 a crucial feature in the pre-GUI era when players needed physical printouts to strategize. Tracking down MaF in 2025 led me to his company, where he\u2019s now Director of Product Security. His response: \u201cOh, that was a long time ago. But yes, that was me. And I have no problem with relicensing it to GPL.\u201d\nRichard Caley: More Than Just a Legal Footnote\nBut not all searches end with an answer. Some end with silence. My investigation of Richard Caley followed the same digital breadcrumbs. I traced him to the University of Edinburgh, where he worked on speech synthesis. I found his technical contributions to FreeBSD. But the trail went cold around 2005.",
    "But the trail went cold around 2005. Then I found him \u2013 not in a USENET archive, but on the front page of his own website, preserved exactly as he left it in web.archive.org. \u201cRichard Caley suffered a fatal heart attack on the 22nd of April, 2005. He was only 41, but had been an undiagnosed diabetic, probably for some considerable time. His web pages remain as he left them.\u201d\nReading those words felt different from finding a historical record. This wasn\u2019t archival research \u2013 this was walking into someone\u2019s house years after they\u2019d gone and finding a note on the table. The page continued:\n\u201cOver and above his tremendous ability with computers and programming, Richard had a keen mind and knowledge of an extraordinary range of topics, both of which he used in frequent contributions to on-line discussions. Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code",
    "Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code The \u201cCaleyisms\u201d \u2013 The Man Behind the Code\nAnd then I discovered his \u201cCaleyisms\u201d \u2013 a curated collection of his most brilliant USENET responses that revealed not just a programmer, but a person:\nWhat\u2019s a shell suit? \u201cOil company executive.\u201d\nHow do you prepare for a pyroclastic flow hitting Edinburgh? \u201cHang 1000 battered Mars bars on strings and stand back?\u201d\nOn his book addiction:\n\u201cI never got the hang of libraries, they keep wanting the things back and get upset when they need a crowbar to force it out of my hands.\u201d\nHis humor was dry, intelligent, and uniquely British. In technical discussions, he could be brutally precise:\n\u201cLack of proper punctuation, spacing, line breaks, capitalisation etc. is like bad handwriting, it doesn\u2019t make it impossible to read what was written, just harder. But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved",
    "But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved A Digital Office Preserved\nExploring his preserved website felt like walking through his digital office. The directory structure revealed his passions: FreeBSD how-tos, POVRAY experiments, wallpaper images, technical projects. His self-deprecating humor shone through in his \u201cAbout\u201d section:\n\u201cThankfully I don\u2019t have a photograph to inflict on you. Just use the picture of Iman Bowie to the left and then imagine someone who looks exactly the opposite in every possible way. This probably explains why she is married to David Bowie and I\u2019m not.\u201d\nHere was a complete person \u2013 technical director at Interactive Information Ltd, speech synthesis researcher, FreeBSD enthusiast, Kate Bush fan, and a wit who brightened countless online discussions. The legal reality was harsh: Richard\u2019s contributions to Conquer couldn\u2019t be relicensed. The university couldn\u2019t help contact heirs due to privacy laws. His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)",
    "His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O) His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)\n\\_/@@\\\n\\\\~~/\n~~\n- RJC RIP\nIn the Conquer project documentation, Richard Caley isn\u2019t remembered as a \u201cproblem case\u201d or \u201cunlicensable code.\u201d He\u2019s honored as the vibrant person he was \u2013 the brilliant mind behind the \u201cCaleyisms,\u201d the researcher who contributed to speech synthesis, the FreeBSD advocate, and the witty participant in early online communities whose words continue to amuse and inform, decades after he wrote them. Chapter 6: Modern Renaissance \u2013 Enter GitHub, CICD and Modern Distributions\nHere\u2019s where the story gets really interesting. While working on preserving these Unix classics, I decided to learn modern packaging techniques. I chose to implement both APK (Alpine Linux) and Debian packaging for the games.",
    "I chose to implement both APK (Alpine Linux) and Debian packaging for the games. For APK packages, I used Melange \u2013 a sophisticated build system that creates provenance-tracked, reproducible packages for the Wolfi \u201cundistro\u201d. The irony? I discovered this tool when some friend started to work for the company that created it. Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite",
    "Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite - Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite\nOriginal Conquer v4 code, by Ed Barlow and Adam Bryant\n(Conquer running in docker container alongside Apache, Curses to WebSockets output thanks to ttyd. Now we can play through the web!) Conquer Version 5 \u2013 The evolution of the classical Conquer, by Adam Bryant\nChapter 8: The Human Element: Why This Matters\nThis isn\u2019t just about preserving old games \u2013 it\u2019s about preserving the story of computing itself. Ed Barlow and Adam Bryant were pioneers who built sophisticated multiplayer experiences when most people had never heard of the internet. They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community.",
    "They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community. Martin Forssen\u2019s PostScript utilities represent the ingenuity of early developers who solved problems with whatever tools were available. Want to visualize your game state? Write a PostScript generator! The 20-year relicensing effort demonstrates something crucial about open source: it\u2019s not just about code, it\u2019s about community and continuity. Every time someone maintains a legacy project, documents its history, or tracks down long-lost contributors, they\u2019re weaving the threads that connect computing\u2019s past to its future. Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure",
    "Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure - Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure\n- Modern tools can revive ancient code: Melange and modern CI/CD gave 1987 software a 2025 renaissance\nThe Continuing Story\nBoth Conquer games are now fully GPL v3 licensed and available with modern packaging. They represent not just playable software, but a complete case study in software archaeology, legal frameworks for preservation, and the evolution of development practices across four decades. The next chapter? Teaching these classic strategy games to a new generation of developers and gamers, while demonstrating that proper legal frameworks and modern tooling can give any historical software a second life. Sometimes the best way to learn cutting-edge technology is by applying it to preserve computing history. What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators?",
    "Have you ever traced the lineage of code back to its original creators? What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators? #FreeSoftware #OpenSource #SoftwarePreservation #Unix #GNU #Linux #Packaging #Melange #TechHistory #GameDevelopment #Unix #USENET #GPL #FST #Debian #ncurses #terminal #shell\nRead this article in Spanish / Lee este art\u00edculo en espa\u00f1ol:\nhttps://vejeta.com/conquer-una-odisea-de-20-anos-en-arqueologia-digital/\nThis article was originally written in both English and Spanish, with additional insights and cultural context in the Spanish version.",
    "Email verification protocol",
    "Email verification protocol Verifying control of an email address is a frequent activity on the web today and is used both to prove the user has provided a valid email address, and as a means of authenticating the user when returning to an application. Verification is performed by either:\n-\nSending the user a link they click on or a verification code. This requires the user to switch from the application they are using to their email address and having to wait for the email arrive, and then perform the verification action. This friction often causes drop off in users completing the task. There are privacy implications as the email transmission informs the mail service the applications the user is using and when they used them. -",
    "- -\nThe user logs in with a social login provider such as Apple or Google that provide a verified email address. This requires the application to have set up a relationship with each social provider, and the user to be using one of those services and wanting to share the additional profile information that is also provided in the OpenID Connect flow. The Email Verification Protocol enables a web application to obtain a verified email address without sending an email, and without the user leaving the web page they are on. To enable the functionality, the mail domain delegates email verification to an issuer that has authentication cookies for the user. When the user provides an email to the HTML form field, the browser calls the issuer passing authentication cookies, the issuer returns a token, which the browser verifies and updates and provides to the web application. The web application then verifies the token and has a verified email address for the user.",
    "The web application then verifies the token and has a verified email address for the user. User privacy is enhanced as the issuer does not learn which web application is making the request as the request is mediated by the browser. -\nSD-JWT+KB token: The selective disclosure json web token with key binding is specified in Selective Disclosure for JWT. This protocol does not use the selective disclosure features, it uses the key binding feature which enables a separation of token issuance and token presentation. The SD-JWT+KB is a token composed of two JWTs separated by the\n~\ncharacter. The first JWT is an SD-JWT aka the issuance token and is signed by the issuer and contains theemail\nandemail_verified\nclaims for the user, and the public key used by the browser to make the request. The second JWT is a KB token and is signed by the browser and contains a hash of the first JWT. The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application -",
    "The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application - Issuer: The service that verifies the user controls an email address. A DNS record for the email domain delegates email verification to the issuer. The issuer serves a\n.well-known/email-verification\nmetadata file that contains itsissuance_endpoint\nthat is called to obtain an issuance token, and itsjwks_uri\nthat points to the JWKS file containing the public keys used to verify the SD-JWT. The issuer is identified by its domain, an eTLD+1 (egissuer.example\n). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer.",
    "This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. ). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. Verified Email Release: The user navigates to any website that requires a verified email address and an input field to enter the email address. The user focusses on the input field and the browser provides one or emails for the user to select based on emails the user has provided previously to the browser. The user selects a verified email and the app proceeds having obtained the verified email. Are emails that can be verified decorated by the browser in the autocomplete UI? What UX is presented to the user when the app gets a verified email so the user knows it is already verified? sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site",
    "sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site participant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site\nRP->>RPS: Nonce request\nRPS->>RPS: Generate nonce, bind to session\nRPS->>RP: Nonce\nRP->>B: Display page\nNote over U,DNS: Step 2: Email Selection\nU->>RP: Focus on email input field\nRP->>B: Input field focused\nB->>U: Display email address list\nU->>B: Select email address\nNote over U,DNS: Step 3: Token Request\nB->>DNS: DNS TXT lookup<br/>_email-verification.$EMAIL_DOMAIN\nDNS->>B: Return iss=issuer.example\nB->>I: GET /.well-known/email-verification\nI->>B: Return metadata\nB->>B: Generate key pair<br/>Create request token\nB->>I: POST request_token=JWT... Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS",
    "Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS I->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS\nB->>B: Create KB\nB->>RP: Provide SD-JWT+KB\nNote over U,DNS: Step 6: Token Verification\nRP->>RPS: Send SD-JWT+KB\nRPS->>RPS: Parse SD-JWT+KB\nRPS->>DNS: DNS TXT lookup for email domain\nDNS->>RPS: Return iss=issuer.example\nRPS->>I: GET /.well-known/email-verification\nI->>RPS: Return metadata with jwks_uri\nRPS->>I: GET jwks_uri\nI->>RPS: Return JWKS public keys\nRPS->>RPS: Verify SD-JWT\nRPS->>RPS: Verify KB-JWT\nRPS->>RP: Email verification complete\nUser navigates to a site that will act as the RP. -\n1.1 - the RP Server generates a nonce and binds the nonce to the session. -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token",
    "If the browser receives anissuance_token -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token\nper 4.4 below, then it sends aemailverifed\nevent that has apresentationToken\nproperty. Following is an example of the HTML in the page:\n<input id=\"email\"\ntype=\"email\"\nautocomplete=\"email\"\nnonce=\"12345677890..random\">\n<script>\nconst input = document.getElementById('email')\ninput.addEventListener('emailverified', e => {\n// e.presentationToken is SD-JWT+KB\nconsole.log({\npresentationToken: e.presentationToken\n})\n})\n</script>\nAuthors are exploring alternative HTML and JS API approaches\n-\n2.1 - User focusses on email input field\n-\n2.2 - The browser displays the list of email addresses it has for the user. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field.",
    "- 2.3 - User selects an email address from browser selection, or the user types an email into the field. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field. Future: allow user to type in a field so we learn about new emails, or if the user does not want the browser to remember emails, the Email Verification Protocol is still available. In the future when we allow the user to use a passkey to authenticate to the issuer, the user can provide a verified email to a web application using a public computer by authenticating with their passkey and not enter any secrets into the public computer. If the RP has performed (1):\n- 3.1 - the browser parses the email domain ($EMAIL_DOMAIN) from the email address, looks up the\nTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record",
    "example record . The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record\n_email-verification.email-domain.example TXT iss=issuer.example\nThis record states that email-domain.example\nhas delegated email verification to the issuer issuer.example\n. If the email domain and the issuer are the same domain, then the record would be:\n_email-verification.issuer.example TXT iss=issuer.example\nAccess to DNS records and email is often independent of website deployments. This provides assurance that an issuer is truly authorized as an insider with only access to websites on\nissuer.example\ncould setup an issuer that would grant them verified emails for any email atissuer.example\n. - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer.",
    "- 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. . - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. For example, https://issuer.example/.well-known/email-verification\nmay redirect to https://accounts.issuer.example/.well-known/email-verification\n. -\n3.3 - the browser confirms that the\n.well-known/email-verification\nfile contains JSON that includes the following properties: -\nissuance_endpoint - the API endpoint the browser calls to obtain an SD-JWT\n-\njwks_uri - the URL where the issuer provides its public keys to verify the SD-JWT\n-\nsigning_alg_values_supported - OPTIONAL. JSON array containing a list of the JWS signing algorithms (\"alg\" values) supported by both the browser for request tokens and the issuer for issued tokens. The same algorithm MUST be used for both the\nrequest_token\nandissuance",
    "The same algorithm MUST be used for both the\nrequest_token\nandissuance request_token\nandissuance\nwithin a single issuance flow. Algorithm identifiers MUST be from the IANA \"JSON Web Signature and Encryption Algorithms\" registry. If omitted, \"EdDSA\" is the default. \"EdDSA\" SHOULD be included in the supported algorithms list. The value \"none\" MUST NOT be used. Each of these properties MUST include the issuer domain as the root of their hostname. Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token",
    "Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token jwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token\n- email - email address to be verified\nThe browser SHOULD select an algorithm from the issuer's signing_alg_values_supported\narray, or use \"EdDSA\" if the property is not present. An example JWT header:\n{\n\"alg\": \"EdDSA\",\n\"typ\": \"JWT\",\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\" // base64url-encoded public key\n}\n}\ndo we want to register a new JWT\ntyp\nAn example payload\n{\n\"aud\": \"issuer.example\",\n\"iat\": 1692345600,\n\"email\": \"user@example.com\"\n}\n- 3.5 - the browser POSTs to the\nissuance_endpoint\nof the issuer with 1P cookies with a content-type ofapplication/x-www-form-urlencoded\ncontaining arequest_token\nparameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=... parameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail jwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim contains a syntactically valid email address\n-\n4.3 - the issuer checks if the cookies sent represent a logged in user, and if the logged in user has control of the email provided in the request_token. If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0",
    "If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0 fieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0\n- Signature: MUST be signed with the issuer's private key corresponding to a public key in the\njwks_uri\nidentified bykid\n- Header: MUST contain\nExample header:\n{\n\"alg\": \"EdDSA\",\n\"kid\": \"2024-08-19\",\n\"typ\": \"evp+sd-jwt\"\n}\nExample payload:\n{\n\"iss\": \"issuer.example\",\n\"iat\": 1724083200,\n\"cnf\": {\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\"\n}\n},\n\"email\": \"user@example.com\",\n\"email_verified\": true\n}\nThe resulting JWT has the ~\nappended to it, making it a valid SD-JWT. - 4.4 - the issuer returns the SD-JWT to the browser as the value of\nissuance_token\nin anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}",
    "Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"} in anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}\nIf the issuer cannot process the token request successfully, it MUST return an appropriate HTTP status code with a JSON error response containing an error\nfield and optionally an error_description\nfield. When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid.",
    "When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. {\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:",
    "When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability: HTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:\nHTTP 500 Internal Server Error\n{\n\"error\": \"server_error\",\n\"error_description\": \"Temporary server error, please try again later\"\n}\nIn a future version of this spec, the issuer could prompt the user to login via a URL or with a Passkey request. On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the",
    "On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT",
    "email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT : \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT\n- Header:\n- signing the KB-JWT with the browser's private key (the same key pair generated in step 3.4)\n- concatenating the SD-JWT and the KB-JWT separated by a tilde (~) to form the SD-JWT+KB\nExample KB-JWT header:\n{ \"alg\": \"EdDSA\", \"typ\": \"kb+jwt\" }\nExample KB-JWT payload:\n{ \"aud\": \"https://rp.example\", \"nonce\": \"259c5eae-486d-4b0f-b666-2a5b5ce1c925\", \"salt\": \"kR7fY9mP3xQ8wN2vL5jH6tZ1cB4nM9sD8fG3hJ7kL2p\", \"iat\": 1724083260, \"sd_hash\": \"X9yH0Ajrdm1Oij4tWso9UzzKJvPoDxwmuEcO3XAdRC0\" }\n-\n5.3 - the browser sets a TBD hidden field and fires the TBD event ...\ndetails TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n- details TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by: iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-",
    "iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n- iss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-\n6.5 - the RP verifies the KB-JWT signature using the public key from the\ncnf\nclaim in the SD-JWT with thealg\nalgorithm from the KB-JWT header\nBelow are notes capturing some discussions of potential privacy implications. -\nThe email domain operator no longer learns which applications the user is verifying their email address to as the applications are no longer sending an email verification code to the user. By using an SD-JWT+KB, the browser intermediates the request and response so that the issuer does not learn the identity of the RP. -\nThe RP can infer if a user is logged into the issuer as the RP receives a SD-JWT when the user is logged in, and does not when the user is not logged in. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had.",
    "-\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. The web page would call an API passing the email address and nonce. It would return a promise that resolves to the SD_JWT or an error response. The API would only be callable after a user gesture such as clicking a button labelled verify on the web page. This provides the web page in more flexibility in how to gather the email address. For example, if the web page is using EVP for login, and the user has used different emails for login and those are stored in cookies, the page can display the list of emails and an option to provide a different one. The user can then select the email they want to use rather than having to type it into a text field.",
    "The user can then select the email they want to use rather than having to type it into a text field. In addition to, or instead of the browser sending cookies to the Issuer, the Issuer could return a WebAuthN request to the browser if it has credentials for the user identified by the email address. The browser would then interact with the user and provide the WebAuthN response to the Issuer, authenticating the user, and the Issuer would then return the SD-JWT. Rather than the DNS TXT record, the Mail Domain would host a JSON file in the .wellknown domain. This creates challenges for the long tail of individually owned domains:\n- would require a domain that is used just for email to now have to support a web server\n- the mail domain is usually an apex domain, which does not support CNAME, complicating hosting a web site",
    "Using bubblewrap to add sandboxing to NetBSD",
    "Using bubblewrap to add sandboxing to NetBSD Google Summer of Code 2025 Reports: Using bubblewrap to add sandboxing to NetBSD\nThis report was written by Vasyl Lanko as part of Google Summer of Code 2025. Introduction\nAs of the time of writing, there is no real sandboxing technique available to NetBSD. There is chroot, which can be considered a weak sandbox because it modifies the root directory of the process, effectively restricting the process' view of the file system, but it doesn't isolate anything else, so all networking, IPC, and mounts inside this restricted file system are the same as of the system, and are accessible. There has already been some research on implementing kernel-level isolation in NetBSD with tools like gaols, mult and netbsd-sandbox, but they haven't been merged to NetBSD. Other operating systems have their own ways to isolate programs, FreeBSD has jails, and Linux has namespaces. Project Goals",
    "Project Goals Project Goals\nThe goal of this project is to bring a new way of sandboxing to NetBSD. More specifically, we want to implement a mechanism like Linux namespaces. These namespaces allow the isolation of parts of the system from a namespace, or, as the user sees it, from an application. NetBSD has compat_linux to run Linux binaries on NetBSD systems, and the implementation of namespaces can also be utilized to emulate namespace-related functionality of Linux binaries. A simple example to visualize our intended result is to consider an application running under an isolated UTS namespace that modifies the hostname. From the system's view, the hostname remains the same old hostname, but from the application's view it sees the modified hostname. Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details",
    "Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details - UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details\n- mount namespace, it is a prerequisite to most other namespace types because UNIX follows the philosophy of \"everything is a file\", so we need a separate mount namespace to have different configuration files on the same location as the system. Linux creates namespaces via the unshare or clone system calls, and it will also be our way of calling the namespace creation logic. We setup the base for implementing Linux namespaces in the NetBSD kernel using kauth, the subsystem managing all authorization requests inside the kernel. It associates credentials with objects, and because the namespace lifecycle management is related to the credential lifecycle it handles all the credential inheritance and reference counting for us. (Thanks kauth devs!)",
    "(Thanks kauth devs!) We separate the implementation of each namespace in a different secmodel, resulting in a similar framework to Linux which allows the isolation of a single namespace type. Our implementation also allows users to pick whether they want to have namespace support, and of what kind, via compilation flags, just like in Linux. UTS namespace\nUTS stands for UNIX Timesharing System, because it allows multiple users to share a single computer system. Isolating the utsname\ncan be useful to give users the illusion that they have control over the system's hostname, and also, for example, to give different hostnames to virtual servers. The UTS namespace stores the namespace's hostname, domain name, and their lengths. To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace.",
    "To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace. This namespace specific information needs to be saved somewhere, and for that we use the credential's private_data\nfield, so we can use a UTS_key\nto save and retrieve UTS\nrelated information from the secmodel. The key specifies the type of information we want to retrieve from the private_data\n, hence using a UTS_key\nfor the UTS namespace. The key for each namespace is a fixed value (we don't create a new key for every credential), but the retrieved value for that key from different credentials may be different. We had to modify kernel code that was directly accessing the hostname\nand domainname\nvariables, to instead call get_uts()\n, which retrieves the UTS struct for the namespace of the calling process. We didn't modify occurrences in kernel drivers because drivers are not part of any namespace, so they should still access the system's resources directly. MNT namespace",
    "MNT namespace MNT namespace\nThe MNT namespace isolates mounts across namespaces. It is used to have different versions of mounted filesystems across namespaces, meaning a user inside a mount namespace can mount and unmount whatever they want without affecting or even breaking the system. The mount namespace structure in Linux is fairly complicated. To have something similar in NetBSD we need to be able to control the mounts accessed by each namespace, and for that we need to control what is each namespace's mountlist, this is also enough for unmounting file systems, because in practice we can just hide them. For the mount_namespace, mountlist structure and the number of credentials using the mount namespace are stored in the credential's private data with the MNT_key\n. Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()",
    "Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist() . Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()\nwhich returns the correct mountlist for the namespace the calling process resides in. Implementation for the mount namespace is immensely more complex than for the UTS namespace, it involves having a good understanding of both Linux and NetBSD behaviour, and I would frequently find myself wondering how to implement something after reading the Linux man pages, which would lead to me looking for it in the Linux source code, understanding it, then going back to NetBSD source code, trying to implement it, and seeing it's too different to implement in the same way. Project Status\nYou can find all code written during this project in GitHub at maksymlanko/netbsd-src gsoc-bubblewrap\nbranch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap",
    "Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap branch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap\nbranch and this was the last one for the mnt_ns\nstill WIP branch. The link includes implementation of general namespace code via secmodels, implementation of the UTS namespace and related ATF-tests, and the work-in-progress implementation of mount namespaces. The mount namespace functionality is not finished as it would require much more work than the time available for this project. To complete it, it would be required invasive and non-trivial changes to the original source code, and, of course, more time. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement.",
    "Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. I believe that after mount namespaces it would be interesting to implement PID namespaces as this in combination with mount namespaces would permit process isolation from this sandbox. Afterwards, implementing user namespaces would allow users to get capabilities similar to root\nin the namespace, giving them sudo\npermissions while still restricting system-wide actions like shutting down the machine. A lower hanging fruit is to implement the namespace management functionality, which in Linux is lsns to list existing namespaces, and setns to move the current process to an already existing namespace. Challenges\n- Semantics. Did you know the unmount system call with MNT_FORCE flag in Linux (usually) returns EBUSY, and in NetBSD it forces the unmounting? One of them makes it easier to implement mount namespaces.",
    "One of them makes it easier to implement mount namespaces. - The behaviour of namespaces is not fully specified in the man pages. If something is not clear from the man pages you need to read the source code. - Unexpected need to learn a lot of VFS concepts and their differences in NetBSD and Linux. - There was a much bigger research component than I anticipated. In the end, Linux and NetBSD are different operating systems, implemented in different ways. Linux is complex and it is not trivial to port namespaces to NetBSD. Notes\nThe project is called \"Using bubblewrap to add sandboxing to NetBSD\" and was initially projected to emulate the unshare\nsystem call into compat_linux\n, but, seeing that having namespaces could be useful for NetBSD, and that it would be easy to add to compat_linux\nafterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap",
    "Implementing other system calls necessary to make the bwrap afterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap\nlinux binary work correctly also wouldn't be as satisfying as implementing namespaces directly into NetBSD, so this was why the project was initially called \"Using bubblewrap to add sandboxing to NetBSD\" but nowadays it would be more accurate to call it \"Sandboxing in NetBSD with Linux-like namespaces\". Thanks\nI am very grateful to Google for Google Summer of Code, because without it I wouldn't have learned so much this summer, wouldn't have met with smart and interesting people, and for sure wouldn't have tried to contribute to a project like NetBSD, even if I always wanted to write operating systems code... But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects.",
    "But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects. I would also like to thank the members of the NetBSD organization for helping me throughout this project, and more specifically:\n- Taylor R. Campbell, Harold Gutch and Nia Alarie from IRC, for helping me fix a nasty\nLD_LIBRARY_PATH\nbug I had on my system which wouldn't let me finish compiling NetBSD, and general GSoC recomendations. - Emmanuel Dreyfus from\ntech-kern\n, with whom I discussed ideas for projects and proposal suggestions, and in the end inspired the namespaces project. - Christoph Badura and Leonardo Taccari who volunteered to be my mentors. They took time to research and answer my questions, anticipated possible problems in my approaches, and always pointed me in the right direction, daily, during all of GSoC's period. This project is from the 3 of us.",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law Montana has made history as the first state in the U.S. to legally protect its citizens\u2019 right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law. The groundbreaking legislation affirms Montanans\u2019 fundamental right to own and operate computational resources \u2014 including hardware, software, and AI tools \u2014 under the state\u2019s constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world. \u201cMontana is once again leading the way in defending individual liberty,\u201d said Senator Daniel Zolnikov, the bill\u2019s sponsor and a longtime advocate for digital privacy. \u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d",
    "\u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law. The act also includes provisions for AI-controlled critical infrastructure, requiring both a \u201cshutdown mechanism\u201d to allow human control and annual safety reviews \u2014 a move aimed at balancing innovation with public safety concerns. The bill has drawn praise from privacy advocates and tech policy groups. Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d",
    "Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d The MRTCA stands in stark contrast to recent regulatory efforts in other states, such as California, Virginia, and New York, where proposals to rein in AI technologies have either failed or been heavily revised. Montana\u2019s approach leans toward empowering individual users rather than restricting access. The law has already inspired similar efforts in New Hampshire, where lawmakers are pushing a constitutional amendment guaranteeing access to computation. Rep. Keith Ammon, the state\u2019s Majority Floor Leader, praised Montana\u2019s leadership: \u201cThis is the kind of bold move that sets the tone for the rest of the country.\u201d\nNationally, the Right to Compute movement is gaining traction. Spearheaded by the grassroots group RightToCompute.ai, the campaign argues that computation \u2014 like speech and property \u2014 is a fundamental human right. \u201cA computer is an extension of the human capacity to think,\u201d the organization states.",
    "\u201cA computer is an extension of the human capacity to think,\u201d the organization states. The movement is supported by Haltia.AI, a Dubai-based AI startup, and the ASIMOV Protocol, a blockchain consortium advocating for decentralized AI infrastructure. Talal Thabet, Co-Founder of both groups, praised Montana\u2019s law as \u201ca monumental step forward in ensuring individuals retain control of their own data and digital tools.\u201d\nAs debates over AI governance and digital rights continue to evolve, Montana\u2019s bold new law could serve as a blueprint for other states seeking to safeguard freedom in the digital era.",
    "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
    "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit Pipeflow is a lightweight pipeline engine for PHP applications. It lets you describe complex automations as a sequence of small, reusable processing steps called stages. The real superpower is that the entire flow can be expressed in a clear XML format that is easy to read, visualise, and reason about\u2014so even non-developers can review, maintain, and update automations without touching PHP code (but you can also configure the pipelines via hard coded php code). Each stage receives a shared context, performs a focused unit of work, and returns the enriched context to the next stage. By chaining stages together you can orchestrate complex jobs while keeping each piece easy to maintain and test.",
    "By chaining stages together you can orchestrate complex jobs while keeping each piece easy to maintain and test. In other words Pipeflow library gives you the instruments to instantiate one or more pipelines from an xml configuration, providing starting data in an initial context (optionally), and execute them when you want. What you will need to do is use these instruments in your web application to allow your actors to: edit the pipeline's configurations xml (via a text editor), save the pipeline xml configuration somewhere (e.g. your application db), and, when your application need to start a pipeline (manually or through a cron), just load the xml, feed it in the Pipeline class instance, and launch it. - Why Pipeflow matters\n- Real Use Cases\n- Other example use cases\n- Installation and Documentation\n- Quick introduction to pipelines\n- Extending with custom stages\n- Learn more\n- Contribute to PipeFlow\n- License\n- Human-friendly configuration \u2013 describe automations in an XML document that business users and developers alike can read, review, and edit safely.",
    "- Why Pipeflow matters\n- Real Use Cases\n- Other example use cases\n- Installation and Documentation\n- Quick introduction to pipelines\n- Extending with custom stages\n- Learn more\n- Contribute to PipeFlow\n- License\n- Human-friendly configuration \u2013 describe automations in an XML document that business users and developers alike can read, review, and edit safely. - Learn more\n- Contribute to PipeFlow\n- License\n- Human-friendly configuration \u2013 describe automations in an XML document that business users and developers alike can read, review, and edit safely. - Composable workflows \u2013 build sophisticated automations by wiring together focused stages instead of writing one-off scripts. - Consistent execution model \u2013 every stage works with the same\nPipelineContext\n, making it straightforward to pass data between steps. - Configurable runtime \u2013 author pipelines either in PHP or in XML, choose the configuration style that best fits your team. - Extensible catalogue \u2013 register your own custom stages to integrate third-party services, generative AI calls, or bespoke business logic. Here is some real use cases which leverages the power of PipeFlow\n-",
    "Here is some real use cases which leverages the power of PipeFlow\n- PagineDaColorare.it: A wordpress website that automatically create and published coloring pages for children, daily, using the AI. This website uses two wordpress plugins I've developed (that maybe i will publish in future): one of them exposes pipeflow-php into wordpress, adding some custom stages to manage wordpress (creating posts, saving images, setting custom fields, category and tags) and allowing to modify the pipeline's xml from the wordpress admin panel (so that i can refine it, improve the content creation logic, change the logic on holidays, i.e. Christmas, and so on). The other plugins adds some more custom stages to pipeflow which allows to generate text and images with OpenAI apis. All these new custom stages is then used together to automatically run the coloring page content generation pipeline daily, with a cron",
    "All these new custom stages is then used together to automatically run the coloring page content generation pipeline daily, with a cron . All these new custom stages is then used together to automatically run the coloring page content generation pipeline daily, with a cron. The advantage is that anyone, even non-developers, can refine, mantain, edit the coloring page content generation pipeline logic, simply by changing the XML configuration in the wordpress admin panel. The coloring page content generation pipeline configuration is now quite complex, but is very easy to read, understand and mantain: it combines many different stage types which randomizes coloring pages themes, subjects, actions, asking the supporting of the AI in different phases of the pipeline execution.",
    "The coloring page content generation pipeline configuration is now quite complex, but is very easy to read, understand and mantain: it combines many different stage types which randomizes coloring pages themes, subjects, actions, asking the supporting of the AI in different phases of the pipeline execution. -\nFiaberello.it: Similar to the website above, this is another website I've developed with the power of pipeflow. It automaticallys creates and publish fairy tales for children, with a cover image for each tale. This is more a test/example, it's pipeline is more simple and refined than the previous one, so it can be even improved. -\nEditorial automation for any CMS \u2013 Create a plugin for your CMS which leverage pipeflow to build custom workflows which can be easily edited and refined by any actor in your team, even non developers: fetch posts, transform content, and trigger publication flows from scheduled pipelines, with editors able to tweak behaviour directly in XML, allowing any actor in your team (including non-developers) to mantain, refine, change the workflow. -\nBack-office data processing \u2013 build nightly ETL jobs that consume feeds, clean data, and sync results to downstream services without redeploying code. -",
    "- -\nBack-office data processing \u2013 build nightly ETL jobs that consume feeds, clean data, and sync results to downstream services without redeploying code. -\nMarketing and CRM orchestration \u2013 enrich leads, call external APIs, and keep SaaS tools in sync while letting stakeholders adjust logic themselves. -\nAI-assisted content workflows \u2013 combine prompt generation, randomisation, and templating stages to automate creative tasks, like i did on the websites above. -\nAny automation/workflow you can image, easily mantained by even non-developers - By allowing to create custom stages, you can encapsulate your custom business logic in new custom stages, which then can be used in your pipelines. These pipelines can then be edited, mantained or refined by any actor in your team, easily and visually by an easy to reason and read XML configuration. The full reference, including installation instructions, quick start, and detailed stage catalogue, lives in DOCUMENTATION.md.",
    "The full reference, including installation instructions, quick start, and detailed stage catalogue, lives in DOCUMENTATION.md. The full reference, including installation instructions, quick start, and detailed stage catalogue, lives in DOCUMENTATION.md. A pipeline describes the ordered stages that should run and the data they exchange. Typical stages read or write values from the PipelineContext\nwhich is a container which contains parameters and their values, transform data, or control the flow of execution (loops, conditionals, etc.). The pipeline starts with an empty context, but you can feed a starting context from code if you want to provide the pipeline with prepared data to be available to the stages.",
    "The pipeline starts with an empty context, but you can feed a starting context from code if you want to provide the pipeline with prepared data to be available to the stages. Each stage can reads and write data (parameters) to the context and perform operations (even custom operations by implementing custom stages, like calling apis, performing custom business logic operations, etc...), which can then be read and manipulated by the subsequent stages, until the pipelines finish the execution. At that point, the manipulated context is returned by the pipeline (with all the parameters written by the stages that has been executed). Pipelines can be declared in XML so they can be edited without touching PHP code. A minimal XML pipeline looks like the following. <?xml version=\"1.0\" encoding=\"utf-8\"?>\n<pipeline id=\"hello_world\">\n<stages>\n<stage type=\"SetValue\">\n<settings>\n<param name=\"parameterName\">message</param>\n<param name=\"parameterValue\">Hello Pipeflow!</param>\n</settings>\n</stage>\n<stage type=\"Echo\">\n<settings>\n<param name=\"text\">%%message%%</param>\n</settings>\n</stage>\n</stages>\n</pipeline>",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<pipeline id=\"hello_world\">\n<stages>\n<stage type=\"SetValue\">\n<settings>\n<param name=\"parameterName\">message</param>\n<param name=\"parameterValue\">Hello Pipeflow!</param>\n</settings>\n</stage>\n<stage type=\"Echo\">\n<settings>\n<param name=\"text\">%%message%%</param>\n</settings>\n</stage>\n</stages>\n</pipeline> <param name=\"parameterValue\">Hello Pipeflow!</param>\n</settings>\n</stage>\n<stage type=\"Echo\">\n<settings>\n<param name=\"text\">%%message%%</param>\n</settings>\n</stage>\n</stages>\n</pipeline>\nYour application tells pipeflow to load the XML configuration, pipeflow will automatically configure the pipeline and prepares it for execution. Your application can then launch the pipeline when it's needed simply by calling the execute() method on the pipeline instance (on demand or even via a cron). If you want, you can also pass a pre-defined starting context (if you want to feed, for example, some data from code into the pipeline execution, that can be used by the stages). Because the pipeline definition is data-driven, you can adjust parameters or reorder stages without redeploying code.",
    "Because the pipeline definition is data-driven, you can adjust parameters or reorder stages without redeploying code. Because the pipeline definition is data-driven, you can adjust parameters or reorder stages without redeploying code. ### Configuring programmatically via PHP Since XML is the easier way to configure pipelines \"visually\" and allows also non-developers to edit and mantain them by enabling any actor to manage automations in your web applications, you can also configure the pipelines programmatically in your php code. This may have sense for example for those business logic automations that doesn't need to be edited/mantained from your application administration panels, are fixed (doesn't change often), or doesn't need to be mantained by non-developers actors. More info in the DOCUMENTATION.md\nPipeflow ships with a catalogue of built-in stages, but you can register your own custom stages to integrate APIs, internal systems, or platform-specific behaviour. Once registered, custom stages become available to both PHP and XML pipelines, letting you reuse them across projects.",
    "Once registered, custom stages become available to both PHP and XML pipelines, letting you reuse them across projects. The full reference, including installation instructions, control-flow stages, and detailed stage catalogue, lives in DOCUMENTATION.md. Pipeflow thrives on community input and it surely needs improvements and features: Whether you want to improve the core engine, add new features, add new built-in stages, fix bugs, or share feedback from real-world deployments, we would love to collaborate. Check out CONTRIBUTING.md for guidelines on reporting issues, proposing ideas, and submitting pull requests. Pipeflow is distributed under the permissive BSD 3-Clause License, which keeps the project friendly for both open-source and commercial use while encouraging community contributions.",
    "I Am Mark Zuckerberg",
    "I Am Mark Zuckerberg Welcome to iammarkzuckerg.com\nNo, not THAT Mark Zuckerberg-this one's busy helping Hoosiers, not launching social networks. Relax, you haven't accidentally logged into Facebook or the Metaverse. You're on the site of Mark S. Zuckerberg, Indiana's original bearer of the name, proud bankruptcy attorney, and frequent recipient of confused emails from people seeking tech support or handouts of money. What I Really Do:\n- Help people obtain a fresh financial start (no passwords required)\n- Offer dependable, human-involved advice (my artificial intelligence is powered by coffee)\n- Answer local legal questions, not privacy scandals\nReal Zuckerberg Facts:\n- Shares a name, not fortune, with the Facebook founder\n- Gets mistaken daily for a tech billionaire\n- Has written zero social media apps, but plenty of court briefs\nFun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place!",
    "But if you need trustworthy bankruptcy help, you're in exactly the right place! Fun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place! Click around, get to know your (non-billionaire) local Mark, and remember: No login required. Click Here to See How Other\nWebsites Have Reacted to This\nInteresting Things That Have Happened to Me Because My Name is Mark Zuckerberg\nFor a complete list of things that have happened to Mark Zuckerberg click here\nLike I said, I don't wish Mark E. Zuckerberg any ill will at all. I hope the best for him, but let me tell you this: I will rule the search for \"Mark Zuckerberg bankruptcy\". And if he does fall upon difficult financial times, and happens to be in Indiana, I will gladly handle his case in honor of our eponymy.",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel Ironclad is a (partially) formally verified, real-time capable, UNIX-like operating system kernel for general-purpose and embedded uses. It is written in SPARK and Ada, and is comprised of 100% free software. Ironclad features a familiar POSIX-compatible interface, true simultaneous preemptive multitasking, Mandatory Access Control (MAC), and support for hard real-time scheduling. Ironclad is fully open source and distributed under the GPLv3, ensuring it remains free. No firmware blobs are needed or shipped with the kernel. Every piece of the stack is open source. SPARK's state of the art formal verification is employed for ensuring absence of errors and correctness of big portions of Ironclad, like cryptography, MAC, and user-facing facilities. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation.",
    "Dependency on only the GNU toolchain allows for easy cross-compilation. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation. Ironclad will always be free for use, study, and modification, so, to support the project, we rely on the use of donations and grants. Every contribution makes a difference and allows us to do more. This project is funded through NGI Zero Core, a fund established by NLnet with financial support from the European Commission's Next Generation Internet program. Learn more at the NLnet project page. Additionally, we would like to thank the following organizations:",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team Zensical \u2013 A modern static site generator built by the Material for MkDocs team\u00b6\nWe are thrilled to announce Zensical, our next-gen static site generator designed to simplify the process of building documentation sites. Distilled from a decade of experience, Zensical is our effort to overcome the technical limitations of MkDocs, reaching far beyond its capabilities. Zensical is the result of thousands of hours of work \u2013 built from the ground up for a modern and comfortable authoring experience, while making it easy for developers to extend and customize Zensical through its upcoming module system. Our goal is to support docs-as-code workflows with tens of thousands of pages, without compromising performance or usability. To make the transition seamless, compatibility comes first. We're putting significant effort into ensuring a smooth migration from Material for MkDocs for all users. Zensical can natively read mkdocs.yml",
    "Zensical can natively read mkdocs.yml , allowing you to build your existing project with minimal changes. As of now, a subset of plugins is supported, and we're working on feature parity in the coming months. Zensical is fully Open Source, licensed under MIT, and can be used for any purpose, including for commercial use. We're also saying goodbye to our sponsorware model, replacing it with our new offering for professional users: Zensical Spark. This allows us to stay independent, maximizing user value, as we shape the future of Zensical together with you. You can subscribe to our newsletter to stay in the loop. This is the second article in a four-part series:\n- Transforming Material for MkDocs\n- Zensical \u2013 A modern static site generator built by the creators of Material for MkDocs. - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6",
    "- What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6 - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6\nSince its initial release in 2016, Material for MkDocs has helped tens of thousands of teams to publish and maintain reliable documentation. However, in recent years, it has become apparent that we were running up against limitations of our core dependency, MkDocs. These limitations proved impossible to overcome as they are deeply rooted in its architecture. We also mentioned in our update on our foundational work that MkDocs must be considered a supply chain risk, since it's unmaintained since August 2024. It has seen no releases in over a year and is accumulating unresolved issues and pull requests. These developments have forced us to cut our ties to MkDocs as a dependency.",
    "These developments have forced us to cut our ties to MkDocs as a dependency. In order to map out a path forward, we went back to the drawing board, talked to dozens of our professional users and thoroughly analyzed the MkDocs ecosystem. We didn't just want to create a fork or port of MkDocs, but decided to rethink static site generation from first principles. With Zensical, we are creating a modern static site generator, which is compatible with your content and customizations, and addresses MkDocs' limitations. While Material for MkDocs is built on top of MkDocs, Zensical consolidates both projects into one coherent stack, covering static site generation, theming, and customization. What you can expect today:\nAlthough we haven't reached full feature parity yet, you can already use Zensical to build your existing Material for MkDocs projects with minimal changes. You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6",
    "What you can expect\u00b6\nSolid foundation\u00b6 You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6\nOur goal with Zensical is to create a coherent and modern stack, vertically integrating all parts of the authoring experience (AX), developer experience (DX), and user experience (UX). This gives us a significant competitive advantage over solutions that overly rely on third-party frameworks and dependencies, helping us to create much more robust Open Source software. ZRX, our new differential build engine, creates a solid foundation for Zensical, and is an Open Source project of its own. It's a fresh take on making differential data flows easy to build and a joy to work with. Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster.",
    "Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster. Following the principle of architectural hoisting, we moved essential, reusable functionality into ZRX, which allows us to keep Zensical's core simple and focused on static site generation. ZRX handles the heavy lifting \u2013 differential builds, caching, and data flow orchestration. With the upcoming module system and component system, both of which are on our public roadmap, Zensical will gain more degrees of freedom in the coming months, allowing you to extend and customize Zensical in ways that were previously impossible with MkDocs. Modern design\u00b6\nZensical brings a fresh, modern design that breaks out of the Materal Design aesthetic, creating a visual foundation that is more easily brandable and adaptable to different use cases. The new design prioritizes clarity, simplicity, and usability, while having a more professional finish:",
    "The new design prioritizes clarity, simplicity, and usability, while having a more professional finish: Right now, the layout and site structure of Zensical match Material for MkDocs closely, as we're focusing on ensuring maximum compatibility. Once we finish work on our upcoming component system, we'll provide an alternative that is much more flexible and adaptable, and can be tailored to different use cases and branding requirements more easily. You can also keep the Material for MkDocs look and feel with a single line of configuration. Blazing-fast search\u00b6\nClient-side search isn't a compromise \u2013 for the vast majority of static sites, it's the best solution, since it's faster, involves zero maintenance, and doesn't require you to pay for a service. As covered in depth in the first part of this series, the current search implementation in Material for MkDocs has severe limitations, and is based on a now unmaintained library, which is why we decided to build a new search engine from scratch. It's based on the same goals as Zensical itself: performance, flexibility, and extensibility.",
    "It's based on the same goals as Zensical itself: performance, flexibility, and extensibility. Disco, our modular and blazing-fast client-side search engine, is exclusively available in Zensical. When you build your site with Zensical, your users will immediately benefit from Disco's improved ranking algorithm, as well as its filtering and aggregation capabilities:\nIn early 2026, we'll be releasing Disco as a standalone Open Source project. With the feedback of our professional users in Zensical Spark, we're going to evolve the search experience, turning Disco into a highly configurable and customizable search engine that adapts to your needs. You can subscribe to our newsletter to receive news about Disco. Authoring experience\u00b6\nSlow feedback loops can be a major pain point when writing documentation. Almost all of us know the feeling of waiting for the static site generator to finish building the site, just to see a small change reflected in the output. With Zensical, we're finally addressing this issue.",
    "With Zensical, we're finally addressing this issue. It's important to understand that we're not yet utilizing the differential capabilities of ZRX to the fullest extent, as we're forced to make several compromises to ensure maximum compatibility with Material for MkDocs at the moment. Markdown rendering needs to go through Python Markdown, which forces us to pay for extra marshalling costs. While the initial build can sometimes be slower than with MkDocs, repeated builds \u2013 especially when serving the site \u2013 are already 4 to 5x faster, as only changed files need to be rebuilt. We're also working on a new Markdown toolchain based on a CommonMark-compliant parser written in Rust, which will make Markdown processing significantly faster. We'll be tackling this as part of the upcoming component system, which we'll start working on in early 2026. Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content.",
    "Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content. Maximum compatibility\u00b6\nCompatibility with Material for MkDocs is our top priority. We understand that switching to a new static site generator can be challenging, especially for large projects with many customizations. Therefore, we've put significant effort into ensuring that Zensical understands mkdocs.yml\nconfiguration files, so that you can build your projects with minimal changes. This means your existing Markdown files, template overrides, CSS and JavaScript extensions don't need to be touched, primarily because we did not change the generated HTML, and rely on Python Markdown for processing your content. However, plugins are a different story. In MkDocs, practically all plugins have side effects, making it impossible to parallelize builds. We started from first principles and asked: what should extensibility look like in a modern static site generator? Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles:",
    "Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles: - Modules can inject, extend, and re-define functionality\n- Modules are deterministic through topological ordering\n- Modules foster reusability, with the possibility to remix them\n- Modules can cooperate through well-defined contracts\nWe're working on shipping essential functionality as provided by MkDocs plugins as built-in modules. In early 2026, we will open the module system to third-party developers, so they can start building their own modules, as we see Zensical as the heart of a thriving ecosystem. Zensical Spark\u00b6\nZensical Spark, our offering for professionals, is the result of countless calls with professional users of Material for MkDocs. From startups to large enterprises, we enable organizations to realize complex projects in diverse environments. For this, we've created Zensical Spark as a collaborative space. If you're a professional user, Zensical Spark is for you, since:\n-",
    "If you're a professional user, Zensical Spark is for you, since:\n- -\nYou can be confident that Zensical will continue to be developed and maintained in the long term as a set of interconnected and sustainable OSI-compliant Open Source projects. -\nYou can receive the support you need to successfully use, configure and customize Zensical in your organization, receiving first-class support from the Zensical team. -\nYou can influence the future development of Zensical by participating in our new approach to Open Source software development, helping us to build exactly what you need. Let's talk! If you're working in a professional context, reach out to contact@zensical.org to schedule a call and learn how Zensical Spark enables your team to transition to Zensical smoothly and have a voice in its continued development. You should also consider joining the waiting list, since seats are limited. We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical!",
    "We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! At Zensical, Tim is focusing on providing the same seamless experience for generating API reference documentation from source code (via docstrings) as he has done with mkdocstrings, the second biggest project in the MkDocs ecosystem. With his expertise, and Zensical's new stack, we'll be pushing the boundaries of what's possible with API reference documentation. Goodbye, GitHub Sponsors\u00b6\nThank you! To all of you who have supported us over the years through GitHub Sponsors \u2013 we are incredibly grateful for your support. It has been invaluable in helping us to build, maintain and evolve Material for MkDocs, and we couldn't have done it without you. Seriously, thank you!",
    "Seriously, thank you! Material for MkDocs gave us something invaluable: experience building for tens of thousands of users, and the opportunity to build a team around Open Source software. It showed us that making a living from Open Source isn't just possible \u2013 we grew it into one of the largest sponsorware projects on GitHub and inspired others to pursue similar paths. Now we're breaking new ground. Zensical is our next chapter, and we're professionalizing how we approach Open Source development. Our vision is to make Zensical free for everyone to use while building a sustainable business around it through our new approach. This transition means saying goodbye to GitHub Sponsors. It has served us exceptionally well, but as we professionalize and scale, we're making the leap from personal project to company \u2013 building a business and team that can meet the growing demands of professional users while staying true to our values. We're doubling down on Open Source, developing software for everyone.",
    "We're doubling down on Open Source, developing software for everyone. We're doubling down on Open Source, developing software for everyone. If you want to continue supporting our work, please subscribe to our newsletter. We'll be providing new methods to support us in the coming months, with the possibility of getting exclusive goodies. Looking Ahead\u00b6\nMaterial for MkDocs grew organically in a pot that eventually became too small. With Zensical, we're building on solid foundations designed to grow with us \u2013 and with you. Material for MkDocs is now in maintenance mode\nWe want to be transparent about the risks of staying on Material for MkDocs. With MkDocs unmaintained and facing fundamental supply chain concerns, we cannot guarantee Material for MkDocs will continue working reliably in the future. We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical.",
    "We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical. If documentation plays a critical role in your organization, and you're worried how this might affect your business, consider joining Zensical Spark, or feel free to schedule a call by reaching out at contact@zensical.org. Where we'll be in 12 months\u00b6\nOver the next 12 months, following our phased transition strategy, we'll reach Phase 2 and 3 \u2013 introducing our module system and component system, as well as CommonMark support. By replacing Python Markdown with a Rust-based Markdown parser, we'll unlock performance improvements and the modularity needed for flexible templating. This is where Zensical truly starts to unfold its capabilities. Zensical is already powering real projects due to extensive compatibility with Material for MkDocs. We're actively working on closing the gap to reach full feature parity. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help.",
    "If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. Connect with us\u00b6\nIf you have questions we haven't addressed, please reach out to us at contact@zensical.org. We're currently collecting questions from the community about Zensical, and will address them in an FAQ section as part of our documentation in the coming weeks. We're incredibly thankful that you have been part of our journey so far. With Zensical, we're embarking on a new chapter, and we couldn't be more excited to have you with us. You can subscribe to our newsletter to stay in the loop.",
    "The Manuscripts of Edsger W. Dijkstra",
    "The Manuscripts of Edsger W. Dijkstra Home\nNumerical EWD Index: 00xx 01xx 02xx 03xx 04xx 05xx 06xx 07xx 08xx 09xx 10xx 11xx 12xx 13xx\nBibTeX index\nMC Reports\nOther documents\nTranscriptions\nVideo and Audio\nExternal links\nIn addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry. During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award.",
    "During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award. Like most of us, Dijkstra always believed it a scientist\u2019s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as \u201cEWDs\u201d, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands.",
    "Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands. Although most of Dijkstra\u2019s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you\u2019ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable. The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin. Each manuscript file is accessible through either of two indexes:\n0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for.",
    "These contain titles only, but are faster if you know what you\u2019re looking for. 0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for. EWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica) PhD thesis (5.3 MB) Other documents\nEWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)\nPhD thesis (5.3 MB)\nYou can find a table relating EWD numbers to publication years here. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.",
    "Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired. A few of the manuscripts written in Dutch have been translated into English, and one \u2014EWD1036\u2014 has been translated into Spanish. EWD28 has been translated from English into Russian. For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).",
    "Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations). Proofreading Each transcription gets a cursory scan as it\u2019s prepared for uploading, but since a web page can always be updated, I don\u2019t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please\nA compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it. Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn\u2019t provide the backward references, we indicate the relationship by \"see also\" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to",
    "Any reader who notices such a relationship is invited to We have begun adding summaries of the EWDs. This innovation was suggested by G\u00fcnter Rote, who contributed the first dozen summaries. Additional contributions of summaries\u2014especially summaries in English of EWDs in Dutch\u2014are most welcome. Copyrights in most EWDs are held by his children, one of whom \u2014 \u2014 handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder. Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Center\u2019s policies are also applicable. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.",
    "In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on \u201cDijkstra\u2019s sense of what computer science and programming are and what they aren\u2019t.\u201d\nAnother interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.",
    "A transcript is available in the on-line collection of the Charles Babbage Institute. Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute. To mark the occasion of Dijkstra\u2019s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposium\u2019s program (10 MB) contains an outline of Dijkstra\u2019s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration.",
    "Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it). In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend. A blog devoted to Dijkstra\u2019s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011.",
    "An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011. In his blog A Programmer\u2019s Place, Maarten van Emden has an entry entitled \u201cAnother scoop by Dijkstra?\u201d. The entry describes Dijkstra\u2019s \u201cremarkable insight [in \u201cNotes on Structured Programming\u201d (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)\u201d by applying the Assembly-line Principle. The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.",
    "The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021. The folks who contributed most significantly to the site\u2019s creation are acknowledged here. Comments and suggestions about the site are always welcome; please email them to the\nIf you find this site interesting, you may also be interested in another site:\nDiscipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others). Revised 2020-01-12",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology How I spent two decades tracking down the creators of a 1987 USENET game and learned modern packaging tools in the process. The Discovery: A Digital Time Capsule from 1987\nPicture this: October 26, 1987. The Berlin Wall still stands, the World Wide Web is just text, and software is distributed through USENET newsgroups in text files split across multiple posts. On that day, Edward Barlow posted something special to comp.sources.games\n:\n\u201cconquest \u2013 middle earth multi-player game, Part01/05\u201d\nThat\u2019s how Ed Barlow announced it at the time, before quickly changed the name to Conquer. This was Conquer \u2013 a sophisticated multi-player strategy game that would influence countless others. Players controlled nations in Middle Earth, managing resources, armies, magic systems, and diplomatic relations. What made it remarkable wasn\u2019t just the gameplay, but how it was built and distributed in an era when \u201copen source\u201d wasn\u2019t even a term yet. Chapter 0: University Days.",
    "Chapter 0: University Days. Chapter 0: University Days. It was during these days, in the middle of the 90s, that my fellow students and I spent hours experimenting with terminals in the Computer Unix Labs, USENET, links, news, msgs, and of course: conquer. That game was a gem that required to be the leader of a country, and with a map representing as characters each player could control their elven kingdom, orcish empire, or human armies to fight each other while controlling all the details of the economy. But by 2006, this piece of computing history was trapped in legal limbo. Chapter 1: The Quest Begins (2006)\nAs a university student in Spain in the early \u201990s, I\u2019d encountered Conquer in the Unix labs. Fast forward to 2006, and I realized this pioneering game was at risk of being lost forever. The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution.",
    "The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution. I started what I thought would be a simple project: get permission from the original authors to relicense the code under GPL so it could be properly preserved and packaged for modern Linux distributions. Simple, right? Chapter 2: Digital Detective Work\nFinding Edward Barlow and Adam Bryant in 2006 was like archaeological work. Email addresses from the 1980s were long dead. USENET posts provided few clues. I scoured old university directories, googled fragments of names, and followed digital breadcrumbs across decades-old forums. The breakthrough came through pure persistence and a bit of luck. After months of searching, I managed to contact Ed Barlow. His response was refreshingly casual: \u201cYes i delegated it all to adam aeons ago. Im easy on it all\u2026. copyleft didnt exist when i wrote it and it was all for fun so\u2026\u201d\nBut there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)",
    "Chapter 3: The Long Wait (2006-2011) But there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)\nI documented everything on the Debian Legal mailing lists, created a GNU Savannah task (#5945), and even wrote blog posts hoping Adam would find them. The legal experts were clear: I needed explicit written permission from both copyright holders. Years passed. The project stalled. Then, on February 23, 2011, something magical happened. My phone buzzed with a contact form submission:\n\u201cI heard news of the request to release the code. I grant permission to release the code under GPL.\u201d \u2013 Adam Bryant\nHe had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)",
    "Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025) He had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)\nFast forward to 2025, and Stephen Smoogen contacts me about my relicesing efforts in 2006 and how he was particularly interested in reviving: Conquer Version 5 \u2013 a complete rewrite by Adam with advanced features like automatic data conversion, enhanced stability, and sophisticated administrative tools. This wasn\u2019t just an update; it was a complete reimagining of the game. But V5 had a different legal history. In the \u201990s, there had been commercial arrangements. Would Adam agree to GPL this version too? His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic",
    "His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic\nJust when I thought the story was complete, I discovered another contributor: MaF, who had created PostScript utilities for generating printable game maps \u2013 a crucial feature in the pre-GUI era when players needed physical printouts to strategize. Tracking down MaF in 2025 led me to his company, where he\u2019s now Director of Product Security. His response: \u201cOh, that was a long time ago. But yes, that was me. And I have no problem with relicensing it to GPL.\u201d\nRichard Caley: More Than Just a Legal Footnote\nBut not all searches end with an answer. Some end with silence. My investigation of Richard Caley followed the same digital breadcrumbs. I traced him to the University of Edinburgh, where he worked on speech synthesis. I found his technical contributions to FreeBSD. But the trail went cold around 2005.",
    "But the trail went cold around 2005. Then I found him \u2013 not in a USENET archive, but on the front page of his own website, preserved exactly as he left it in web.archive.org. \u201cRichard Caley suffered a fatal heart attack on the 22nd of April, 2005. He was only 41, but had been an undiagnosed diabetic, probably for some considerable time. His web pages remain as he left them.\u201d\nReading those words felt different from finding a historical record. This wasn\u2019t archival research \u2013 this was walking into someone\u2019s house years after they\u2019d gone and finding a note on the table. The page continued:\n\u201cOver and above his tremendous ability with computers and programming, Richard had a keen mind and knowledge of an extraordinary range of topics, both of which he used in frequent contributions to on-line discussions. Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code",
    "Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code The \u201cCaleyisms\u201d \u2013 The Man Behind the Code\nAnd then I discovered his \u201cCaleyisms\u201d \u2013 a curated collection of his most brilliant USENET responses that revealed not just a programmer, but a person:\nWhat\u2019s a shell suit? \u201cOil company executive.\u201d\nHow do you prepare for a pyroclastic flow hitting Edinburgh? \u201cHang 1000 battered Mars bars on strings and stand back?\u201d\nOn his book addiction:\n\u201cI never got the hang of libraries, they keep wanting the things back and get upset when they need a crowbar to force it out of my hands.\u201d\nHis humor was dry, intelligent, and uniquely British. In technical discussions, he could be brutally precise:\n\u201cLack of proper punctuation, spacing, line breaks, capitalisation etc. is like bad handwriting, it doesn\u2019t make it impossible to read what was written, just harder. But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved",
    "But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved A Digital Office Preserved\nExploring his preserved website felt like walking through his digital office. The directory structure revealed his passions: FreeBSD how-tos, POVRAY experiments, wallpaper images, technical projects. His self-deprecating humor shone through in his \u201cAbout\u201d section:\n\u201cThankfully I don\u2019t have a photograph to inflict on you. Just use the picture of Iman Bowie to the left and then imagine someone who looks exactly the opposite in every possible way. This probably explains why she is married to David Bowie and I\u2019m not.\u201d\nHere was a complete person \u2013 technical director at Interactive Information Ltd, speech synthesis researcher, FreeBSD enthusiast, Kate Bush fan, and a wit who brightened countless online discussions. The legal reality was harsh: Richard\u2019s contributions to Conquer couldn\u2019t be relicensed. The university couldn\u2019t help contact heirs due to privacy laws. His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)",
    "His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O) His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)\n\\_/@@\\\n\\\\~~/\n~~\n- RJC RIP\nIn the Conquer project documentation, Richard Caley isn\u2019t remembered as a \u201cproblem case\u201d or \u201cunlicensable code.\u201d He\u2019s honored as the vibrant person he was \u2013 the brilliant mind behind the \u201cCaleyisms,\u201d the researcher who contributed to speech synthesis, the FreeBSD advocate, and the witty participant in early online communities whose words continue to amuse and inform, decades after he wrote them. Chapter 6: Modern Renaissance \u2013 Enter GitHub, CICD and Modern Distributions\nHere\u2019s where the story gets really interesting. While working on preserving these Unix classics, I decided to learn modern packaging techniques. I chose to implement both APK (Alpine Linux) and Debian packaging for the games.",
    "I chose to implement both APK (Alpine Linux) and Debian packaging for the games. For APK packages, I used Melange \u2013 a sophisticated build system that creates provenance-tracked, reproducible packages for the Wolfi \u201cundistro\u201d. The irony? I discovered this tool when some friend started to work for the company that created it. Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite",
    "Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite - Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite\nOriginal Conquer v4 code, by Ed Barlow and Adam Bryant\n(Conquer running in docker container alongside Apache, Curses to WebSockets output thanks to ttyd. Now we can play through the web!) Conquer Version 5 \u2013 The evolution of the classical Conquer, by Adam Bryant\nChapter 8: The Human Element: Why This Matters\nThis isn\u2019t just about preserving old games \u2013 it\u2019s about preserving the story of computing itself. Ed Barlow and Adam Bryant were pioneers who built sophisticated multiplayer experiences when most people had never heard of the internet. They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community.",
    "They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community. Martin Forssen\u2019s PostScript utilities represent the ingenuity of early developers who solved problems with whatever tools were available. Want to visualize your game state? Write a PostScript generator! The 20-year relicensing effort demonstrates something crucial about open source: it\u2019s not just about code, it\u2019s about community and continuity. Every time someone maintains a legacy project, documents its history, or tracks down long-lost contributors, they\u2019re weaving the threads that connect computing\u2019s past to its future. Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure",
    "Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure - Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure\n- Modern tools can revive ancient code: Melange and modern CI/CD gave 1987 software a 2025 renaissance\nThe Continuing Story\nBoth Conquer games are now fully GPL v3 licensed and available with modern packaging. They represent not just playable software, but a complete case study in software archaeology, legal frameworks for preservation, and the evolution of development practices across four decades. The next chapter? Teaching these classic strategy games to a new generation of developers and gamers, while demonstrating that proper legal frameworks and modern tooling can give any historical software a second life. Sometimes the best way to learn cutting-edge technology is by applying it to preserve computing history. What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators?",
    "Have you ever traced the lineage of code back to its original creators? What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators? #FreeSoftware #OpenSource #SoftwarePreservation #Unix #GNU #Linux #Packaging #Melange #TechHistory #GameDevelopment #Unix #USENET #GPL #FST #Debian #ncurses #terminal #shell\nRead this article in Spanish / Lee este art\u00edculo en espa\u00f1ol:\nhttps://vejeta.com/conquer-una-odisea-de-20-anos-en-arqueologia-digital/\nThis article was originally written in both English and Spanish, with additional insights and cultural context in the Spanish version.",
    "Visualize FastAPI endpoints with FastAPI-Voyager\n\nLoading\u2026\nFastAPI Voyager\n{{ state.version }}\nscroll to zoom in/out\ndouble click node to view details. shift + click to see schema's dependencies without unrelated nodes. {{ tag.name }}\n{{ tag.routes.length }}\n{{ route.name }}\nNo routes\n{{ dumpJson }}\nImport core data JSON",
    "Email verification protocol",
    "Email verification protocol Verifying control of an email address is a frequent activity on the web today and is used both to prove the user has provided a valid email address, and as a means of authenticating the user when returning to an application. Verification is performed by either:\n-\nSending the user a link they click on or a verification code. This requires the user to switch from the application they are using to their email address and having to wait for the email arrive, and then perform the verification action. This friction often causes drop off in users completing the task. There are privacy implications as the email transmission informs the mail service the applications the user is using and when they used them. -",
    "- -\nThe user logs in with a social login provider such as Apple or Google that provide a verified email address. This requires the application to have set up a relationship with each social provider, and the user to be using one of those services and wanting to share the additional profile information that is also provided in the OpenID Connect flow. The Email Verification Protocol enables a web application to obtain a verified email address without sending an email, and without the user leaving the web page they are on. To enable the functionality, the mail domain delegates email verification to an issuer that has authentication cookies for the user. When the user provides an email to the HTML form field, the browser calls the issuer passing authentication cookies, the issuer returns a token, which the browser verifies and updates and provides to the web application. The web application then verifies the token and has a verified email address for the user.",
    "The web application then verifies the token and has a verified email address for the user. User privacy is enhanced as the issuer does not learn which web application is making the request as the request is mediated by the browser. -\nSD-JWT+KB token: The selective disclosure json web token with key binding is specified in Selective Disclosure for JWT. This protocol does not use the selective disclosure features, it uses the key binding feature which enables a separation of token issuance and token presentation. The SD-JWT+KB is a token composed of two JWTs separated by the\n~\ncharacter. The first JWT is an SD-JWT aka the issuance token and is signed by the issuer and contains theemail\nandemail_verified\nclaims for the user, and the public key used by the browser to make the request. The second JWT is a KB token and is signed by the browser and contains a hash of the first JWT. The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application -",
    "The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application - Issuer: The service that verifies the user controls an email address. A DNS record for the email domain delegates email verification to the issuer. The issuer serves a\n.well-known/email-verification\nmetadata file that contains itsissuance_endpoint\nthat is called to obtain an issuance token, and itsjwks_uri\nthat points to the JWKS file containing the public keys used to verify the SD-JWT. The issuer is identified by its domain, an eTLD+1 (egissuer.example\n). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer.",
    "This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. ). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. Verified Email Release: The user navigates to any website that requires a verified email address and an input field to enter the email address. The user focusses on the input field and the browser provides one or emails for the user to select based on emails the user has provided previously to the browser. The user selects a verified email and the app proceeds having obtained the verified email. Are emails that can be verified decorated by the browser in the autocomplete UI? What UX is presented to the user when the app gets a verified email so the user knows it is already verified? sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site",
    "sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site participant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site\nRP->>RPS: Nonce request\nRPS->>RPS: Generate nonce, bind to session\nRPS->>RP: Nonce\nRP->>B: Display page\nNote over U,DNS: Step 2: Email Selection\nU->>RP: Focus on email input field\nRP->>B: Input field focused\nB->>U: Display email address list\nU->>B: Select email address\nNote over U,DNS: Step 3: Token Request\nB->>DNS: DNS TXT lookup<br/>_email-verification.$EMAIL_DOMAIN\nDNS->>B: Return iss=issuer.example\nB->>I: GET /.well-known/email-verification\nI->>B: Return metadata\nB->>B: Generate key pair<br/>Create request token\nB->>I: POST request_token=JWT... Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS",
    "Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS I->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS\nB->>B: Create KB\nB->>RP: Provide SD-JWT+KB\nNote over U,DNS: Step 6: Token Verification\nRP->>RPS: Send SD-JWT+KB\nRPS->>RPS: Parse SD-JWT+KB\nRPS->>DNS: DNS TXT lookup for email domain\nDNS->>RPS: Return iss=issuer.example\nRPS->>I: GET /.well-known/email-verification\nI->>RPS: Return metadata with jwks_uri\nRPS->>I: GET jwks_uri\nI->>RPS: Return JWKS public keys\nRPS->>RPS: Verify SD-JWT\nRPS->>RPS: Verify KB-JWT\nRPS->>RP: Email verification complete\nUser navigates to a site that will act as the RP. -\n1.1 - the RP Server generates a nonce and binds the nonce to the session. -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token",
    "If the browser receives anissuance_token -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token\nper 4.4 below, then it sends aemailverifed\nevent that has apresentationToken\nproperty. Following is an example of the HTML in the page:\n<input id=\"email\"\ntype=\"email\"\nautocomplete=\"email\"\nnonce=\"12345677890..random\">\n<script>\nconst input = document.getElementById('email')\ninput.addEventListener('emailverified', e => {\n// e.presentationToken is SD-JWT+KB\nconsole.log({\npresentationToken: e.presentationToken\n})\n})\n</script>\nAuthors are exploring alternative HTML and JS API approaches\n-\n2.1 - User focusses on email input field\n-\n2.2 - The browser displays the list of email addresses it has for the user. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field.",
    "- 2.3 - User selects an email address from browser selection, or the user types an email into the field. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field. Future: allow user to type in a field so we learn about new emails, or if the user does not want the browser to remember emails, the Email Verification Protocol is still available. In the future when we allow the user to use a passkey to authenticate to the issuer, the user can provide a verified email to a web application using a public computer by authenticating with their passkey and not enter any secrets into the public computer. If the RP has performed (1):\n- 3.1 - the browser parses the email domain ($EMAIL_DOMAIN) from the email address, looks up the\nTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record",
    "example record . The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record\n_email-verification.email-domain.example TXT iss=issuer.example\nThis record states that email-domain.example\nhas delegated email verification to the issuer issuer.example\n. If the email domain and the issuer are the same domain, then the record would be:\n_email-verification.issuer.example TXT iss=issuer.example\nAccess to DNS records and email is often independent of website deployments. This provides assurance that an issuer is truly authorized as an insider with only access to websites on\nissuer.example\ncould setup an issuer that would grant them verified emails for any email atissuer.example\n. - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer.",
    "- 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. . - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. For example, https://issuer.example/.well-known/email-verification\nmay redirect to https://accounts.issuer.example/.well-known/email-verification\n. -\n3.3 - the browser confirms that the\n.well-known/email-verification\nfile contains JSON that includes the following properties: -\nissuance_endpoint - the API endpoint the browser calls to obtain an SD-JWT\n-\njwks_uri - the URL where the issuer provides its public keys to verify the SD-JWT\n-\nsigning_alg_values_supported - OPTIONAL. JSON array containing a list of the JWS signing algorithms (\"alg\" values) supported by both the browser for request tokens and the issuer for issued tokens. The same algorithm MUST be used for both the\nrequest_token\nandissuance",
    "The same algorithm MUST be used for both the\nrequest_token\nandissuance request_token\nandissuance\nwithin a single issuance flow. Algorithm identifiers MUST be from the IANA \"JSON Web Signature and Encryption Algorithms\" registry. If omitted, \"EdDSA\" is the default. \"EdDSA\" SHOULD be included in the supported algorithms list. The value \"none\" MUST NOT be used. Each of these properties MUST include the issuer domain as the root of their hostname. Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token",
    "Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token jwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token\n- email - email address to be verified\nThe browser SHOULD select an algorithm from the issuer's signing_alg_values_supported\narray, or use \"EdDSA\" if the property is not present. An example JWT header:\n{\n\"alg\": \"EdDSA\",\n\"typ\": \"JWT\",\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\" // base64url-encoded public key\n}\n}\ndo we want to register a new JWT\ntyp\nAn example payload\n{\n\"aud\": \"issuer.example\",\n\"iat\": 1692345600,\n\"email\": \"user@example.com\"\n}\n- 3.5 - the browser POSTs to the\nissuance_endpoint\nof the issuer with 1P cookies with a content-type ofapplication/x-www-form-urlencoded\ncontaining arequest_token\nparameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=... parameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail jwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim contains a syntactically valid email address\n-\n4.3 - the issuer checks if the cookies sent represent a logged in user, and if the logged in user has control of the email provided in the request_token. If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0",
    "If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0 fieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0\n- Signature: MUST be signed with the issuer's private key corresponding to a public key in the\njwks_uri\nidentified bykid\n- Header: MUST contain\nExample header:\n{\n\"alg\": \"EdDSA\",\n\"kid\": \"2024-08-19\",\n\"typ\": \"evp+sd-jwt\"\n}\nExample payload:\n{\n\"iss\": \"issuer.example\",\n\"iat\": 1724083200,\n\"cnf\": {\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\"\n}\n},\n\"email\": \"user@example.com\",\n\"email_verified\": true\n}\nThe resulting JWT has the ~\nappended to it, making it a valid SD-JWT. - 4.4 - the issuer returns the SD-JWT to the browser as the value of\nissuance_token\nin anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}",
    "Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"} in anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}\nIf the issuer cannot process the token request successfully, it MUST return an appropriate HTTP status code with a JSON error response containing an error\nfield and optionally an error_description\nfield. When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid.",
    "When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. {\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:",
    "When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability: HTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:\nHTTP 500 Internal Server Error\n{\n\"error\": \"server_error\",\n\"error_description\": \"Temporary server error, please try again later\"\n}\nIn a future version of this spec, the issuer could prompt the user to login via a URL or with a Passkey request. On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the",
    "On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT",
    "email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT : \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT\n- Header:\n- signing the KB-JWT with the browser's private key (the same key pair generated in step 3.4)\n- concatenating the SD-JWT and the KB-JWT separated by a tilde (~) to form the SD-JWT+KB\nExample KB-JWT header:\n{ \"alg\": \"EdDSA\", \"typ\": \"kb+jwt\" }\nExample KB-JWT payload:\n{ \"aud\": \"https://rp.example\", \"nonce\": \"259c5eae-486d-4b0f-b666-2a5b5ce1c925\", \"salt\": \"kR7fY9mP3xQ8wN2vL5jH6tZ1cB4nM9sD8fG3hJ7kL2p\", \"iat\": 1724083260, \"sd_hash\": \"X9yH0Ajrdm1Oij4tWso9UzzKJvPoDxwmuEcO3XAdRC0\" }\n-\n5.3 - the browser sets a TBD hidden field and fires the TBD event ...\ndetails TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n- details TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by: iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-",
    "iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n- iss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-\n6.5 - the RP verifies the KB-JWT signature using the public key from the\ncnf\nclaim in the SD-JWT with thealg\nalgorithm from the KB-JWT header\nBelow are notes capturing some discussions of potential privacy implications. -\nThe email domain operator no longer learns which applications the user is verifying their email address to as the applications are no longer sending an email verification code to the user. By using an SD-JWT+KB, the browser intermediates the request and response so that the issuer does not learn the identity of the RP. -\nThe RP can infer if a user is logged into the issuer as the RP receives a SD-JWT when the user is logged in, and does not when the user is not logged in. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had.",
    "-\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. The web page would call an API passing the email address and nonce. It would return a promise that resolves to the SD_JWT or an error response. The API would only be callable after a user gesture such as clicking a button labelled verify on the web page. This provides the web page in more flexibility in how to gather the email address. For example, if the web page is using EVP for login, and the user has used different emails for login and those are stored in cookies, the page can display the list of emails and an option to provide a different one. The user can then select the email they want to use rather than having to type it into a text field.",
    "The user can then select the email they want to use rather than having to type it into a text field. In addition to, or instead of the browser sending cookies to the Issuer, the Issuer could return a WebAuthN request to the browser if it has credentials for the user identified by the email address. The browser would then interact with the user and provide the WebAuthN response to the Issuer, authenticating the user, and the Issuer would then return the SD-JWT. Rather than the DNS TXT record, the Mail Domain would host a JSON file in the .wellknown domain. This creates challenges for the long tail of individually owned domains:\n- would require a domain that is used just for email to now have to support a web server\n- the mail domain is usually an apex domain, which does not support CNAME, complicating hosting a web site",
    "Using bubblewrap to add sandboxing to NetBSD",
    "Using bubblewrap to add sandboxing to NetBSD Google Summer of Code 2025 Reports: Using bubblewrap to add sandboxing to NetBSD\nThis report was written by Vasyl Lanko as part of Google Summer of Code 2025. Introduction\nAs of the time of writing, there is no real sandboxing technique available to NetBSD. There is chroot, which can be considered a weak sandbox because it modifies the root directory of the process, effectively restricting the process' view of the file system, but it doesn't isolate anything else, so all networking, IPC, and mounts inside this restricted file system are the same as of the system, and are accessible. There has already been some research on implementing kernel-level isolation in NetBSD with tools like gaols, mult and netbsd-sandbox, but they haven't been merged to NetBSD. Other operating systems have their own ways to isolate programs, FreeBSD has jails, and Linux has namespaces. Project Goals",
    "Project Goals Project Goals\nThe goal of this project is to bring a new way of sandboxing to NetBSD. More specifically, we want to implement a mechanism like Linux namespaces. These namespaces allow the isolation of parts of the system from a namespace, or, as the user sees it, from an application. NetBSD has compat_linux to run Linux binaries on NetBSD systems, and the implementation of namespaces can also be utilized to emulate namespace-related functionality of Linux binaries. A simple example to visualize our intended result is to consider an application running under an isolated UTS namespace that modifies the hostname. From the system's view, the hostname remains the same old hostname, but from the application's view it sees the modified hostname. Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details",
    "Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details - UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details\n- mount namespace, it is a prerequisite to most other namespace types because UNIX follows the philosophy of \"everything is a file\", so we need a separate mount namespace to have different configuration files on the same location as the system. Linux creates namespaces via the unshare or clone system calls, and it will also be our way of calling the namespace creation logic. We setup the base for implementing Linux namespaces in the NetBSD kernel using kauth, the subsystem managing all authorization requests inside the kernel. It associates credentials with objects, and because the namespace lifecycle management is related to the credential lifecycle it handles all the credential inheritance and reference counting for us. (Thanks kauth devs!)",
    "(Thanks kauth devs!) We separate the implementation of each namespace in a different secmodel, resulting in a similar framework to Linux which allows the isolation of a single namespace type. Our implementation also allows users to pick whether they want to have namespace support, and of what kind, via compilation flags, just like in Linux. UTS namespace\nUTS stands for UNIX Timesharing System, because it allows multiple users to share a single computer system. Isolating the utsname\ncan be useful to give users the illusion that they have control over the system's hostname, and also, for example, to give different hostnames to virtual servers. The UTS namespace stores the namespace's hostname, domain name, and their lengths. To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace.",
    "To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace. This namespace specific information needs to be saved somewhere, and for that we use the credential's private_data\nfield, so we can use a UTS_key\nto save and retrieve UTS\nrelated information from the secmodel. The key specifies the type of information we want to retrieve from the private_data\n, hence using a UTS_key\nfor the UTS namespace. The key for each namespace is a fixed value (we don't create a new key for every credential), but the retrieved value for that key from different credentials may be different. We had to modify kernel code that was directly accessing the hostname\nand domainname\nvariables, to instead call get_uts()\n, which retrieves the UTS struct for the namespace of the calling process. We didn't modify occurrences in kernel drivers because drivers are not part of any namespace, so they should still access the system's resources directly. MNT namespace",
    "MNT namespace MNT namespace\nThe MNT namespace isolates mounts across namespaces. It is used to have different versions of mounted filesystems across namespaces, meaning a user inside a mount namespace can mount and unmount whatever they want without affecting or even breaking the system. The mount namespace structure in Linux is fairly complicated. To have something similar in NetBSD we need to be able to control the mounts accessed by each namespace, and for that we need to control what is each namespace's mountlist, this is also enough for unmounting file systems, because in practice we can just hide them. For the mount_namespace, mountlist structure and the number of credentials using the mount namespace are stored in the credential's private data with the MNT_key\n. Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()",
    "Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist() . Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()\nwhich returns the correct mountlist for the namespace the calling process resides in. Implementation for the mount namespace is immensely more complex than for the UTS namespace, it involves having a good understanding of both Linux and NetBSD behaviour, and I would frequently find myself wondering how to implement something after reading the Linux man pages, which would lead to me looking for it in the Linux source code, understanding it, then going back to NetBSD source code, trying to implement it, and seeing it's too different to implement in the same way. Project Status\nYou can find all code written during this project in GitHub at maksymlanko/netbsd-src gsoc-bubblewrap\nbranch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap",
    "Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap branch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap\nbranch and this was the last one for the mnt_ns\nstill WIP branch. The link includes implementation of general namespace code via secmodels, implementation of the UTS namespace and related ATF-tests, and the work-in-progress implementation of mount namespaces. The mount namespace functionality is not finished as it would require much more work than the time available for this project. To complete it, it would be required invasive and non-trivial changes to the original source code, and, of course, more time. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement.",
    "Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. I believe that after mount namespaces it would be interesting to implement PID namespaces as this in combination with mount namespaces would permit process isolation from this sandbox. Afterwards, implementing user namespaces would allow users to get capabilities similar to root\nin the namespace, giving them sudo\npermissions while still restricting system-wide actions like shutting down the machine. A lower hanging fruit is to implement the namespace management functionality, which in Linux is lsns to list existing namespaces, and setns to move the current process to an already existing namespace. Challenges\n- Semantics. Did you know the unmount system call with MNT_FORCE flag in Linux (usually) returns EBUSY, and in NetBSD it forces the unmounting? One of them makes it easier to implement mount namespaces.",
    "One of them makes it easier to implement mount namespaces. - The behaviour of namespaces is not fully specified in the man pages. If something is not clear from the man pages you need to read the source code. - Unexpected need to learn a lot of VFS concepts and their differences in NetBSD and Linux. - There was a much bigger research component than I anticipated. In the end, Linux and NetBSD are different operating systems, implemented in different ways. Linux is complex and it is not trivial to port namespaces to NetBSD. Notes\nThe project is called \"Using bubblewrap to add sandboxing to NetBSD\" and was initially projected to emulate the unshare\nsystem call into compat_linux\n, but, seeing that having namespaces could be useful for NetBSD, and that it would be easy to add to compat_linux\nafterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap",
    "Implementing other system calls necessary to make the bwrap afterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap\nlinux binary work correctly also wouldn't be as satisfying as implementing namespaces directly into NetBSD, so this was why the project was initially called \"Using bubblewrap to add sandboxing to NetBSD\" but nowadays it would be more accurate to call it \"Sandboxing in NetBSD with Linux-like namespaces\". Thanks\nI am very grateful to Google for Google Summer of Code, because without it I wouldn't have learned so much this summer, wouldn't have met with smart and interesting people, and for sure wouldn't have tried to contribute to a project like NetBSD, even if I always wanted to write operating systems code... But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects.",
    "But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects. I would also like to thank the members of the NetBSD organization for helping me throughout this project, and more specifically:\n- Taylor R. Campbell, Harold Gutch and Nia Alarie from IRC, for helping me fix a nasty\nLD_LIBRARY_PATH\nbug I had on my system which wouldn't let me finish compiling NetBSD, and general GSoC recomendations. - Emmanuel Dreyfus from\ntech-kern\n, with whom I discussed ideas for projects and proposal suggestions, and in the end inspired the namespaces project. - Christoph Badura and Leonardo Taccari who volunteered to be my mentors. They took time to research and answer my questions, anticipated possible problems in my approaches, and always pointed me in the right direction, daily, during all of GSoC's period. This project is from the 3 of us.",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law Montana has made history as the first state in the U.S. to legally protect its citizens\u2019 right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law. The groundbreaking legislation affirms Montanans\u2019 fundamental right to own and operate computational resources \u2014 including hardware, software, and AI tools \u2014 under the state\u2019s constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world. \u201cMontana is once again leading the way in defending individual liberty,\u201d said Senator Daniel Zolnikov, the bill\u2019s sponsor and a longtime advocate for digital privacy. \u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d",
    "\u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law. The act also includes provisions for AI-controlled critical infrastructure, requiring both a \u201cshutdown mechanism\u201d to allow human control and annual safety reviews \u2014 a move aimed at balancing innovation with public safety concerns. The bill has drawn praise from privacy advocates and tech policy groups. Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d",
    "Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d The MRTCA stands in stark contrast to recent regulatory efforts in other states, such as California, Virginia, and New York, where proposals to rein in AI technologies have either failed or been heavily revised. Montana\u2019s approach leans toward empowering individual users rather than restricting access. The law has already inspired similar efforts in New Hampshire, where lawmakers are pushing a constitutional amendment guaranteeing access to computation. Rep. Keith Ammon, the state\u2019s Majority Floor Leader, praised Montana\u2019s leadership: \u201cThis is the kind of bold move that sets the tone for the rest of the country.\u201d\nNationally, the Right to Compute movement is gaining traction. Spearheaded by the grassroots group RightToCompute.ai, the campaign argues that computation \u2014 like speech and property \u2014 is a fundamental human right. \u201cA computer is an extension of the human capacity to think,\u201d the organization states.",
    "\u201cA computer is an extension of the human capacity to think,\u201d the organization states. The movement is supported by Haltia.AI, a Dubai-based AI startup, and the ASIMOV Protocol, a blockchain consortium advocating for decentralized AI infrastructure. Talal Thabet, Co-Founder of both groups, praised Montana\u2019s law as \u201ca monumental step forward in ensuring individuals retain control of their own data and digital tools.\u201d\nAs debates over AI governance and digital rights continue to evolve, Montana\u2019s bold new law could serve as a blueprint for other states seeking to safeguard freedom in the digital era.",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team Zensical \u2013 A modern static site generator built by the Material for MkDocs team\u00b6\nWe are thrilled to announce Zensical, our next-gen static site generator designed to simplify the process of building documentation sites. Distilled from a decade of experience, Zensical is our effort to overcome the technical limitations of MkDocs, reaching far beyond its capabilities. Zensical is the result of thousands of hours of work \u2013 built from the ground up for a modern and comfortable authoring experience, while making it easy for developers to extend and customize Zensical through its upcoming module system. Our goal is to support docs-as-code workflows with tens of thousands of pages, without compromising performance or usability. To make the transition seamless, compatibility comes first. We're putting significant effort into ensuring a smooth migration from Material for MkDocs for all users. Zensical can natively read mkdocs.yml",
    "Zensical can natively read mkdocs.yml , allowing you to build your existing project with minimal changes. As of now, a subset of plugins is supported, and we're working on feature parity in the coming months. Zensical is fully Open Source, licensed under MIT, and can be used for any purpose, including for commercial use. We're also saying goodbye to our sponsorware model, replacing it with our new offering for professional users: Zensical Spark. This allows us to stay independent, maximizing user value, as we shape the future of Zensical together with you. You can subscribe to our newsletter to stay in the loop. This is the second article in a four-part series:\n- Transforming Material for MkDocs\n- Zensical \u2013 A modern static site generator built by the creators of Material for MkDocs. - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6",
    "- What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6 - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6\nSince its initial release in 2016, Material for MkDocs has helped tens of thousands of teams to publish and maintain reliable documentation. However, in recent years, it has become apparent that we were running up against limitations of our core dependency, MkDocs. These limitations proved impossible to overcome as they are deeply rooted in its architecture. We also mentioned in our update on our foundational work that MkDocs must be considered a supply chain risk, since it's unmaintained since August 2024. It has seen no releases in over a year and is accumulating unresolved issues and pull requests. These developments have forced us to cut our ties to MkDocs as a dependency.",
    "These developments have forced us to cut our ties to MkDocs as a dependency. In order to map out a path forward, we went back to the drawing board, talked to dozens of our professional users and thoroughly analyzed the MkDocs ecosystem. We didn't just want to create a fork or port of MkDocs, but decided to rethink static site generation from first principles. With Zensical, we are creating a modern static site generator, which is compatible with your content and customizations, and addresses MkDocs' limitations. While Material for MkDocs is built on top of MkDocs, Zensical consolidates both projects into one coherent stack, covering static site generation, theming, and customization. What you can expect today:\nAlthough we haven't reached full feature parity yet, you can already use Zensical to build your existing Material for MkDocs projects with minimal changes. You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6",
    "What you can expect\u00b6\nSolid foundation\u00b6 You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6\nOur goal with Zensical is to create a coherent and modern stack, vertically integrating all parts of the authoring experience (AX), developer experience (DX), and user experience (UX). This gives us a significant competitive advantage over solutions that overly rely on third-party frameworks and dependencies, helping us to create much more robust Open Source software. ZRX, our new differential build engine, creates a solid foundation for Zensical, and is an Open Source project of its own. It's a fresh take on making differential data flows easy to build and a joy to work with. Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster.",
    "Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster. Following the principle of architectural hoisting, we moved essential, reusable functionality into ZRX, which allows us to keep Zensical's core simple and focused on static site generation. ZRX handles the heavy lifting \u2013 differential builds, caching, and data flow orchestration. With the upcoming module system and component system, both of which are on our public roadmap, Zensical will gain more degrees of freedom in the coming months, allowing you to extend and customize Zensical in ways that were previously impossible with MkDocs. Modern design\u00b6\nZensical brings a fresh, modern design that breaks out of the Materal Design aesthetic, creating a visual foundation that is more easily brandable and adaptable to different use cases. The new design prioritizes clarity, simplicity, and usability, while having a more professional finish:",
    "The new design prioritizes clarity, simplicity, and usability, while having a more professional finish: Right now, the layout and site structure of Zensical match Material for MkDocs closely, as we're focusing on ensuring maximum compatibility. Once we finish work on our upcoming component system, we'll provide an alternative that is much more flexible and adaptable, and can be tailored to different use cases and branding requirements more easily. You can also keep the Material for MkDocs look and feel with a single line of configuration. Blazing-fast search\u00b6\nClient-side search isn't a compromise \u2013 for the vast majority of static sites, it's the best solution, since it's faster, involves zero maintenance, and doesn't require you to pay for a service. As covered in depth in the first part of this series, the current search implementation in Material for MkDocs has severe limitations, and is based on a now unmaintained library, which is why we decided to build a new search engine from scratch. It's based on the same goals as Zensical itself: performance, flexibility, and extensibility.",
    "It's based on the same goals as Zensical itself: performance, flexibility, and extensibility. Disco, our modular and blazing-fast client-side search engine, is exclusively available in Zensical. When you build your site with Zensical, your users will immediately benefit from Disco's improved ranking algorithm, as well as its filtering and aggregation capabilities:\nIn early 2026, we'll be releasing Disco as a standalone Open Source project. With the feedback of our professional users in Zensical Spark, we're going to evolve the search experience, turning Disco into a highly configurable and customizable search engine that adapts to your needs. You can subscribe to our newsletter to receive news about Disco. Authoring experience\u00b6\nSlow feedback loops can be a major pain point when writing documentation. Almost all of us know the feeling of waiting for the static site generator to finish building the site, just to see a small change reflected in the output. With Zensical, we're finally addressing this issue.",
    "With Zensical, we're finally addressing this issue. It's important to understand that we're not yet utilizing the differential capabilities of ZRX to the fullest extent, as we're forced to make several compromises to ensure maximum compatibility with Material for MkDocs at the moment. Markdown rendering needs to go through Python Markdown, which forces us to pay for extra marshalling costs. While the initial build can sometimes be slower than with MkDocs, repeated builds \u2013 especially when serving the site \u2013 are already 4 to 5x faster, as only changed files need to be rebuilt. We're also working on a new Markdown toolchain based on a CommonMark-compliant parser written in Rust, which will make Markdown processing significantly faster. We'll be tackling this as part of the upcoming component system, which we'll start working on in early 2026. Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content.",
    "Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content. Maximum compatibility\u00b6\nCompatibility with Material for MkDocs is our top priority. We understand that switching to a new static site generator can be challenging, especially for large projects with many customizations. Therefore, we've put significant effort into ensuring that Zensical understands mkdocs.yml\nconfiguration files, so that you can build your projects with minimal changes. This means your existing Markdown files, template overrides, CSS and JavaScript extensions don't need to be touched, primarily because we did not change the generated HTML, and rely on Python Markdown for processing your content. However, plugins are a different story. In MkDocs, practically all plugins have side effects, making it impossible to parallelize builds. We started from first principles and asked: what should extensibility look like in a modern static site generator? Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles:",
    "Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles: - Modules can inject, extend, and re-define functionality\n- Modules are deterministic through topological ordering\n- Modules foster reusability, with the possibility to remix them\n- Modules can cooperate through well-defined contracts\nWe're working on shipping essential functionality as provided by MkDocs plugins as built-in modules. In early 2026, we will open the module system to third-party developers, so they can start building their own modules, as we see Zensical as the heart of a thriving ecosystem. Zensical Spark\u00b6\nZensical Spark, our offering for professionals, is the result of countless calls with professional users of Material for MkDocs. From startups to large enterprises, we enable organizations to realize complex projects in diverse environments. For this, we've created Zensical Spark as a collaborative space. If you're a professional user, Zensical Spark is for you, since:\n-",
    "If you're a professional user, Zensical Spark is for you, since:\n- -\nYou can be confident that Zensical will continue to be developed and maintained in the long term as a set of interconnected and sustainable OSI-compliant Open Source projects. -\nYou can receive the support you need to successfully use, configure and customize Zensical in your organization, receiving first-class support from the Zensical team. -\nYou can influence the future development of Zensical by participating in our new approach to Open Source software development, helping us to build exactly what you need. Let's talk! If you're working in a professional context, reach out to contact@zensical.org to schedule a call and learn how Zensical Spark enables your team to transition to Zensical smoothly and have a voice in its continued development. You should also consider joining the waiting list, since seats are limited. We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical!",
    "We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! At Zensical, Tim is focusing on providing the same seamless experience for generating API reference documentation from source code (via docstrings) as he has done with mkdocstrings, the second biggest project in the MkDocs ecosystem. With his expertise, and Zensical's new stack, we'll be pushing the boundaries of what's possible with API reference documentation. Goodbye, GitHub Sponsors\u00b6\nThank you! To all of you who have supported us over the years through GitHub Sponsors \u2013 we are incredibly grateful for your support. It has been invaluable in helping us to build, maintain and evolve Material for MkDocs, and we couldn't have done it without you. Seriously, thank you!",
    "Seriously, thank you! Material for MkDocs gave us something invaluable: experience building for tens of thousands of users, and the opportunity to build a team around Open Source software. It showed us that making a living from Open Source isn't just possible \u2013 we grew it into one of the largest sponsorware projects on GitHub and inspired others to pursue similar paths. Now we're breaking new ground. Zensical is our next chapter, and we're professionalizing how we approach Open Source development. Our vision is to make Zensical free for everyone to use while building a sustainable business around it through our new approach. This transition means saying goodbye to GitHub Sponsors. It has served us exceptionally well, but as we professionalize and scale, we're making the leap from personal project to company \u2013 building a business and team that can meet the growing demands of professional users while staying true to our values. We're doubling down on Open Source, developing software for everyone.",
    "We're doubling down on Open Source, developing software for everyone. We're doubling down on Open Source, developing software for everyone. If you want to continue supporting our work, please subscribe to our newsletter. We'll be providing new methods to support us in the coming months, with the possibility of getting exclusive goodies. Looking Ahead\u00b6\nMaterial for MkDocs grew organically in a pot that eventually became too small. With Zensical, we're building on solid foundations designed to grow with us \u2013 and with you. Material for MkDocs is now in maintenance mode\nWe want to be transparent about the risks of staying on Material for MkDocs. With MkDocs unmaintained and facing fundamental supply chain concerns, we cannot guarantee Material for MkDocs will continue working reliably in the future. We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical.",
    "We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical. If documentation plays a critical role in your organization, and you're worried how this might affect your business, consider joining Zensical Spark, or feel free to schedule a call by reaching out at contact@zensical.org. Where we'll be in 12 months\u00b6\nOver the next 12 months, following our phased transition strategy, we'll reach Phase 2 and 3 \u2013 introducing our module system and component system, as well as CommonMark support. By replacing Python Markdown with a Rust-based Markdown parser, we'll unlock performance improvements and the modularity needed for flexible templating. This is where Zensical truly starts to unfold its capabilities. Zensical is already powering real projects due to extensive compatibility with Material for MkDocs. We're actively working on closing the gap to reach full feature parity. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help.",
    "If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. Connect with us\u00b6\nIf you have questions we haven't addressed, please reach out to us at contact@zensical.org. We're currently collecting questions from the community about Zensical, and will address them in an FAQ section as part of our documentation in the coming weeks. We're incredibly thankful that you have been part of our journey so far. With Zensical, we're embarking on a new chapter, and we couldn't be more excited to have you with us. You can subscribe to our newsletter to stay in the loop.",
    "I Am Mark Zuckerberg",
    "I Am Mark Zuckerberg Welcome to iammarkzuckerg.com\nNo, not THAT Mark Zuckerberg-this one's busy helping Hoosiers, not launching social networks. Relax, you haven't accidentally logged into Facebook or the Metaverse. You're on the site of Mark S. Zuckerberg, Indiana's original bearer of the name, proud bankruptcy attorney, and frequent recipient of confused emails from people seeking tech support or handouts of money. What I Really Do:\n- Help people obtain a fresh financial start (no passwords required)\n- Offer dependable, human-involved advice (my artificial intelligence is powered by coffee)\n- Answer local legal questions, not privacy scandals\nReal Zuckerberg Facts:\n- Shares a name, not fortune, with the Facebook founder\n- Gets mistaken daily for a tech billionaire\n- Has written zero social media apps, but plenty of court briefs\nFun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place!",
    "But if you need trustworthy bankruptcy help, you're in exactly the right place! Fun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place! Click around, get to know your (non-billionaire) local Mark, and remember: No login required. Click Here to See How Other\nWebsites Have Reacted to This\nInteresting Things That Have Happened to Me Because My Name is Mark Zuckerberg\nFor a complete list of things that have happened to Mark Zuckerberg click here\nLike I said, I don't wish Mark E. Zuckerberg any ill will at all. I hope the best for him, but let me tell you this: I will rule the search for \"Mark Zuckerberg bankruptcy\". And if he does fall upon difficult financial times, and happens to be in Indiana, I will gladly handle his case in honor of our eponymy.",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel Ironclad is a (partially) formally verified, real-time capable, UNIX-like operating system kernel for general-purpose and embedded uses. It is written in SPARK and Ada, and is comprised of 100% free software. Ironclad features a familiar POSIX-compatible interface, true simultaneous preemptive multitasking, Mandatory Access Control (MAC), and support for hard real-time scheduling. Ironclad is fully open source and distributed under the GPLv3, ensuring it remains free. No firmware blobs are needed or shipped with the kernel. Every piece of the stack is open source. SPARK's state of the art formal verification is employed for ensuring absence of errors and correctness of big portions of Ironclad, like cryptography, MAC, and user-facing facilities. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation.",
    "Dependency on only the GNU toolchain allows for easy cross-compilation. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation. Ironclad will always be free for use, study, and modification, so, to support the project, we rely on the use of donations and grants. Every contribution makes a difference and allows us to do more. This project is funded through NGI Zero Core, a fund established by NLnet with financial support from the European Commission's Next Generation Internet program. Learn more at the NLnet project page. Additionally, we would like to thank the following organizations:",
    "The Manuscripts of Edsger W. Dijkstra",
    "The Manuscripts of Edsger W. Dijkstra Home\nNumerical EWD Index: 00xx 01xx 02xx 03xx 04xx 05xx 06xx 07xx 08xx 09xx 10xx 11xx 12xx 13xx\nBibTeX index\nMC Reports\nOther documents\nTranscriptions\nVideo and Audio\nExternal links\nIn addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry. During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award.",
    "During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award. Like most of us, Dijkstra always believed it a scientist\u2019s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as \u201cEWDs\u201d, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands.",
    "Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands. Although most of Dijkstra\u2019s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you\u2019ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable. The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin. Each manuscript file is accessible through either of two indexes:\n0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for.",
    "These contain titles only, but are faster if you know what you\u2019re looking for. 0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for. EWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica) PhD thesis (5.3 MB) Other documents\nEWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)\nPhD thesis (5.3 MB)\nYou can find a table relating EWD numbers to publication years here. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.",
    "Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired. A few of the manuscripts written in Dutch have been translated into English, and one \u2014EWD1036\u2014 has been translated into Spanish. EWD28 has been translated from English into Russian. For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).",
    "Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations). Proofreading Each transcription gets a cursory scan as it\u2019s prepared for uploading, but since a web page can always be updated, I don\u2019t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please\nA compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it. Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn\u2019t provide the backward references, we indicate the relationship by \"see also\" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to",
    "Any reader who notices such a relationship is invited to We have begun adding summaries of the EWDs. This innovation was suggested by G\u00fcnter Rote, who contributed the first dozen summaries. Additional contributions of summaries\u2014especially summaries in English of EWDs in Dutch\u2014are most welcome. Copyrights in most EWDs are held by his children, one of whom \u2014 \u2014 handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder. Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Center\u2019s policies are also applicable. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.",
    "In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on \u201cDijkstra\u2019s sense of what computer science and programming are and what they aren\u2019t.\u201d\nAnother interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.",
    "A transcript is available in the on-line collection of the Charles Babbage Institute. Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute. To mark the occasion of Dijkstra\u2019s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposium\u2019s program (10 MB) contains an outline of Dijkstra\u2019s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration.",
    "Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it). In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend. A blog devoted to Dijkstra\u2019s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011.",
    "An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011. In his blog A Programmer\u2019s Place, Maarten van Emden has an entry entitled \u201cAnother scoop by Dijkstra?\u201d. The entry describes Dijkstra\u2019s \u201cremarkable insight [in \u201cNotes on Structured Programming\u201d (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)\u201d by applying the Assembly-line Principle. The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.",
    "The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021. The folks who contributed most significantly to the site\u2019s creation are acknowledged here. Comments and suggestions about the site are always welcome; please email them to the\nIf you find this site interesting, you may also be interested in another site:\nDiscipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others). Revised 2020-01-12",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology How I spent two decades tracking down the creators of a 1987 USENET game and learned modern packaging tools in the process. The Discovery: A Digital Time Capsule from 1987\nPicture this: October 26, 1987. The Berlin Wall still stands, the World Wide Web is just text, and software is distributed through USENET newsgroups in text files split across multiple posts. On that day, Edward Barlow posted something special to comp.sources.games\n:\n\u201cconquest \u2013 middle earth multi-player game, Part01/05\u201d\nThat\u2019s how Ed Barlow announced it at the time, before quickly changed the name to Conquer. This was Conquer \u2013 a sophisticated multi-player strategy game that would influence countless others. Players controlled nations in Middle Earth, managing resources, armies, magic systems, and diplomatic relations. What made it remarkable wasn\u2019t just the gameplay, but how it was built and distributed in an era when \u201copen source\u201d wasn\u2019t even a term yet. Chapter 0: University Days.",
    "Chapter 0: University Days. Chapter 0: University Days. It was during these days, in the middle of the 90s, that my fellow students and I spent hours experimenting with terminals in the Computer Unix Labs, USENET, links, news, msgs, and of course: conquer. That game was a gem that required to be the leader of a country, and with a map representing as characters each player could control their elven kingdom, orcish empire, or human armies to fight each other while controlling all the details of the economy. But by 2006, this piece of computing history was trapped in legal limbo. Chapter 1: The Quest Begins (2006)\nAs a university student in Spain in the early \u201990s, I\u2019d encountered Conquer in the Unix labs. Fast forward to 2006, and I realized this pioneering game was at risk of being lost forever. The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution.",
    "The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution. I started what I thought would be a simple project: get permission from the original authors to relicense the code under GPL so it could be properly preserved and packaged for modern Linux distributions. Simple, right? Chapter 2: Digital Detective Work\nFinding Edward Barlow and Adam Bryant in 2006 was like archaeological work. Email addresses from the 1980s were long dead. USENET posts provided few clues. I scoured old university directories, googled fragments of names, and followed digital breadcrumbs across decades-old forums. The breakthrough came through pure persistence and a bit of luck. After months of searching, I managed to contact Ed Barlow. His response was refreshingly casual: \u201cYes i delegated it all to adam aeons ago. Im easy on it all\u2026. copyleft didnt exist when i wrote it and it was all for fun so\u2026\u201d\nBut there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)",
    "Chapter 3: The Long Wait (2006-2011) But there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)\nI documented everything on the Debian Legal mailing lists, created a GNU Savannah task (#5945), and even wrote blog posts hoping Adam would find them. The legal experts were clear: I needed explicit written permission from both copyright holders. Years passed. The project stalled. Then, on February 23, 2011, something magical happened. My phone buzzed with a contact form submission:\n\u201cI heard news of the request to release the code. I grant permission to release the code under GPL.\u201d \u2013 Adam Bryant\nHe had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)",
    "Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025) He had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)\nFast forward to 2025, and Stephen Smoogen contacts me about my relicesing efforts in 2006 and how he was particularly interested in reviving: Conquer Version 5 \u2013 a complete rewrite by Adam with advanced features like automatic data conversion, enhanced stability, and sophisticated administrative tools. This wasn\u2019t just an update; it was a complete reimagining of the game. But V5 had a different legal history. In the \u201990s, there had been commercial arrangements. Would Adam agree to GPL this version too? His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic",
    "His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic\nJust when I thought the story was complete, I discovered another contributor: MaF, who had created PostScript utilities for generating printable game maps \u2013 a crucial feature in the pre-GUI era when players needed physical printouts to strategize. Tracking down MaF in 2025 led me to his company, where he\u2019s now Director of Product Security. His response: \u201cOh, that was a long time ago. But yes, that was me. And I have no problem with relicensing it to GPL.\u201d\nRichard Caley: More Than Just a Legal Footnote\nBut not all searches end with an answer. Some end with silence. My investigation of Richard Caley followed the same digital breadcrumbs. I traced him to the University of Edinburgh, where he worked on speech synthesis. I found his technical contributions to FreeBSD. But the trail went cold around 2005.",
    "But the trail went cold around 2005. Then I found him \u2013 not in a USENET archive, but on the front page of his own website, preserved exactly as he left it in web.archive.org. \u201cRichard Caley suffered a fatal heart attack on the 22nd of April, 2005. He was only 41, but had been an undiagnosed diabetic, probably for some considerable time. His web pages remain as he left them.\u201d\nReading those words felt different from finding a historical record. This wasn\u2019t archival research \u2013 this was walking into someone\u2019s house years after they\u2019d gone and finding a note on the table. The page continued:\n\u201cOver and above his tremendous ability with computers and programming, Richard had a keen mind and knowledge of an extraordinary range of topics, both of which he used in frequent contributions to on-line discussions. Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code",
    "Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code The \u201cCaleyisms\u201d \u2013 The Man Behind the Code\nAnd then I discovered his \u201cCaleyisms\u201d \u2013 a curated collection of his most brilliant USENET responses that revealed not just a programmer, but a person:\nWhat\u2019s a shell suit? \u201cOil company executive.\u201d\nHow do you prepare for a pyroclastic flow hitting Edinburgh? \u201cHang 1000 battered Mars bars on strings and stand back?\u201d\nOn his book addiction:\n\u201cI never got the hang of libraries, they keep wanting the things back and get upset when they need a crowbar to force it out of my hands.\u201d\nHis humor was dry, intelligent, and uniquely British. In technical discussions, he could be brutally precise:\n\u201cLack of proper punctuation, spacing, line breaks, capitalisation etc. is like bad handwriting, it doesn\u2019t make it impossible to read what was written, just harder. But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved",
    "But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved A Digital Office Preserved\nExploring his preserved website felt like walking through his digital office. The directory structure revealed his passions: FreeBSD how-tos, POVRAY experiments, wallpaper images, technical projects. His self-deprecating humor shone through in his \u201cAbout\u201d section:\n\u201cThankfully I don\u2019t have a photograph to inflict on you. Just use the picture of Iman Bowie to the left and then imagine someone who looks exactly the opposite in every possible way. This probably explains why she is married to David Bowie and I\u2019m not.\u201d\nHere was a complete person \u2013 technical director at Interactive Information Ltd, speech synthesis researcher, FreeBSD enthusiast, Kate Bush fan, and a wit who brightened countless online discussions. The legal reality was harsh: Richard\u2019s contributions to Conquer couldn\u2019t be relicensed. The university couldn\u2019t help contact heirs due to privacy laws. His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)",
    "His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O) His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)\n\\_/@@\\\n\\\\~~/\n~~\n- RJC RIP\nIn the Conquer project documentation, Richard Caley isn\u2019t remembered as a \u201cproblem case\u201d or \u201cunlicensable code.\u201d He\u2019s honored as the vibrant person he was \u2013 the brilliant mind behind the \u201cCaleyisms,\u201d the researcher who contributed to speech synthesis, the FreeBSD advocate, and the witty participant in early online communities whose words continue to amuse and inform, decades after he wrote them. Chapter 6: Modern Renaissance \u2013 Enter GitHub, CICD and Modern Distributions\nHere\u2019s where the story gets really interesting. While working on preserving these Unix classics, I decided to learn modern packaging techniques. I chose to implement both APK (Alpine Linux) and Debian packaging for the games.",
    "I chose to implement both APK (Alpine Linux) and Debian packaging for the games. For APK packages, I used Melange \u2013 a sophisticated build system that creates provenance-tracked, reproducible packages for the Wolfi \u201cundistro\u201d. The irony? I discovered this tool when some friend started to work for the company that created it. Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite",
    "Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite - Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite\nOriginal Conquer v4 code, by Ed Barlow and Adam Bryant\n(Conquer running in docker container alongside Apache, Curses to WebSockets output thanks to ttyd. Now we can play through the web!) Conquer Version 5 \u2013 The evolution of the classical Conquer, by Adam Bryant\nChapter 8: The Human Element: Why This Matters\nThis isn\u2019t just about preserving old games \u2013 it\u2019s about preserving the story of computing itself. Ed Barlow and Adam Bryant were pioneers who built sophisticated multiplayer experiences when most people had never heard of the internet. They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community.",
    "They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community. Martin Forssen\u2019s PostScript utilities represent the ingenuity of early developers who solved problems with whatever tools were available. Want to visualize your game state? Write a PostScript generator! The 20-year relicensing effort demonstrates something crucial about open source: it\u2019s not just about code, it\u2019s about community and continuity. Every time someone maintains a legacy project, documents its history, or tracks down long-lost contributors, they\u2019re weaving the threads that connect computing\u2019s past to its future. Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure",
    "Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure - Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure\n- Modern tools can revive ancient code: Melange and modern CI/CD gave 1987 software a 2025 renaissance\nThe Continuing Story\nBoth Conquer games are now fully GPL v3 licensed and available with modern packaging. They represent not just playable software, but a complete case study in software archaeology, legal frameworks for preservation, and the evolution of development practices across four decades. The next chapter? Teaching these classic strategy games to a new generation of developers and gamers, while demonstrating that proper legal frameworks and modern tooling can give any historical software a second life. Sometimes the best way to learn cutting-edge technology is by applying it to preserve computing history. What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators?",
    "Have you ever traced the lineage of code back to its original creators? What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators? #FreeSoftware #OpenSource #SoftwarePreservation #Unix #GNU #Linux #Packaging #Melange #TechHistory #GameDevelopment #Unix #USENET #GPL #FST #Debian #ncurses #terminal #shell\nRead this article in Spanish / Lee este art\u00edculo en espa\u00f1ol:\nhttps://vejeta.com/conquer-una-odisea-de-20-anos-en-arqueologia-digital/\nThis article was originally written in both English and Spanish, with additional insights and cultural context in the Spanish version.",
    "Visualize FastAPI endpoints with FastAPI-Voyager\n\nLoading\u2026\nFastAPI Voyager\n{{ state.version }}\nscroll to zoom in/out\ndouble click node to view details. shift + click to see schema's dependencies without unrelated nodes. {{ tag.name }}\n{{ tag.routes.length }}\n{{ route.name }}\nNo routes\n{{ dumpJson }}\nImport core data JSON",
    "Email verification protocol",
    "Email verification protocol Verifying control of an email address is a frequent activity on the web today and is used both to prove the user has provided a valid email address, and as a means of authenticating the user when returning to an application. Verification is performed by either:\n-\nSending the user a link they click on or a verification code. This requires the user to switch from the application they are using to their email address and having to wait for the email arrive, and then perform the verification action. This friction often causes drop off in users completing the task. There are privacy implications as the email transmission informs the mail service the applications the user is using and when they used them. -",
    "- -\nThe user logs in with a social login provider such as Apple or Google that provide a verified email address. This requires the application to have set up a relationship with each social provider, and the user to be using one of those services and wanting to share the additional profile information that is also provided in the OpenID Connect flow. The Email Verification Protocol enables a web application to obtain a verified email address without sending an email, and without the user leaving the web page they are on. To enable the functionality, the mail domain delegates email verification to an issuer that has authentication cookies for the user. When the user provides an email to the HTML form field, the browser calls the issuer passing authentication cookies, the issuer returns a token, which the browser verifies and updates and provides to the web application. The web application then verifies the token and has a verified email address for the user.",
    "The web application then verifies the token and has a verified email address for the user. User privacy is enhanced as the issuer does not learn which web application is making the request as the request is mediated by the browser. -\nSD-JWT+KB token: The selective disclosure json web token with key binding is specified in Selective Disclosure for JWT. This protocol does not use the selective disclosure features, it uses the key binding feature which enables a separation of token issuance and token presentation. The SD-JWT+KB is a token composed of two JWTs separated by the\n~\ncharacter. The first JWT is an SD-JWT aka the issuance token and is signed by the issuer and contains theemail\nandemail_verified\nclaims for the user, and the public key used by the browser to make the request. The second JWT is a KB token and is signed by the browser and contains a hash of the first JWT. The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application -",
    "The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application - Issuer: The service that verifies the user controls an email address. A DNS record for the email domain delegates email verification to the issuer. The issuer serves a\n.well-known/email-verification\nmetadata file that contains itsissuance_endpoint\nthat is called to obtain an issuance token, and itsjwks_uri\nthat points to the JWKS file containing the public keys used to verify the SD-JWT. The issuer is identified by its domain, an eTLD+1 (egissuer.example\n). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer.",
    "This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. ). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. Verified Email Release: The user navigates to any website that requires a verified email address and an input field to enter the email address. The user focusses on the input field and the browser provides one or emails for the user to select based on emails the user has provided previously to the browser. The user selects a verified email and the app proceeds having obtained the verified email. Are emails that can be verified decorated by the browser in the autocomplete UI? What UX is presented to the user when the app gets a verified email so the user knows it is already verified? sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site",
    "sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site participant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site\nRP->>RPS: Nonce request\nRPS->>RPS: Generate nonce, bind to session\nRPS->>RP: Nonce\nRP->>B: Display page\nNote over U,DNS: Step 2: Email Selection\nU->>RP: Focus on email input field\nRP->>B: Input field focused\nB->>U: Display email address list\nU->>B: Select email address\nNote over U,DNS: Step 3: Token Request\nB->>DNS: DNS TXT lookup<br/>_email-verification.$EMAIL_DOMAIN\nDNS->>B: Return iss=issuer.example\nB->>I: GET /.well-known/email-verification\nI->>B: Return metadata\nB->>B: Generate key pair<br/>Create request token\nB->>I: POST request_token=JWT... Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS",
    "Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS I->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS\nB->>B: Create KB\nB->>RP: Provide SD-JWT+KB\nNote over U,DNS: Step 6: Token Verification\nRP->>RPS: Send SD-JWT+KB\nRPS->>RPS: Parse SD-JWT+KB\nRPS->>DNS: DNS TXT lookup for email domain\nDNS->>RPS: Return iss=issuer.example\nRPS->>I: GET /.well-known/email-verification\nI->>RPS: Return metadata with jwks_uri\nRPS->>I: GET jwks_uri\nI->>RPS: Return JWKS public keys\nRPS->>RPS: Verify SD-JWT\nRPS->>RPS: Verify KB-JWT\nRPS->>RP: Email verification complete\nUser navigates to a site that will act as the RP. -\n1.1 - the RP Server generates a nonce and binds the nonce to the session. -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token",
    "If the browser receives anissuance_token -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token\nper 4.4 below, then it sends aemailverifed\nevent that has apresentationToken\nproperty. Following is an example of the HTML in the page:\n<input id=\"email\"\ntype=\"email\"\nautocomplete=\"email\"\nnonce=\"12345677890..random\">\n<script>\nconst input = document.getElementById('email')\ninput.addEventListener('emailverified', e => {\n// e.presentationToken is SD-JWT+KB\nconsole.log({\npresentationToken: e.presentationToken\n})\n})\n</script>\nAuthors are exploring alternative HTML and JS API approaches\n-\n2.1 - User focusses on email input field\n-\n2.2 - The browser displays the list of email addresses it has for the user. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field.",
    "- 2.3 - User selects an email address from browser selection, or the user types an email into the field. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field. Future: allow user to type in a field so we learn about new emails, or if the user does not want the browser to remember emails, the Email Verification Protocol is still available. In the future when we allow the user to use a passkey to authenticate to the issuer, the user can provide a verified email to a web application using a public computer by authenticating with their passkey and not enter any secrets into the public computer. If the RP has performed (1):\n- 3.1 - the browser parses the email domain ($EMAIL_DOMAIN) from the email address, looks up the\nTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record",
    "example record . The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record\n_email-verification.email-domain.example TXT iss=issuer.example\nThis record states that email-domain.example\nhas delegated email verification to the issuer issuer.example\n. If the email domain and the issuer are the same domain, then the record would be:\n_email-verification.issuer.example TXT iss=issuer.example\nAccess to DNS records and email is often independent of website deployments. This provides assurance that an issuer is truly authorized as an insider with only access to websites on\nissuer.example\ncould setup an issuer that would grant them verified emails for any email atissuer.example\n. - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer.",
    "- 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. . - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. For example, https://issuer.example/.well-known/email-verification\nmay redirect to https://accounts.issuer.example/.well-known/email-verification\n. -\n3.3 - the browser confirms that the\n.well-known/email-verification\nfile contains JSON that includes the following properties: -\nissuance_endpoint - the API endpoint the browser calls to obtain an SD-JWT\n-\njwks_uri - the URL where the issuer provides its public keys to verify the SD-JWT\n-\nsigning_alg_values_supported - OPTIONAL. JSON array containing a list of the JWS signing algorithms (\"alg\" values) supported by both the browser for request tokens and the issuer for issued tokens. The same algorithm MUST be used for both the\nrequest_token\nandissuance",
    "The same algorithm MUST be used for both the\nrequest_token\nandissuance request_token\nandissuance\nwithin a single issuance flow. Algorithm identifiers MUST be from the IANA \"JSON Web Signature and Encryption Algorithms\" registry. If omitted, \"EdDSA\" is the default. \"EdDSA\" SHOULD be included in the supported algorithms list. The value \"none\" MUST NOT be used. Each of these properties MUST include the issuer domain as the root of their hostname. Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token",
    "Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token jwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token\n- email - email address to be verified\nThe browser SHOULD select an algorithm from the issuer's signing_alg_values_supported\narray, or use \"EdDSA\" if the property is not present. An example JWT header:\n{\n\"alg\": \"EdDSA\",\n\"typ\": \"JWT\",\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\" // base64url-encoded public key\n}\n}\ndo we want to register a new JWT\ntyp\nAn example payload\n{\n\"aud\": \"issuer.example\",\n\"iat\": 1692345600,\n\"email\": \"user@example.com\"\n}\n- 3.5 - the browser POSTs to the\nissuance_endpoint\nof the issuer with 1P cookies with a content-type ofapplication/x-www-form-urlencoded\ncontaining arequest_token\nparameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=... parameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail jwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim contains a syntactically valid email address\n-\n4.3 - the issuer checks if the cookies sent represent a logged in user, and if the logged in user has control of the email provided in the request_token. If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0",
    "If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0 fieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0\n- Signature: MUST be signed with the issuer's private key corresponding to a public key in the\njwks_uri\nidentified bykid\n- Header: MUST contain\nExample header:\n{\n\"alg\": \"EdDSA\",\n\"kid\": \"2024-08-19\",\n\"typ\": \"evp+sd-jwt\"\n}\nExample payload:\n{\n\"iss\": \"issuer.example\",\n\"iat\": 1724083200,\n\"cnf\": {\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\"\n}\n},\n\"email\": \"user@example.com\",\n\"email_verified\": true\n}\nThe resulting JWT has the ~\nappended to it, making it a valid SD-JWT. - 4.4 - the issuer returns the SD-JWT to the browser as the value of\nissuance_token\nin anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}",
    "Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"} in anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}\nIf the issuer cannot process the token request successfully, it MUST return an appropriate HTTP status code with a JSON error response containing an error\nfield and optionally an error_description\nfield. When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid.",
    "When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. {\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:",
    "When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability: HTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:\nHTTP 500 Internal Server Error\n{\n\"error\": \"server_error\",\n\"error_description\": \"Temporary server error, please try again later\"\n}\nIn a future version of this spec, the issuer could prompt the user to login via a URL or with a Passkey request. On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the",
    "On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT",
    "email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT : \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT\n- Header:\n- signing the KB-JWT with the browser's private key (the same key pair generated in step 3.4)\n- concatenating the SD-JWT and the KB-JWT separated by a tilde (~) to form the SD-JWT+KB\nExample KB-JWT header:\n{ \"alg\": \"EdDSA\", \"typ\": \"kb+jwt\" }\nExample KB-JWT payload:\n{ \"aud\": \"https://rp.example\", \"nonce\": \"259c5eae-486d-4b0f-b666-2a5b5ce1c925\", \"salt\": \"kR7fY9mP3xQ8wN2vL5jH6tZ1cB4nM9sD8fG3hJ7kL2p\", \"iat\": 1724083260, \"sd_hash\": \"X9yH0Ajrdm1Oij4tWso9UzzKJvPoDxwmuEcO3XAdRC0\" }\n-\n5.3 - the browser sets a TBD hidden field and fires the TBD event ...\ndetails TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n- details TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by: iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-",
    "iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n- iss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-\n6.5 - the RP verifies the KB-JWT signature using the public key from the\ncnf\nclaim in the SD-JWT with thealg\nalgorithm from the KB-JWT header\nBelow are notes capturing some discussions of potential privacy implications. -\nThe email domain operator no longer learns which applications the user is verifying their email address to as the applications are no longer sending an email verification code to the user. By using an SD-JWT+KB, the browser intermediates the request and response so that the issuer does not learn the identity of the RP. -\nThe RP can infer if a user is logged into the issuer as the RP receives a SD-JWT when the user is logged in, and does not when the user is not logged in. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had.",
    "-\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. The web page would call an API passing the email address and nonce. It would return a promise that resolves to the SD_JWT or an error response. The API would only be callable after a user gesture such as clicking a button labelled verify on the web page. This provides the web page in more flexibility in how to gather the email address. For example, if the web page is using EVP for login, and the user has used different emails for login and those are stored in cookies, the page can display the list of emails and an option to provide a different one. The user can then select the email they want to use rather than having to type it into a text field.",
    "The user can then select the email they want to use rather than having to type it into a text field. In addition to, or instead of the browser sending cookies to the Issuer, the Issuer could return a WebAuthN request to the browser if it has credentials for the user identified by the email address. The browser would then interact with the user and provide the WebAuthN response to the Issuer, authenticating the user, and the Issuer would then return the SD-JWT. Rather than the DNS TXT record, the Mail Domain would host a JSON file in the .wellknown domain. This creates challenges for the long tail of individually owned domains:\n- would require a domain that is used just for email to now have to support a web server\n- the mail domain is usually an apex domain, which does not support CNAME, complicating hosting a web site",
    "Using bubblewrap to add sandboxing to NetBSD",
    "Using bubblewrap to add sandboxing to NetBSD Google Summer of Code 2025 Reports: Using bubblewrap to add sandboxing to NetBSD\nThis report was written by Vasyl Lanko as part of Google Summer of Code 2025. Introduction\nAs of the time of writing, there is no real sandboxing technique available to NetBSD. There is chroot, which can be considered a weak sandbox because it modifies the root directory of the process, effectively restricting the process' view of the file system, but it doesn't isolate anything else, so all networking, IPC, and mounts inside this restricted file system are the same as of the system, and are accessible. There has already been some research on implementing kernel-level isolation in NetBSD with tools like gaols, mult and netbsd-sandbox, but they haven't been merged to NetBSD. Other operating systems have their own ways to isolate programs, FreeBSD has jails, and Linux has namespaces. Project Goals",
    "Project Goals Project Goals\nThe goal of this project is to bring a new way of sandboxing to NetBSD. More specifically, we want to implement a mechanism like Linux namespaces. These namespaces allow the isolation of parts of the system from a namespace, or, as the user sees it, from an application. NetBSD has compat_linux to run Linux binaries on NetBSD systems, and the implementation of namespaces can also be utilized to emulate namespace-related functionality of Linux binaries. A simple example to visualize our intended result is to consider an application running under an isolated UTS namespace that modifies the hostname. From the system's view, the hostname remains the same old hostname, but from the application's view it sees the modified hostname. Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details",
    "Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details - UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details\n- mount namespace, it is a prerequisite to most other namespace types because UNIX follows the philosophy of \"everything is a file\", so we need a separate mount namespace to have different configuration files on the same location as the system. Linux creates namespaces via the unshare or clone system calls, and it will also be our way of calling the namespace creation logic. We setup the base for implementing Linux namespaces in the NetBSD kernel using kauth, the subsystem managing all authorization requests inside the kernel. It associates credentials with objects, and because the namespace lifecycle management is related to the credential lifecycle it handles all the credential inheritance and reference counting for us. (Thanks kauth devs!)",
    "(Thanks kauth devs!) We separate the implementation of each namespace in a different secmodel, resulting in a similar framework to Linux which allows the isolation of a single namespace type. Our implementation also allows users to pick whether they want to have namespace support, and of what kind, via compilation flags, just like in Linux. UTS namespace\nUTS stands for UNIX Timesharing System, because it allows multiple users to share a single computer system. Isolating the utsname\ncan be useful to give users the illusion that they have control over the system's hostname, and also, for example, to give different hostnames to virtual servers. The UTS namespace stores the namespace's hostname, domain name, and their lengths. To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace.",
    "To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace. This namespace specific information needs to be saved somewhere, and for that we use the credential's private_data\nfield, so we can use a UTS_key\nto save and retrieve UTS\nrelated information from the secmodel. The key specifies the type of information we want to retrieve from the private_data\n, hence using a UTS_key\nfor the UTS namespace. The key for each namespace is a fixed value (we don't create a new key for every credential), but the retrieved value for that key from different credentials may be different. We had to modify kernel code that was directly accessing the hostname\nand domainname\nvariables, to instead call get_uts()\n, which retrieves the UTS struct for the namespace of the calling process. We didn't modify occurrences in kernel drivers because drivers are not part of any namespace, so they should still access the system's resources directly. MNT namespace",
    "MNT namespace MNT namespace\nThe MNT namespace isolates mounts across namespaces. It is used to have different versions of mounted filesystems across namespaces, meaning a user inside a mount namespace can mount and unmount whatever they want without affecting or even breaking the system. The mount namespace structure in Linux is fairly complicated. To have something similar in NetBSD we need to be able to control the mounts accessed by each namespace, and for that we need to control what is each namespace's mountlist, this is also enough for unmounting file systems, because in practice we can just hide them. For the mount_namespace, mountlist structure and the number of credentials using the mount namespace are stored in the credential's private data with the MNT_key\n. Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()",
    "Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist() . Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()\nwhich returns the correct mountlist for the namespace the calling process resides in. Implementation for the mount namespace is immensely more complex than for the UTS namespace, it involves having a good understanding of both Linux and NetBSD behaviour, and I would frequently find myself wondering how to implement something after reading the Linux man pages, which would lead to me looking for it in the Linux source code, understanding it, then going back to NetBSD source code, trying to implement it, and seeing it's too different to implement in the same way. Project Status\nYou can find all code written during this project in GitHub at maksymlanko/netbsd-src gsoc-bubblewrap\nbranch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap",
    "Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap branch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap\nbranch and this was the last one for the mnt_ns\nstill WIP branch. The link includes implementation of general namespace code via secmodels, implementation of the UTS namespace and related ATF-tests, and the work-in-progress implementation of mount namespaces. The mount namespace functionality is not finished as it would require much more work than the time available for this project. To complete it, it would be required invasive and non-trivial changes to the original source code, and, of course, more time. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement.",
    "Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. I believe that after mount namespaces it would be interesting to implement PID namespaces as this in combination with mount namespaces would permit process isolation from this sandbox. Afterwards, implementing user namespaces would allow users to get capabilities similar to root\nin the namespace, giving them sudo\npermissions while still restricting system-wide actions like shutting down the machine. A lower hanging fruit is to implement the namespace management functionality, which in Linux is lsns to list existing namespaces, and setns to move the current process to an already existing namespace. Challenges\n- Semantics. Did you know the unmount system call with MNT_FORCE flag in Linux (usually) returns EBUSY, and in NetBSD it forces the unmounting? One of them makes it easier to implement mount namespaces.",
    "One of them makes it easier to implement mount namespaces. - The behaviour of namespaces is not fully specified in the man pages. If something is not clear from the man pages you need to read the source code. - Unexpected need to learn a lot of VFS concepts and their differences in NetBSD and Linux. - There was a much bigger research component than I anticipated. In the end, Linux and NetBSD are different operating systems, implemented in different ways. Linux is complex and it is not trivial to port namespaces to NetBSD. Notes\nThe project is called \"Using bubblewrap to add sandboxing to NetBSD\" and was initially projected to emulate the unshare\nsystem call into compat_linux\n, but, seeing that having namespaces could be useful for NetBSD, and that it would be easy to add to compat_linux\nafterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap",
    "Implementing other system calls necessary to make the bwrap afterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap\nlinux binary work correctly also wouldn't be as satisfying as implementing namespaces directly into NetBSD, so this was why the project was initially called \"Using bubblewrap to add sandboxing to NetBSD\" but nowadays it would be more accurate to call it \"Sandboxing in NetBSD with Linux-like namespaces\". Thanks\nI am very grateful to Google for Google Summer of Code, because without it I wouldn't have learned so much this summer, wouldn't have met with smart and interesting people, and for sure wouldn't have tried to contribute to a project like NetBSD, even if I always wanted to write operating systems code... But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects.",
    "But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects. I would also like to thank the members of the NetBSD organization for helping me throughout this project, and more specifically:\n- Taylor R. Campbell, Harold Gutch and Nia Alarie from IRC, for helping me fix a nasty\nLD_LIBRARY_PATH\nbug I had on my system which wouldn't let me finish compiling NetBSD, and general GSoC recomendations. - Emmanuel Dreyfus from\ntech-kern\n, with whom I discussed ideas for projects and proposal suggestions, and in the end inspired the namespaces project. - Christoph Badura and Leonardo Taccari who volunteered to be my mentors. They took time to research and answer my questions, anticipated possible problems in my approaches, and always pointed me in the right direction, daily, during all of GSoC's period. This project is from the 3 of us.",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law Montana has made history as the first state in the U.S. to legally protect its citizens\u2019 right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law. The groundbreaking legislation affirms Montanans\u2019 fundamental right to own and operate computational resources \u2014 including hardware, software, and AI tools \u2014 under the state\u2019s constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world. \u201cMontana is once again leading the way in defending individual liberty,\u201d said Senator Daniel Zolnikov, the bill\u2019s sponsor and a longtime advocate for digital privacy. \u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d",
    "\u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law. The act also includes provisions for AI-controlled critical infrastructure, requiring both a \u201cshutdown mechanism\u201d to allow human control and annual safety reviews \u2014 a move aimed at balancing innovation with public safety concerns. The bill has drawn praise from privacy advocates and tech policy groups. Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d",
    "Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d The MRTCA stands in stark contrast to recent regulatory efforts in other states, such as California, Virginia, and New York, where proposals to rein in AI technologies have either failed or been heavily revised. Montana\u2019s approach leans toward empowering individual users rather than restricting access. The law has already inspired similar efforts in New Hampshire, where lawmakers are pushing a constitutional amendment guaranteeing access to computation. Rep. Keith Ammon, the state\u2019s Majority Floor Leader, praised Montana\u2019s leadership: \u201cThis is the kind of bold move that sets the tone for the rest of the country.\u201d\nNationally, the Right to Compute movement is gaining traction. Spearheaded by the grassroots group RightToCompute.ai, the campaign argues that computation \u2014 like speech and property \u2014 is a fundamental human right. \u201cA computer is an extension of the human capacity to think,\u201d the organization states.",
    "\u201cA computer is an extension of the human capacity to think,\u201d the organization states. The movement is supported by Haltia.AI, a Dubai-based AI startup, and the ASIMOV Protocol, a blockchain consortium advocating for decentralized AI infrastructure. Talal Thabet, Co-Founder of both groups, praised Montana\u2019s law as \u201ca monumental step forward in ensuring individuals retain control of their own data and digital tools.\u201d\nAs debates over AI governance and digital rights continue to evolve, Montana\u2019s bold new law could serve as a blueprint for other states seeking to safeguard freedom in the digital era.",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team Zensical \u2013 A modern static site generator built by the Material for MkDocs team\u00b6\nWe are thrilled to announce Zensical, our next-gen static site generator designed to simplify the process of building documentation sites. Distilled from a decade of experience, Zensical is our effort to overcome the technical limitations of MkDocs, reaching far beyond its capabilities. Zensical is the result of thousands of hours of work \u2013 built from the ground up for a modern and comfortable authoring experience, while making it easy for developers to extend and customize Zensical through its upcoming module system. Our goal is to support docs-as-code workflows with tens of thousands of pages, without compromising performance or usability. To make the transition seamless, compatibility comes first. We're putting significant effort into ensuring a smooth migration from Material for MkDocs for all users. Zensical can natively read mkdocs.yml",
    "Zensical can natively read mkdocs.yml , allowing you to build your existing project with minimal changes. As of now, a subset of plugins is supported, and we're working on feature parity in the coming months. Zensical is fully Open Source, licensed under MIT, and can be used for any purpose, including for commercial use. We're also saying goodbye to our sponsorware model, replacing it with our new offering for professional users: Zensical Spark. This allows us to stay independent, maximizing user value, as we shape the future of Zensical together with you. You can subscribe to our newsletter to stay in the loop. This is the second article in a four-part series:\n- Transforming Material for MkDocs\n- Zensical \u2013 A modern static site generator built by the creators of Material for MkDocs. - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6",
    "- What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6 - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6\nSince its initial release in 2016, Material for MkDocs has helped tens of thousands of teams to publish and maintain reliable documentation. However, in recent years, it has become apparent that we were running up against limitations of our core dependency, MkDocs. These limitations proved impossible to overcome as they are deeply rooted in its architecture. We also mentioned in our update on our foundational work that MkDocs must be considered a supply chain risk, since it's unmaintained since August 2024. It has seen no releases in over a year and is accumulating unresolved issues and pull requests. These developments have forced us to cut our ties to MkDocs as a dependency.",
    "These developments have forced us to cut our ties to MkDocs as a dependency. In order to map out a path forward, we went back to the drawing board, talked to dozens of our professional users and thoroughly analyzed the MkDocs ecosystem. We didn't just want to create a fork or port of MkDocs, but decided to rethink static site generation from first principles. With Zensical, we are creating a modern static site generator, which is compatible with your content and customizations, and addresses MkDocs' limitations. While Material for MkDocs is built on top of MkDocs, Zensical consolidates both projects into one coherent stack, covering static site generation, theming, and customization. What you can expect today:\nAlthough we haven't reached full feature parity yet, you can already use Zensical to build your existing Material for MkDocs projects with minimal changes. You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6",
    "What you can expect\u00b6\nSolid foundation\u00b6 You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6\nOur goal with Zensical is to create a coherent and modern stack, vertically integrating all parts of the authoring experience (AX), developer experience (DX), and user experience (UX). This gives us a significant competitive advantage over solutions that overly rely on third-party frameworks and dependencies, helping us to create much more robust Open Source software. ZRX, our new differential build engine, creates a solid foundation for Zensical, and is an Open Source project of its own. It's a fresh take on making differential data flows easy to build and a joy to work with. Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster.",
    "Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster. Following the principle of architectural hoisting, we moved essential, reusable functionality into ZRX, which allows us to keep Zensical's core simple and focused on static site generation. ZRX handles the heavy lifting \u2013 differential builds, caching, and data flow orchestration. With the upcoming module system and component system, both of which are on our public roadmap, Zensical will gain more degrees of freedom in the coming months, allowing you to extend and customize Zensical in ways that were previously impossible with MkDocs. Modern design\u00b6\nZensical brings a fresh, modern design that breaks out of the Materal Design aesthetic, creating a visual foundation that is more easily brandable and adaptable to different use cases. The new design prioritizes clarity, simplicity, and usability, while having a more professional finish:",
    "The new design prioritizes clarity, simplicity, and usability, while having a more professional finish: Right now, the layout and site structure of Zensical match Material for MkDocs closely, as we're focusing on ensuring maximum compatibility. Once we finish work on our upcoming component system, we'll provide an alternative that is much more flexible and adaptable, and can be tailored to different use cases and branding requirements more easily. You can also keep the Material for MkDocs look and feel with a single line of configuration. Blazing-fast search\u00b6\nClient-side search isn't a compromise \u2013 for the vast majority of static sites, it's the best solution, since it's faster, involves zero maintenance, and doesn't require you to pay for a service. As covered in depth in the first part of this series, the current search implementation in Material for MkDocs has severe limitations, and is based on a now unmaintained library, which is why we decided to build a new search engine from scratch. It's based on the same goals as Zensical itself: performance, flexibility, and extensibility.",
    "It's based on the same goals as Zensical itself: performance, flexibility, and extensibility. Disco, our modular and blazing-fast client-side search engine, is exclusively available in Zensical. When you build your site with Zensical, your users will immediately benefit from Disco's improved ranking algorithm, as well as its filtering and aggregation capabilities:\nIn early 2026, we'll be releasing Disco as a standalone Open Source project. With the feedback of our professional users in Zensical Spark, we're going to evolve the search experience, turning Disco into a highly configurable and customizable search engine that adapts to your needs. You can subscribe to our newsletter to receive news about Disco. Authoring experience\u00b6\nSlow feedback loops can be a major pain point when writing documentation. Almost all of us know the feeling of waiting for the static site generator to finish building the site, just to see a small change reflected in the output. With Zensical, we're finally addressing this issue.",
    "With Zensical, we're finally addressing this issue. It's important to understand that we're not yet utilizing the differential capabilities of ZRX to the fullest extent, as we're forced to make several compromises to ensure maximum compatibility with Material for MkDocs at the moment. Markdown rendering needs to go through Python Markdown, which forces us to pay for extra marshalling costs. While the initial build can sometimes be slower than with MkDocs, repeated builds \u2013 especially when serving the site \u2013 are already 4 to 5x faster, as only changed files need to be rebuilt. We're also working on a new Markdown toolchain based on a CommonMark-compliant parser written in Rust, which will make Markdown processing significantly faster. We'll be tackling this as part of the upcoming component system, which we'll start working on in early 2026. Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content.",
    "Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content. Maximum compatibility\u00b6\nCompatibility with Material for MkDocs is our top priority. We understand that switching to a new static site generator can be challenging, especially for large projects with many customizations. Therefore, we've put significant effort into ensuring that Zensical understands mkdocs.yml\nconfiguration files, so that you can build your projects with minimal changes. This means your existing Markdown files, template overrides, CSS and JavaScript extensions don't need to be touched, primarily because we did not change the generated HTML, and rely on Python Markdown for processing your content. However, plugins are a different story. In MkDocs, practically all plugins have side effects, making it impossible to parallelize builds. We started from first principles and asked: what should extensibility look like in a modern static site generator? Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles:",
    "Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles: - Modules can inject, extend, and re-define functionality\n- Modules are deterministic through topological ordering\n- Modules foster reusability, with the possibility to remix them\n- Modules can cooperate through well-defined contracts\nWe're working on shipping essential functionality as provided by MkDocs plugins as built-in modules. In early 2026, we will open the module system to third-party developers, so they can start building their own modules, as we see Zensical as the heart of a thriving ecosystem. Zensical Spark\u00b6\nZensical Spark, our offering for professionals, is the result of countless calls with professional users of Material for MkDocs. From startups to large enterprises, we enable organizations to realize complex projects in diverse environments. For this, we've created Zensical Spark as a collaborative space. If you're a professional user, Zensical Spark is for you, since:\n-",
    "If you're a professional user, Zensical Spark is for you, since:\n- -\nYou can be confident that Zensical will continue to be developed and maintained in the long term as a set of interconnected and sustainable OSI-compliant Open Source projects. -\nYou can receive the support you need to successfully use, configure and customize Zensical in your organization, receiving first-class support from the Zensical team. -\nYou can influence the future development of Zensical by participating in our new approach to Open Source software development, helping us to build exactly what you need. Let's talk! If you're working in a professional context, reach out to contact@zensical.org to schedule a call and learn how Zensical Spark enables your team to transition to Zensical smoothly and have a voice in its continued development. You should also consider joining the waiting list, since seats are limited. We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical!",
    "We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! At Zensical, Tim is focusing on providing the same seamless experience for generating API reference documentation from source code (via docstrings) as he has done with mkdocstrings, the second biggest project in the MkDocs ecosystem. With his expertise, and Zensical's new stack, we'll be pushing the boundaries of what's possible with API reference documentation. Goodbye, GitHub Sponsors\u00b6\nThank you! To all of you who have supported us over the years through GitHub Sponsors \u2013 we are incredibly grateful for your support. It has been invaluable in helping us to build, maintain and evolve Material for MkDocs, and we couldn't have done it without you. Seriously, thank you!",
    "Seriously, thank you! Material for MkDocs gave us something invaluable: experience building for tens of thousands of users, and the opportunity to build a team around Open Source software. It showed us that making a living from Open Source isn't just possible \u2013 we grew it into one of the largest sponsorware projects on GitHub and inspired others to pursue similar paths. Now we're breaking new ground. Zensical is our next chapter, and we're professionalizing how we approach Open Source development. Our vision is to make Zensical free for everyone to use while building a sustainable business around it through our new approach. This transition means saying goodbye to GitHub Sponsors. It has served us exceptionally well, but as we professionalize and scale, we're making the leap from personal project to company \u2013 building a business and team that can meet the growing demands of professional users while staying true to our values. We're doubling down on Open Source, developing software for everyone.",
    "We're doubling down on Open Source, developing software for everyone. We're doubling down on Open Source, developing software for everyone. If you want to continue supporting our work, please subscribe to our newsletter. We'll be providing new methods to support us in the coming months, with the possibility of getting exclusive goodies. Looking Ahead\u00b6\nMaterial for MkDocs grew organically in a pot that eventually became too small. With Zensical, we're building on solid foundations designed to grow with us \u2013 and with you. Material for MkDocs is now in maintenance mode\nWe want to be transparent about the risks of staying on Material for MkDocs. With MkDocs unmaintained and facing fundamental supply chain concerns, we cannot guarantee Material for MkDocs will continue working reliably in the future. We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical.",
    "We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical. If documentation plays a critical role in your organization, and you're worried how this might affect your business, consider joining Zensical Spark, or feel free to schedule a call by reaching out at contact@zensical.org. Where we'll be in 12 months\u00b6\nOver the next 12 months, following our phased transition strategy, we'll reach Phase 2 and 3 \u2013 introducing our module system and component system, as well as CommonMark support. By replacing Python Markdown with a Rust-based Markdown parser, we'll unlock performance improvements and the modularity needed for flexible templating. This is where Zensical truly starts to unfold its capabilities. Zensical is already powering real projects due to extensive compatibility with Material for MkDocs. We're actively working on closing the gap to reach full feature parity. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help.",
    "If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. Connect with us\u00b6\nIf you have questions we haven't addressed, please reach out to us at contact@zensical.org. We're currently collecting questions from the community about Zensical, and will address them in an FAQ section as part of our documentation in the coming weeks. We're incredibly thankful that you have been part of our journey so far. With Zensical, we're embarking on a new chapter, and we couldn't be more excited to have you with us. You can subscribe to our newsletter to stay in the loop.",
    "I Am Mark Zuckerberg",
    "I Am Mark Zuckerberg Welcome to iammarkzuckerg.com\nNo, not THAT Mark Zuckerberg-this one's busy helping Hoosiers, not launching social networks. Relax, you haven't accidentally logged into Facebook or the Metaverse. You're on the site of Mark S. Zuckerberg, Indiana's original bearer of the name, proud bankruptcy attorney, and frequent recipient of confused emails from people seeking tech support or handouts of money. What I Really Do:\n- Help people obtain a fresh financial start (no passwords required)\n- Offer dependable, human-involved advice (my artificial intelligence is powered by coffee)\n- Answer local legal questions, not privacy scandals\nReal Zuckerberg Facts:\n- Shares a name, not fortune, with the Facebook founder\n- Gets mistaken daily for a tech billionaire\n- Has written zero social media apps, but plenty of court briefs\nFun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place!",
    "But if you need trustworthy bankruptcy help, you're in exactly the right place! Fun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place! Click around, get to know your (non-billionaire) local Mark, and remember: No login required. Click Here to See How Other\nWebsites Have Reacted to This\nInteresting Things That Have Happened to Me Because My Name is Mark Zuckerberg\nFor a complete list of things that have happened to Mark Zuckerberg click here\nLike I said, I don't wish Mark E. Zuckerberg any ill will at all. I hope the best for him, but let me tell you this: I will rule the search for \"Mark Zuckerberg bankruptcy\". And if he does fall upon difficult financial times, and happens to be in Indiana, I will gladly handle his case in honor of our eponymy.",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel Ironclad is a (partially) formally verified, real-time capable, UNIX-like operating system kernel for general-purpose and embedded uses. It is written in SPARK and Ada, and is comprised of 100% free software. Ironclad features a familiar POSIX-compatible interface, true simultaneous preemptive multitasking, Mandatory Access Control (MAC), and support for hard real-time scheduling. Ironclad is fully open source and distributed under the GPLv3, ensuring it remains free. No firmware blobs are needed or shipped with the kernel. Every piece of the stack is open source. SPARK's state of the art formal verification is employed for ensuring absence of errors and correctness of big portions of Ironclad, like cryptography, MAC, and user-facing facilities. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation.",
    "Dependency on only the GNU toolchain allows for easy cross-compilation. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation. Ironclad will always be free for use, study, and modification, so, to support the project, we rely on the use of donations and grants. Every contribution makes a difference and allows us to do more. This project is funded through NGI Zero Core, a fund established by NLnet with financial support from the European Commission's Next Generation Internet program. Learn more at the NLnet project page. Additionally, we would like to thank the following organizations:",
    "The Manuscripts of Edsger W. Dijkstra",
    "The Manuscripts of Edsger W. Dijkstra Home\nNumerical EWD Index: 00xx 01xx 02xx 03xx 04xx 05xx 06xx 07xx 08xx 09xx 10xx 11xx 12xx 13xx\nBibTeX index\nMC Reports\nOther documents\nTranscriptions\nVideo and Audio\nExternal links\nIn addition, Dijkstra was intensely interested in teaching, and in the relationships between academic computing science and the software industry. During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award.",
    "During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra\u2019s contributions brought him many prizes and awards, including computing science\u2019s highest honor, the ACM Turing Award. Like most of us, Dijkstra always believed it a scientist\u2019s duty to maintain a lively correspondence with his scientific colleagues. To a greater extent than most of us, he put that conviction into practice. For over four decades, he mailed copies of his consecutively numbered technical notes, trip reports, insightful observations, and pungent commentaries, known collectively as \u201cEWDs\u201d, to several dozen recipients in academia and industry. Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands.",
    "Thanks to the ubiquity of the photocopier and the wide interest in Dijkstra\u2019s writings, the informal circulation of many of the EWDs eventually reached into the thousands. Although most of Dijkstra\u2019s publications began life as EWD manuscripts, the great majority of his manuscripts remain unpublished. They have been inaccessible to many potential readers, and those who have received copies have been unable to cite them in their own work. To alleviate both of these problems, the department has collected over a thousand of the manuscripts in this permanent web site, in the form of PDF bitmap documents (to read them, you\u2019ll need a copy of Acrobat Reader). We hope you will find it convenient, useful, inspiring, and enjoyable. The original manuscripts, along with diaries, correspondence, photographs, and other papers, are housed at The Center for American History of The University of Texas at Austin. Each manuscript file is accessible through either of two indexes:\n0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for.",
    "These contain titles only, but are faster if you know what you\u2019re looking for. 0. BibTeX index. Each entry includes all the available bibliographic data. 1. Ad-hoc indexes. These contain titles only, but are faster if you know what you\u2019re looking for. EWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica) PhD thesis (5.3 MB) Other documents\nEWD-numbered documents (This index gives an approximate correspondence between manuscripts\u2019 EWD numbers and the year in which they appeared.) Technical reports from the Mathematical Centre (now CWI: Centrum voor Wiskunde en Informatica)\nPhD thesis (5.3 MB)\nYou can find a table relating EWD numbers to publication years here. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers.",
    "Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. Many of the privately circulated manuscripts collected here were subsequently published; their copyrights are held by their respective publishers. A growing number of the PDF bitmap documents have been transcribed to make them searchable and accessible to visitors who are visually impaired. A few of the manuscripts written in Dutch have been translated into English, and one \u2014EWD1036\u2014 has been translated into Spanish. EWD28 has been translated from English into Russian. For these transcriptions and translations we are grateful to over sixty contributors. Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations).",
    "Volunteers willing to transcribe manuscripts are always welcome (Note: doing EWDs justice in translation has turned out to be too difficult, so we are no longer soliciting translations). Proofreading Each transcription gets a cursory scan as it\u2019s prepared for uploading, but since a web page can always be updated, I don\u2019t strive for (unattainable) perfection before installing it. On the web, proofreading is a game that can be played by every reader; if you spot an error, please\nA compilation of cross-references has been contributed by Diethard Michaelis. As its author notes, the collection is incomplete, and all readers are invited to add to it. Dijkstra often returned to topics about which he had already written, when he had something new to say or even just a better way of saying it. When Dijkstra himself didn\u2019t provide the backward references, we indicate the relationship by \"see also\" links in the index, leaving the judgment of the extent to which the earlier EWD is superseded by the later one to the reader. Any reader who notices such a relationship is invited to",
    "Any reader who notices such a relationship is invited to We have begun adding summaries of the EWDs. This innovation was suggested by G\u00fcnter Rote, who contributed the first dozen summaries. Additional contributions of summaries\u2014especially summaries in English of EWDs in Dutch\u2014are most welcome. Copyrights in most EWDs are held by his children, one of whom \u2014 \u2014 handles requests for permission to publish reproductions. The exceptions are documents that were published, and whose copyrights are held by their publishers; those documents are listed here, and each one is provided with a cover page identifying the copyright holder. Because the original manuscripts are in possession of the Briscoe Center for American History at The University of Texas, the Center\u2019s policies are also applicable. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews.",
    "In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. In addition to the manuscripts, you may enjoy some recordings of Dijkstra lectures and interviews. An interview with Dijkstra (Spanish translation here) was conducted in 1985 by Rogier F. van Vlissingen, who has also written a personal reflection on \u201cDijkstra\u2019s sense of what computer science and programming are and what they aren\u2019t.\u201d\nAnother interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute.",
    "A transcript is available in the on-line collection of the Charles Babbage Institute. Another interview was conducted by Philip L. Frana in August 2001. A transcript is available in the on-line collection of the Charles Babbage Institute. To mark the occasion of Dijkstra\u2019s retirement in November 1999 from the Schlumberger Centennial Chair in Computer Sciences, which he had occupied since 1984, and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, In Pursuit of Simplicity, which took place on his birthday in May 2000. The symposium\u2019s program (10 MB) contains an outline of Dijkstra\u2019s career, as well as a collection of quotes culled from his writings, from his blackboard, and from what others have said about him. Banquet speeches by David Gries, Fred Schneider, Krzysztof Apt, W.M. Turski, and H. Richards were recorded on a video. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration.",
    "Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. Dijkstra\u2019s death in August 2002 was marked by many obituaries and memorials, including the Computer Sciences department\u2019s memorial celebration. A remembrance of Dijkstra was posted in May 2008 by Maarten van Emden (thanks to Tristram Brelstaff for noting it). In 2021 Krzysztof R. Apt and Tony Hoare edited a commemoration of Edsger Dijkstra written by more than twenty computer scientists who knew him as a colleague, teacher, and friend. A blog devoted to Dijkstra\u2019s works and thoughts has been created, and is being maintained, by the historian of computing Edgar G. Daylight. An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011.",
    "An article by Daylight, \u201cDijkstra\u2019s Rallying Cry for Generalization: the Advent of the Recursive Procedure, late 1950s - early 1960s,\u201d appeared in The Computer Journal, March 2011. In his blog A Programmer\u2019s Place, Maarten van Emden has an entry entitled \u201cAnother scoop by Dijkstra?\u201d. The entry describes Dijkstra\u2019s \u201cremarkable insight [in \u201cNotes on Structured Programming\u201d (EWD 249)] that resolves the stand-off between the Sieve of Eratosthenes (efficient in terms of time, but not memory) and the method of Trial Division (efficient in terms of memory, but not time)\u201d by applying the Assembly-line Principle. The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010.",
    "The Edsger W. Dijkstra Prize in Distributed Computing honors Dijkstra\u2019s \u201cfoundational work on concurrency primitives (such as the semaphore), concurrency problems (such as mutual exclusion and deadlock), reasoning about concurrent systems, and self-stabilization [, which] comprises one of the most important supports upon which the field of distributed computing is built.\u201d\nA series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. A series of annual lectures in memory of Dijkstra commenced at The University of Texas in October 2010. Recent significant changes in the site are listed here; the most recent change was posted on 30 March 2021. The folks who contributed most significantly to the site\u2019s creation are acknowledged here. Comments and suggestions about the site are always welcome; please email them to the\nIf you find this site interesting, you may also be interested in another site:\nDiscipline in Thought which is a website dedicated to disciplined thinking, calculational mathematics, and mathematical methodology. The members of this site are markedly influenced by the works of EWD, and the material shared through the website continues in the traditions set by EWD (among others). Revised 2020-01-12",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
    "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology How I spent two decades tracking down the creators of a 1987 USENET game and learned modern packaging tools in the process. The Discovery: A Digital Time Capsule from 1987\nPicture this: October 26, 1987. The Berlin Wall still stands, the World Wide Web is just text, and software is distributed through USENET newsgroups in text files split across multiple posts. On that day, Edward Barlow posted something special to comp.sources.games\n:\n\u201cconquest \u2013 middle earth multi-player game, Part01/05\u201d\nThat\u2019s how Ed Barlow announced it at the time, before quickly changed the name to Conquer. This was Conquer \u2013 a sophisticated multi-player strategy game that would influence countless others. Players controlled nations in Middle Earth, managing resources, armies, magic systems, and diplomatic relations. What made it remarkable wasn\u2019t just the gameplay, but how it was built and distributed in an era when \u201copen source\u201d wasn\u2019t even a term yet. Chapter 0: University Days.",
    "Chapter 0: University Days. Chapter 0: University Days. It was during these days, in the middle of the 90s, that my fellow students and I spent hours experimenting with terminals in the Computer Unix Labs, USENET, links, news, msgs, and of course: conquer. That game was a gem that required to be the leader of a country, and with a map representing as characters each player could control their elven kingdom, orcish empire, or human armies to fight each other while controlling all the details of the economy. But by 2006, this piece of computing history was trapped in legal limbo. Chapter 1: The Quest Begins (2006)\nAs a university student in Spain in the early \u201990s, I\u2019d encountered Conquer in the Unix labs. Fast forward to 2006, and I realized this pioneering game was at risk of being lost forever. The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution.",
    "The source code existed, scattered across ancient USENET archives, but its licensing was unclear \u2013 typical of the \u201cpost it and see what happens\u201d era of early internet software distribution. I started what I thought would be a simple project: get permission from the original authors to relicense the code under GPL so it could be properly preserved and packaged for modern Linux distributions. Simple, right? Chapter 2: Digital Detective Work\nFinding Edward Barlow and Adam Bryant in 2006 was like archaeological work. Email addresses from the 1980s were long dead. USENET posts provided few clues. I scoured old university directories, googled fragments of names, and followed digital breadcrumbs across decades-old forums. The breakthrough came through pure persistence and a bit of luck. After months of searching, I managed to contact Ed Barlow. His response was refreshingly casual: \u201cYes i delegated it all to adam aeons ago. Im easy on it all\u2026. copyleft didnt exist when i wrote it and it was all for fun so\u2026\u201d\nBut there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)",
    "Chapter 3: The Long Wait (2006-2011) But there was a catch \u2013 I needed permission from Adam Bryant too, and he seemed to have vanished into the digital ether. Chapter 3: The Long Wait (2006-2011)\nI documented everything on the Debian Legal mailing lists, created a GNU Savannah task (#5945), and even wrote blog posts hoping Adam would find them. The legal experts were clear: I needed explicit written permission from both copyright holders. Years passed. The project stalled. Then, on February 23, 2011, something magical happened. My phone buzzed with a contact form submission:\n\u201cI heard news of the request to release the code. I grant permission to release the code under GPL.\u201d \u2013 Adam Bryant\nHe had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)",
    "Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025) He had found one of my articles online and reached out on his own. Chapter 4: The Plot Twist \u2013 Version 5 Emerges (2025)\nFast forward to 2025, and Stephen Smoogen contacts me about my relicesing efforts in 2006 and how he was particularly interested in reviving: Conquer Version 5 \u2013 a complete rewrite by Adam with advanced features like automatic data conversion, enhanced stability, and sophisticated administrative tools. This wasn\u2019t just an update; it was a complete reimagining of the game. But V5 had a different legal history. In the \u201990s, there had been commercial arrangements. Would Adam agree to GPL this version too? His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic",
    "His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic His response: \u201cI have no issues with applying a new GPL license to Version 5 as well.\u201d\nChapter 5: The Missing Piece \u2013 PostScript Magic\nJust when I thought the story was complete, I discovered another contributor: MaF, who had created PostScript utilities for generating printable game maps \u2013 a crucial feature in the pre-GUI era when players needed physical printouts to strategize. Tracking down MaF in 2025 led me to his company, where he\u2019s now Director of Product Security. His response: \u201cOh, that was a long time ago. But yes, that was me. And I have no problem with relicensing it to GPL.\u201d\nRichard Caley: More Than Just a Legal Footnote\nBut not all searches end with an answer. Some end with silence. My investigation of Richard Caley followed the same digital breadcrumbs. I traced him to the University of Edinburgh, where he worked on speech synthesis. I found his technical contributions to FreeBSD. But the trail went cold around 2005.",
    "But the trail went cold around 2005. Then I found him \u2013 not in a USENET archive, but on the front page of his own website, preserved exactly as he left it in web.archive.org. \u201cRichard Caley suffered a fatal heart attack on the 22nd of April, 2005. He was only 41, but had been an undiagnosed diabetic, probably for some considerable time. His web pages remain as he left them.\u201d\nReading those words felt different from finding a historical record. This wasn\u2019t archival research \u2013 this was walking into someone\u2019s house years after they\u2019d gone and finding a note on the table. The page continued:\n\u201cOver and above his tremendous ability with computers and programming, Richard had a keen mind and knowledge of an extraordinary range of topics, both of which he used in frequent contributions to on-line discussions. Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code",
    "Despite his unique approach to speling, his prolific contributions to various news group debates informed and amused many over the years.\u201d\nThe \u201cCaleyisms\u201d \u2013 The Man Behind the Code The \u201cCaleyisms\u201d \u2013 The Man Behind the Code\nAnd then I discovered his \u201cCaleyisms\u201d \u2013 a curated collection of his most brilliant USENET responses that revealed not just a programmer, but a person:\nWhat\u2019s a shell suit? \u201cOil company executive.\u201d\nHow do you prepare for a pyroclastic flow hitting Edinburgh? \u201cHang 1000 battered Mars bars on strings and stand back?\u201d\nOn his book addiction:\n\u201cI never got the hang of libraries, they keep wanting the things back and get upset when they need a crowbar to force it out of my hands.\u201d\nHis humor was dry, intelligent, and uniquely British. In technical discussions, he could be brutally precise:\n\u201cLack of proper punctuation, spacing, line breaks, capitalisation etc. is like bad handwriting, it doesn\u2019t make it impossible to read what was written, just harder. But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved",
    "But you probably write in green crayon anyway.\u201d\nA Digital Office Preserved A Digital Office Preserved\nExploring his preserved website felt like walking through his digital office. The directory structure revealed his passions: FreeBSD how-tos, POVRAY experiments, wallpaper images, technical projects. His self-deprecating humor shone through in his \u201cAbout\u201d section:\n\u201cThankfully I don\u2019t have a photograph to inflict on you. Just use the picture of Iman Bowie to the left and then imagine someone who looks exactly the opposite in every possible way. This probably explains why she is married to David Bowie and I\u2019m not.\u201d\nHere was a complete person \u2013 technical director at Interactive Information Ltd, speech synthesis researcher, FreeBSD enthusiast, Kate Bush fan, and a wit who brightened countless online discussions. The legal reality was harsh: Richard\u2019s contributions to Conquer couldn\u2019t be relicensed. The university couldn\u2019t help contact heirs due to privacy laws. His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)",
    "His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O) His friends had preserved his memory with a simple ASCII tribute at the end of his page:\n^_^\n(O O)\n\\_/@@\\\n\\\\~~/\n~~\n- RJC RIP\nIn the Conquer project documentation, Richard Caley isn\u2019t remembered as a \u201cproblem case\u201d or \u201cunlicensable code.\u201d He\u2019s honored as the vibrant person he was \u2013 the brilliant mind behind the \u201cCaleyisms,\u201d the researcher who contributed to speech synthesis, the FreeBSD advocate, and the witty participant in early online communities whose words continue to amuse and inform, decades after he wrote them. Chapter 6: Modern Renaissance \u2013 Enter GitHub, CICD and Modern Distributions\nHere\u2019s where the story gets really interesting. While working on preserving these Unix classics, I decided to learn modern packaging techniques. I chose to implement both APK (Alpine Linux) and Debian packaging for the games.",
    "I chose to implement both APK (Alpine Linux) and Debian packaging for the games. For APK packages, I used Melange \u2013 a sophisticated build system that creates provenance-tracked, reproducible packages for the Wolfi \u201cundistro\u201d. The irony? I discovered this tool when some friend started to work for the company that created it. Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite",
    "Chapter 7: The Technical Journey: From USENET to Modern CI/CD\nThe transformation has been remarkable:\n1987 Original:\n- Distributed as split USENET posts\n- Manual compilation with system-specific Makefiles\n- No version control or automated testing\n2025 Revival:\n# Modern CI/CD with GitHub Actions\n- name: Build APK package\nrun: melange build conquer.yaml\n- name: Build Debian package\nrun: dpkg-buildpackage -b\nKey Modern Additions:\n- GPLv3 relicensing\n- Make building system modernization\n- C Codebase partially updated to support modern ANSI C99 specification\n- Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite - Debian packaging\n- APK packaging with Melange\nYou can see the complete transformation in the repositories:\n- Conquer v4 \u2013 The original classic\n- Conquer v5 \u2013 The advanced rewrite\nOriginal Conquer v4 code, by Ed Barlow and Adam Bryant\n(Conquer running in docker container alongside Apache, Curses to WebSockets output thanks to ttyd. Now we can play through the web!) Conquer Version 5 \u2013 The evolution of the classical Conquer, by Adam Bryant\nChapter 8: The Human Element: Why This Matters\nThis isn\u2019t just about preserving old games \u2013 it\u2019s about preserving the story of computing itself. Ed Barlow and Adam Bryant were pioneers who built sophisticated multiplayer experiences when most people had never heard of the internet. They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community.",
    "They distributed software through USENET because that\u2019s what you did \u2013 you shared cool things with the community. Martin Forssen\u2019s PostScript utilities represent the ingenuity of early developers who solved problems with whatever tools were available. Want to visualize your game state? Write a PostScript generator! The 20-year relicensing effort demonstrates something crucial about open source: it\u2019s not just about code, it\u2019s about community and continuity. Every time someone maintains a legacy project, documents its history, or tracks down long-lost contributors, they\u2019re weaving the threads that connect computing\u2019s past to its future. Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure",
    "Lessons for Modern Developers\n- Document everything: Those casual USENET posts became crucial legal evidence decades later\n- License clearly: Ed\u2019s comment that \u201ccopyleft didnt exist when i wrote it\u201d highlights how licensing landscapes evolve\n- Community matters: Adam found my articles because the community was talking about preservation\n- Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure - Technical debt is temporal: What seems like legacy tech today might be tomorrow\u2019s archaeological treasure\n- Modern tools can revive ancient code: Melange and modern CI/CD gave 1987 software a 2025 renaissance\nThe Continuing Story\nBoth Conquer games are now fully GPL v3 licensed and available with modern packaging. They represent not just playable software, but a complete case study in software archaeology, legal frameworks for preservation, and the evolution of development practices across four decades. The next chapter? Teaching these classic strategy games to a new generation of developers and gamers, while demonstrating that proper legal frameworks and modern tooling can give any historical software a second life. Sometimes the best way to learn cutting-edge technology is by applying it to preserve computing history. What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators?",
    "Have you ever traced the lineage of code back to its original creators? What historical software deserves preservation in your field? Have you ever traced the lineage of code back to its original creators? #FreeSoftware #OpenSource #SoftwarePreservation #Unix #GNU #Linux #Packaging #Melange #TechHistory #GameDevelopment #Unix #USENET #GPL #FST #Debian #ncurses #terminal #shell\nRead this article in Spanish / Lee este art\u00edculo en espa\u00f1ol:\nhttps://vejeta.com/conquer-una-odisea-de-20-anos-en-arqueologia-digital/\nThis article was originally written in both English and Spanish, with additional insights and cultural context in the Spanish version.",
    "Visualize FastAPI endpoints with FastAPI-Voyager\n\nLoading\u2026\nFastAPI Voyager\n{{ state.version }}\nscroll to zoom in/out\ndouble click node to view details. shift + click to see schema's dependencies without unrelated nodes. {{ tag.name }}\n{{ tag.routes.length }}\n{{ route.name }}\nNo routes\n{{ dumpJson }}\nImport core data JSON",
    "Email verification protocol",
    "Email verification protocol Verifying control of an email address is a frequent activity on the web today and is used both to prove the user has provided a valid email address, and as a means of authenticating the user when returning to an application. Verification is performed by either:\n-\nSending the user a link they click on or a verification code. This requires the user to switch from the application they are using to their email address and having to wait for the email arrive, and then perform the verification action. This friction often causes drop off in users completing the task. There are privacy implications as the email transmission informs the mail service the applications the user is using and when they used them. -",
    "- -\nThe user logs in with a social login provider such as Apple or Google that provide a verified email address. This requires the application to have set up a relationship with each social provider, and the user to be using one of those services and wanting to share the additional profile information that is also provided in the OpenID Connect flow. The Email Verification Protocol enables a web application to obtain a verified email address without sending an email, and without the user leaving the web page they are on. To enable the functionality, the mail domain delegates email verification to an issuer that has authentication cookies for the user. When the user provides an email to the HTML form field, the browser calls the issuer passing authentication cookies, the issuer returns a token, which the browser verifies and updates and provides to the web application. The web application then verifies the token and has a verified email address for the user.",
    "The web application then verifies the token and has a verified email address for the user. User privacy is enhanced as the issuer does not learn which web application is making the request as the request is mediated by the browser. -\nSD-JWT+KB token: The selective disclosure json web token with key binding is specified in Selective Disclosure for JWT. This protocol does not use the selective disclosure features, it uses the key binding feature which enables a separation of token issuance and token presentation. The SD-JWT+KB is a token composed of two JWTs separated by the\n~\ncharacter. The first JWT is an SD-JWT aka the issuance token and is signed by the issuer and contains theemail\nandemail_verified\nclaims for the user, and the public key used by the browser to make the request. The second JWT is a KB token and is signed by the browser and contains a hash of the first JWT. The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application -",
    "The resulting SD-JWT+KB is the presentation token, and enables the application to verify the issuer provided the email address for the user without the issuer learning about the specific application - Issuer: The service that verifies the user controls an email address. A DNS record for the email domain delegates email verification to the issuer. The issuer serves a\n.well-known/email-verification\nmetadata file that contains itsissuance_endpoint\nthat is called to obtain an issuance token, and itsjwks_uri\nthat points to the JWKS file containing the public keys used to verify the SD-JWT. The issuer is identified by its domain, an eTLD+1 (egissuer.example\n). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer.",
    "This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. ). The hostname in all URLs from the issuer's metadata MUST end with the issuer's domain. This identifier is what binds the SD-JWT, the DNS delegation, with the issuer. Verified Email Release: The user navigates to any website that requires a verified email address and an input field to enter the email address. The user focusses on the input field and the browser provides one or emails for the user to select based on emails the user has provided previously to the browser. The user selects a verified email and the app proceeds having obtained the verified email. Are emails that can be verified decorated by the browser in the autocomplete UI? What UX is presented to the user when the app gets a verified email so the user knows it is already verified? sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site",
    "sequenceDiagram\nparticipant U as User\nparticipant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site participant B as Browser\nparticipant RP as RP Page\nparticipant RPS as RP Server\nparticipant I as Issuer\nparticipant DNS as DNS\nNote over U,DNS: Step 1: Email Request\nU->>RP: Navigate to site\nRP->>RPS: Nonce request\nRPS->>RPS: Generate nonce, bind to session\nRPS->>RP: Nonce\nRP->>B: Display page\nNote over U,DNS: Step 2: Email Selection\nU->>RP: Focus on email input field\nRP->>B: Input field focused\nB->>U: Display email address list\nU->>B: Select email address\nNote over U,DNS: Step 3: Token Request\nB->>DNS: DNS TXT lookup<br/>_email-verification.$EMAIL_DOMAIN\nDNS->>B: Return iss=issuer.example\nB->>I: GET /.well-known/email-verification\nI->>B: Return metadata\nB->>B: Generate key pair<br/>Create request token\nB->>I: POST request_token=JWT... Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS",
    "Note over U,DNS: Step 4: Token Issuance\nI->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS I->>I: Verify request\nI->>I: Generate SD-JWT\nI->>B: {\"issuance_token\":\"SD-JWT\"}\nNote over U,DNS: Step 5: Token Presentation\nB->>B: Verify SD-JWT\nB->>I: GET jwks_uri for public keys\nI->>B: Return JWKS\nB->>B: Create KB\nB->>RP: Provide SD-JWT+KB\nNote over U,DNS: Step 6: Token Verification\nRP->>RPS: Send SD-JWT+KB\nRPS->>RPS: Parse SD-JWT+KB\nRPS->>DNS: DNS TXT lookup for email domain\nDNS->>RPS: Return iss=issuer.example\nRPS->>I: GET /.well-known/email-verification\nI->>RPS: Return metadata with jwks_uri\nRPS->>I: GET jwks_uri\nI->>RPS: Return JWKS public keys\nRPS->>RPS: Verify SD-JWT\nRPS->>RPS: Verify KB-JWT\nRPS->>RP: Email verification complete\nUser navigates to a site that will act as the RP. -\n1.1 - the RP Server generates a nonce and binds the nonce to the session. -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token",
    "If the browser receives anissuance_token -\n1.2 - the RP Server returns a page that has an input field with the\nautocomplete\nproperty set to\"email\"\nand thenonce\nproperty set the the nonce. If the browser receives anissuance_token\nper 4.4 below, then it sends aemailverifed\nevent that has apresentationToken\nproperty. Following is an example of the HTML in the page:\n<input id=\"email\"\ntype=\"email\"\nautocomplete=\"email\"\nnonce=\"12345677890..random\">\n<script>\nconst input = document.getElementById('email')\ninput.addEventListener('emailverified', e => {\n// e.presentationToken is SD-JWT+KB\nconsole.log({\npresentationToken: e.presentationToken\n})\n})\n</script>\nAuthors are exploring alternative HTML and JS API approaches\n-\n2.1 - User focusses on email input field\n-\n2.2 - The browser displays the list of email addresses it has for the user. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field.",
    "- 2.3 - User selects an email address from browser selection, or the user types an email into the field. Q: Are emails that could be verified decorated for user to understand? - 2.3 - User selects an email address from browser selection, or the user types an email into the field. Future: allow user to type in a field so we learn about new emails, or if the user does not want the browser to remember emails, the Email Verification Protocol is still available. In the future when we allow the user to use a passkey to authenticate to the issuer, the user can provide a verified email to a web application using a public computer by authenticating with their passkey and not enter any secrets into the public computer. If the RP has performed (1):\n- 3.1 - the browser parses the email domain ($EMAIL_DOMAIN) from the email address, looks up the\nTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record",
    "example record . The contents of the record MUST start withiss=\nfollowed by the issuer identifier. There MUST be only oneTXT\nrecord for_email-verification.$EMAIL_DOMAIN\n. example record\n_email-verification.email-domain.example TXT iss=issuer.example\nThis record states that email-domain.example\nhas delegated email verification to the issuer issuer.example\n. If the email domain and the issuer are the same domain, then the record would be:\n_email-verification.issuer.example TXT iss=issuer.example\nAccess to DNS records and email is often independent of website deployments. This provides assurance that an issuer is truly authorized as an insider with only access to websites on\nissuer.example\ncould setup an issuer that would grant them verified emails for any email atissuer.example\n. - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer.",
    "- 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. . - 3.2 - if an issuer is found, the browser loads\nhttps://$ISSUER$/.well-known/email-verification\nand MUST follow redirects to the same path but with a different subdomain of the Issuer. For example, https://issuer.example/.well-known/email-verification\nmay redirect to https://accounts.issuer.example/.well-known/email-verification\n. -\n3.3 - the browser confirms that the\n.well-known/email-verification\nfile contains JSON that includes the following properties: -\nissuance_endpoint - the API endpoint the browser calls to obtain an SD-JWT\n-\njwks_uri - the URL where the issuer provides its public keys to verify the SD-JWT\n-\nsigning_alg_values_supported - OPTIONAL. JSON array containing a list of the JWS signing algorithms (\"alg\" values) supported by both the browser for request tokens and the issuer for issued tokens. The same algorithm MUST be used for both the\nrequest_token\nandissuance",
    "The same algorithm MUST be used for both the\nrequest_token\nandissuance request_token\nandissuance\nwithin a single issuance flow. Algorithm identifiers MUST be from the IANA \"JSON Web Signature and Encryption Algorithms\" registry. If omitted, \"EdDSA\" is the default. \"EdDSA\" SHOULD be included in the supported algorithms list. The value \"none\" MUST NOT be used. Each of these properties MUST include the issuer domain as the root of their hostname. Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token",
    "Following is an example .well-known/email-verification\nfile\n{\n\"issuance_endpoint\": \"https://accounts.issuer.example/email-verification/issuance\",\n\"jwks_uri\": \"https://accounts.issuer.example/email-verification/jwks\",\n\"signing_alg_values_supported\": [\"EdDSA\", \"RS256\"]\n}\n-\n3.4 - the browser generates a fresh private / public key and signs a JWT with the private key that has the public key in the JWT header in the JWK format as a\njwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token jwk\nclaim that contains the following claims in the payload:- aud - the issuer\n- iat - time when the JWT was signed\n- jti - unique identifier for the token\n- email - email address to be verified\nThe browser SHOULD select an algorithm from the issuer's signing_alg_values_supported\narray, or use \"EdDSA\" if the property is not present. An example JWT header:\n{\n\"alg\": \"EdDSA\",\n\"typ\": \"JWT\",\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\" // base64url-encoded public key\n}\n}\ndo we want to register a new JWT\ntyp\nAn example payload\n{\n\"aud\": \"issuer.example\",\n\"iat\": 1692345600,\n\"email\": \"user@example.com\"\n}\n- 3.5 - the browser POSTs to the\nissuance_endpoint\nof the issuer with 1P cookies with a content-type ofapplication/x-www-form-urlencoded\ncontaining arequest_token\nparameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=... parameter set to the signed JWT and theSec-Fetch-Dest\nheader set toemail-verification\n. POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail",
    "POST /email-verification/issuance HTTP/1.1\nHost: accounts.issuer.example\nCookie: session=...\nContent-Type: application/x-www-form-urlencoded\nSec-Fetch-Dest: email-verification\nrequest_token=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVC...\nOn receipt of a token request:\n-\n4.1 - the issuer MUST verify the request headers:\nContent-Type\nisapplication/x-www-form-urlencoded\nSec-Fetch-Dest\nisemail-verification\n-\n4.2 - the issuer MUST verify the request_token by:\n- parsing the JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\njwk\nandalg\nfields from the JWT header, and theaud\n,iat\n, andemail\n, claims from the payload - verifying the JWT signature using the\njwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail jwk\nwith thealg\nalgorithm - verifying the\naud\nclaim exactly matches the issuer's identifier - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim contains a syntactically valid email address\n-\n4.3 - the issuer checks if the cookies sent represent a logged in user, and if the logged in user has control of the email provided in the request_token. If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0",
    "If so the issuer generates an SD-JWT with the following properties:\n- Header: MUST contain\nalg\n: signing algorithm (SHOULD match the algorithm from the request_token)kid\n: key identifier of key used to signtyp\nset to \"evp+sd-jwt\"\n- Payload: MUST contain the following claims:\niss\n: the issuer identifieriat\n: issued at timecnf\n: confirmation claim containing the public key from the request_token'sjwk\nfieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0 fieldemail\n: claim containing the email address from the request_tokenemail_verified\n: claim that email is verified per OpenID Connect 1.0\n- Signature: MUST be signed with the issuer's private key corresponding to a public key in the\njwks_uri\nidentified bykid\n- Header: MUST contain\nExample header:\n{\n\"alg\": \"EdDSA\",\n\"kid\": \"2024-08-19\",\n\"typ\": \"evp+sd-jwt\"\n}\nExample payload:\n{\n\"iss\": \"issuer.example\",\n\"iat\": 1724083200,\n\"cnf\": {\n\"jwk\": {\n\"kty\": \"OKP\",\n\"crv\": \"Ed25519\",\n\"x\": \"11qYAYdk9E6z7mT6rk6j1QnXb6pYq4v9wXb6pYq4v9w\"\n}\n},\n\"email\": \"user@example.com\",\n\"email_verified\": true\n}\nThe resulting JWT has the ~\nappended to it, making it a valid SD-JWT. - 4.4 - the issuer returns the SD-JWT to the browser as the value of\nissuance_token\nin anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}",
    "Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"} in anapplication/json\nresponse. Example:\nHTTP/1.1 200 OK\nContent-Type: application/json\n{\"issuance_token\":\"eyJhbGciOiJFZERTQSIsImtpZCI6IjIwMjQtMDgtMTkiLCJ0eXAiOiJ3ZWItaWRlbnRpdHkrc2Qtand0In0...\"}\nIf the issuer cannot process the token request successfully, it MUST return an appropriate HTTP status code with a JSON error response containing an error\nfield and optionally an error_description\nfield. When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid.",
    "When the request does not include the required Content-Type: application/x-www-form-urlencoded\nheader, the server MUST return the 415 HTTP response code\nWhen the request does not include the required Sec-Fetch-Dest: email-verification\nheader:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. {\n\"error\": \"invalid-request\",\n\"error_description\": \"Missing or invalid Sec-Fetch-Dest header\"\n}\nThe error_description\nSHOULD specify that the Sec-Fetch-Dest header is missing or invalid. When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:",
    "When the request lacks valid authentication cookies, contains expired/invalid cookies, or the authenticated user does not have control of the requested email address:\nHTTP 401 Unauthorized\n{\n\"error\": \"authentication_required\",\n\"error_description\": \"User must be authenticated and have control of the requested email address\"\n}\nWhen the request_token\nis malformed, missing required claims, or contains invalid values:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_request\",\n\"error_description\": \"Invalid or malformed request_token\"\n}\nWhen the request_token\nsignature verification fails or the token structure is invalid:\nHTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability: HTTP 400 Bad Request\n{\n\"error\": \"invalid_token\",\n\"error_description\": \"Token signature verification failed or token structure is invalid\"\n}\nFor internal server errors or temporary unavailability:\nHTTP 500 Internal Server Error\n{\n\"error\": \"server_error\",\n\"error_description\": \"Temporary server error, please try again later\"\n}\nIn a future version of this spec, the issuer could prompt the user to login via a URL or with a Passkey request. On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the",
    "On receiving the issuance_token\n:\n-\n5.1 - the browser MUST verify the SD-JWT per (SD-JWT spec) by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT",
    "email\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niat\nclaim is within 60 seconds of the current time - verifying the\nemail\nclaim matches the email address the user selected - verifying the\nemail_verified\nclaim is true\n-\n5.2 - the browser then creates an SD-JWT+KB by:\n- taking the verified SD-JWT from step 5.1 as the base token\n- creating a Key Binding JWT (KB-JWT) with the following structure:\n- Header:\nalg\n: same signing algorithm used by the browser's private keytyp\n: \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT : \"kb+jwt\"\n- Payload:\naud\n: the RP's originnonce\n: the nonce from the originalnavigator.credentials.get()\ncalliat\n: current time when creating the KB-JWTsd_hash\n: SHA-256 hash of the SD-JWT\n- Header:\n- signing the KB-JWT with the browser's private key (the same key pair generated in step 3.4)\n- concatenating the SD-JWT and the KB-JWT separated by a tilde (~) to form the SD-JWT+KB\nExample KB-JWT header:\n{ \"alg\": \"EdDSA\", \"typ\": \"kb+jwt\" }\nExample KB-JWT payload:\n{ \"aud\": \"https://rp.example\", \"nonce\": \"259c5eae-486d-4b0f-b666-2a5b5ce1c925\", \"salt\": \"kR7fY9mP3xQ8wN2vL5jH6tZ1cB4nM9sD8fG3hJ7kL2p\", \"iat\": 1724083260, \"sd_hash\": \"X9yH0Ajrdm1Oij4tWso9UzzKJvPoDxwmuEcO3XAdRC0\" }\n-\n5.3 - the browser sets a TBD hidden field and fires the TBD event ...\ndetails TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n- details TBD\nThe RP web page now has the SD-JWT+KB from the event, and passes it to the RP server, or the token was posted to the RP server. details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:",
    "details TBD\nThe RP server MUST verify the SD-JWT+KB by:\n-\n6.1 - the RP server receives the SD-JWT+KB from the web page\n-\n6.2 - the RP parses the SD-JWT+KB by separating the SD-JWT and KB-JWT components (separated by tilde ~)\n-\n6.3 - the RP verifies the KB-JWT by:\n- parsing the KB-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nfield from the KB-JWT header, and theaud\n,nonce\n,iat\n, andsd_hash\nclaims from the payload - verifying the\naud\nclaim matches the RP's origin - verifying the\nnonce\nclaim matches the nonce from the RP's session with the web page - verifying the\niat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by: iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-",
    "iat\nclaim is within a reasonable time window - computing the SHA-256 hash of the SD-JWT and verifying it matches the\nsd_hash\nclaim\n-\n6.4 - the RP verifies the SD-JWT by:\n- parsing the SD-JWT into header, payload, and signature components\n- confirming the presence of, and extracting the\nalg\nandkid\nfields from the SD-JWT header, and theiss\n,iat\n,cnf\n,email\n, andemail_verified\nclaims from the payload - parsing the email domain from the\nemail\nclaim and looking up theTXT\nrecord for_email-verification.$EMAIL_DOMAIN\nto verify theiss\nclaim matches the issuer identifier in the DNS record - fetching the issuer's public keys from the\njwks_uri\nspecified in the.well-known/email-verification\nfile - verifying the SD-JWT signature using the public key identified by\nkid\nfrom the JWKS with thealg\nalgorithm - verifying the\niss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n- iss\nclaim exactly matches the issuer identifier from the DNS record - verifying the\niat\nclaim is within a reasonable time window - verifying the\nemail_verified\nclaim is true\n-\n6.5 - the RP verifies the KB-JWT signature using the public key from the\ncnf\nclaim in the SD-JWT with thealg\nalgorithm from the KB-JWT header\nBelow are notes capturing some discussions of potential privacy implications. -\nThe email domain operator no longer learns which applications the user is verifying their email address to as the applications are no longer sending an email verification code to the user. By using an SD-JWT+KB, the browser intermediates the request and response so that the issuer does not learn the identity of the RP. -\nThe RP can infer if a user is logged into the issuer as the RP receives a SD-JWT when the user is logged in, and does not when the user is not logged in. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had.",
    "-\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. -\nThe issuer may learn the user has email at a mail domain it is authoritative for that it did not know the user had. The web page would call an API passing the email address and nonce. It would return a promise that resolves to the SD_JWT or an error response. The API would only be callable after a user gesture such as clicking a button labelled verify on the web page. This provides the web page in more flexibility in how to gather the email address. For example, if the web page is using EVP for login, and the user has used different emails for login and those are stored in cookies, the page can display the list of emails and an option to provide a different one. The user can then select the email they want to use rather than having to type it into a text field.",
    "The user can then select the email they want to use rather than having to type it into a text field. In addition to, or instead of the browser sending cookies to the Issuer, the Issuer could return a WebAuthN request to the browser if it has credentials for the user identified by the email address. The browser would then interact with the user and provide the WebAuthN response to the Issuer, authenticating the user, and the Issuer would then return the SD-JWT. Rather than the DNS TXT record, the Mail Domain would host a JSON file in the .wellknown domain. This creates challenges for the long tail of individually owned domains:\n- would require a domain that is used just for email to now have to support a web server\n- the mail domain is usually an apex domain, which does not support CNAME, complicating hosting a web site",
    "Using bubblewrap to add sandboxing to NetBSD",
    "Using bubblewrap to add sandboxing to NetBSD Google Summer of Code 2025 Reports: Using bubblewrap to add sandboxing to NetBSD\nThis report was written by Vasyl Lanko as part of Google Summer of Code 2025. Introduction\nAs of the time of writing, there is no real sandboxing technique available to NetBSD. There is chroot, which can be considered a weak sandbox because it modifies the root directory of the process, effectively restricting the process' view of the file system, but it doesn't isolate anything else, so all networking, IPC, and mounts inside this restricted file system are the same as of the system, and are accessible. There has already been some research on implementing kernel-level isolation in NetBSD with tools like gaols, mult and netbsd-sandbox, but they haven't been merged to NetBSD. Other operating systems have their own ways to isolate programs, FreeBSD has jails, and Linux has namespaces. Project Goals",
    "Project Goals Project Goals\nThe goal of this project is to bring a new way of sandboxing to NetBSD. More specifically, we want to implement a mechanism like Linux namespaces. These namespaces allow the isolation of parts of the system from a namespace, or, as the user sees it, from an application. NetBSD has compat_linux to run Linux binaries on NetBSD systems, and the implementation of namespaces can also be utilized to emulate namespace-related functionality of Linux binaries. A simple example to visualize our intended result is to consider an application running under an isolated UTS namespace that modifies the hostname. From the system's view, the hostname remains the same old hostname, but from the application's view it sees the modified hostname. Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details",
    "Project Implementation\nLinux has 8 namespace types, in this project we will focus on only 2 of them:\n- UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details - UTS namespace, it is the simplest so we can focus on building the general namespace infrastructure with little namespace-specific details\n- mount namespace, it is a prerequisite to most other namespace types because UNIX follows the philosophy of \"everything is a file\", so we need a separate mount namespace to have different configuration files on the same location as the system. Linux creates namespaces via the unshare or clone system calls, and it will also be our way of calling the namespace creation logic. We setup the base for implementing Linux namespaces in the NetBSD kernel using kauth, the subsystem managing all authorization requests inside the kernel. It associates credentials with objects, and because the namespace lifecycle management is related to the credential lifecycle it handles all the credential inheritance and reference counting for us. (Thanks kauth devs!)",
    "(Thanks kauth devs!) We separate the implementation of each namespace in a different secmodel, resulting in a similar framework to Linux which allows the isolation of a single namespace type. Our implementation also allows users to pick whether they want to have namespace support, and of what kind, via compilation flags, just like in Linux. UTS namespace\nUTS stands for UNIX Timesharing System, because it allows multiple users to share a single computer system. Isolating the utsname\ncan be useful to give users the illusion that they have control over the system's hostname, and also, for example, to give different hostnames to virtual servers. The UTS namespace stores the namespace's hostname, domain name, and their lengths. To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace.",
    "To isolate the utsname\nwe need to first create a copy of the current UTS information, plus we need a variable containing the number of credentials referencing this namespace, or, in simpler terms, the reference count of this namespace. This namespace specific information needs to be saved somewhere, and for that we use the credential's private_data\nfield, so we can use a UTS_key\nto save and retrieve UTS\nrelated information from the secmodel. The key specifies the type of information we want to retrieve from the private_data\n, hence using a UTS_key\nfor the UTS namespace. The key for each namespace is a fixed value (we don't create a new key for every credential), but the retrieved value for that key from different credentials may be different. We had to modify kernel code that was directly accessing the hostname\nand domainname\nvariables, to instead call get_uts()\n, which retrieves the UTS struct for the namespace of the calling process. We didn't modify occurrences in kernel drivers because drivers are not part of any namespace, so they should still access the system's resources directly. MNT namespace",
    "MNT namespace MNT namespace\nThe MNT namespace isolates mounts across namespaces. It is used to have different versions of mounted filesystems across namespaces, meaning a user inside a mount namespace can mount and unmount whatever they want without affecting or even breaking the system. The mount namespace structure in Linux is fairly complicated. To have something similar in NetBSD we need to be able to control the mounts accessed by each namespace, and for that we need to control what is each namespace's mountlist, this is also enough for unmounting file systems, because in practice we can just hide them. For the mount_namespace, mountlist structure and the number of credentials using the mount namespace are stored in the credential's private data with the MNT_key\n. Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()",
    "Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist() . Similarly to the UTS namespace, we had to modify kernel code to not directly access the mountlist\n, but instead go through a wrapper called get_mountlist()\nwhich returns the correct mountlist for the namespace the calling process resides in. Implementation for the mount namespace is immensely more complex than for the UTS namespace, it involves having a good understanding of both Linux and NetBSD behaviour, and I would frequently find myself wondering how to implement something after reading the Linux man pages, which would lead to me looking for it in the Linux source code, understanding it, then going back to NetBSD source code, trying to implement it, and seeing it's too different to implement in the same way. Project Status\nYou can find all code written during this project in GitHub at maksymlanko/netbsd-src gsoc-bubblewrap\nbranch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap",
    "Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap branch. Because I intend to continue this work outside of GSoC, I want to reinforce that this was the last commit still during GSoC on gsoc-bubblewrap\nbranch and this was the last one for the mnt_ns\nstill WIP branch. The link includes implementation of general namespace code via secmodels, implementation of the UTS namespace and related ATF-tests, and the work-in-progress implementation of mount namespaces. The mount namespace functionality is not finished as it would require much more work than the time available for this project. To complete it, it would be required invasive and non-trivial changes to the original source code, and, of course, more time. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement.",
    "Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. Future Work\nAs previously mentioned, Linux has 8 namespace types, it is important to see which of the missing namespaces are considered useful and feasible to implement. I believe that after mount namespaces it would be interesting to implement PID namespaces as this in combination with mount namespaces would permit process isolation from this sandbox. Afterwards, implementing user namespaces would allow users to get capabilities similar to root\nin the namespace, giving them sudo\npermissions while still restricting system-wide actions like shutting down the machine. A lower hanging fruit is to implement the namespace management functionality, which in Linux is lsns to list existing namespaces, and setns to move the current process to an already existing namespace. Challenges\n- Semantics. Did you know the unmount system call with MNT_FORCE flag in Linux (usually) returns EBUSY, and in NetBSD it forces the unmounting? One of them makes it easier to implement mount namespaces.",
    "One of them makes it easier to implement mount namespaces. - The behaviour of namespaces is not fully specified in the man pages. If something is not clear from the man pages you need to read the source code. - Unexpected need to learn a lot of VFS concepts and their differences in NetBSD and Linux. - There was a much bigger research component than I anticipated. In the end, Linux and NetBSD are different operating systems, implemented in different ways. Linux is complex and it is not trivial to port namespaces to NetBSD. Notes\nThe project is called \"Using bubblewrap to add sandboxing to NetBSD\" and was initially projected to emulate the unshare\nsystem call into compat_linux\n, but, seeing that having namespaces could be useful for NetBSD, and that it would be easy to add to compat_linux\nafterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap",
    "Implementing other system calls necessary to make the bwrap afterwards, we decided to instead implement namespaces directly in the NetBSD kernel. Implementing other system calls necessary to make the bwrap\nlinux binary work correctly also wouldn't be as satisfying as implementing namespaces directly into NetBSD, so this was why the project was initially called \"Using bubblewrap to add sandboxing to NetBSD\" but nowadays it would be more accurate to call it \"Sandboxing in NetBSD with Linux-like namespaces\". Thanks\nI am very grateful to Google for Google Summer of Code, because without it I wouldn't have learned so much this summer, wouldn't have met with smart and interesting people, and for sure wouldn't have tried to contribute to a project like NetBSD, even if I always wanted to write operating systems code... But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects.",
    "But, the biggest thing I will take with me from this project is the confidence to be able to contribute to NetBSD and other open source projects. I would also like to thank the members of the NetBSD organization for helping me throughout this project, and more specifically:\n- Taylor R. Campbell, Harold Gutch and Nia Alarie from IRC, for helping me fix a nasty\nLD_LIBRARY_PATH\nbug I had on my system which wouldn't let me finish compiling NetBSD, and general GSoC recomendations. - Emmanuel Dreyfus from\ntech-kern\n, with whom I discussed ideas for projects and proposal suggestions, and in the end inspired the namespaces project. - Christoph Badura and Leonardo Taccari who volunteered to be my mentors. They took time to research and answer my questions, anticipated possible problems in my approaches, and always pointed me in the right direction, daily, during all of GSoC's period. This project is from the 3 of us.",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
    "Montana Becomes First State to Enshrine 'Right to Compute' into Law Montana has made history as the first state in the U.S. to legally protect its citizens\u2019 right to access and use computational tools and artificial intelligence technologies. Governor Greg Gianforte signed Senate Bill 212, officially known as the Montana Right to Compute Act (MRTCA), into law. The groundbreaking legislation affirms Montanans\u2019 fundamental right to own and operate computational resources \u2014 including hardware, software, and AI tools \u2014 under the state\u2019s constitutional protections for property and free expression. Supporters of the bill say it represents a major step in securing digital freedoms in an increasingly AI-driven world. \u201cMontana is once again leading the way in defending individual liberty,\u201d said Senator Daniel Zolnikov, the bill\u2019s sponsor and a longtime advocate for digital privacy. \u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d",
    "\u201cWith the Right to Compute Act, we are ensuring that every Montanan can access and control the tools of the future.\u201d While the law allows state regulation of computation in the interest of public health and safety, it sets a high bar: any restrictions must be demonstrably necessary and narrowly tailored to serve a compelling interest. Legal experts note that this is one of the most protective standards available under Montana law. The act also includes provisions for AI-controlled critical infrastructure, requiring both a \u201cshutdown mechanism\u201d to allow human control and annual safety reviews \u2014 a move aimed at balancing innovation with public safety concerns. The bill has drawn praise from privacy advocates and tech policy groups. Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d",
    "Tanner Avery, Policy Director at the free-market think tank Frontier Institute, called the law a \u201cflag in the ground\u201d for digital rights, adding: \u201cMontana has made clear it will treat any attempt to infringe on fundamental digital freedoms with the utmost scrutiny.\u201d The MRTCA stands in stark contrast to recent regulatory efforts in other states, such as California, Virginia, and New York, where proposals to rein in AI technologies have either failed or been heavily revised. Montana\u2019s approach leans toward empowering individual users rather than restricting access. The law has already inspired similar efforts in New Hampshire, where lawmakers are pushing a constitutional amendment guaranteeing access to computation. Rep. Keith Ammon, the state\u2019s Majority Floor Leader, praised Montana\u2019s leadership: \u201cThis is the kind of bold move that sets the tone for the rest of the country.\u201d\nNationally, the Right to Compute movement is gaining traction. Spearheaded by the grassroots group RightToCompute.ai, the campaign argues that computation \u2014 like speech and property \u2014 is a fundamental human right. \u201cA computer is an extension of the human capacity to think,\u201d the organization states.",
    "\u201cA computer is an extension of the human capacity to think,\u201d the organization states. The movement is supported by Haltia.AI, a Dubai-based AI startup, and the ASIMOV Protocol, a blockchain consortium advocating for decentralized AI infrastructure. Talal Thabet, Co-Founder of both groups, praised Montana\u2019s law as \u201ca monumental step forward in ensuring individuals retain control of their own data and digital tools.\u201d\nAs debates over AI governance and digital rights continue to evolve, Montana\u2019s bold new law could serve as a blueprint for other states seeking to safeguard freedom in the digital era.",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
    "Zensical \u2013 A modern static site generator built by the Material for MkDocs team Zensical \u2013 A modern static site generator built by the Material for MkDocs team\u00b6\nWe are thrilled to announce Zensical, our next-gen static site generator designed to simplify the process of building documentation sites. Distilled from a decade of experience, Zensical is our effort to overcome the technical limitations of MkDocs, reaching far beyond its capabilities. Zensical is the result of thousands of hours of work \u2013 built from the ground up for a modern and comfortable authoring experience, while making it easy for developers to extend and customize Zensical through its upcoming module system. Our goal is to support docs-as-code workflows with tens of thousands of pages, without compromising performance or usability. To make the transition seamless, compatibility comes first. We're putting significant effort into ensuring a smooth migration from Material for MkDocs for all users. Zensical can natively read mkdocs.yml",
    "Zensical can natively read mkdocs.yml , allowing you to build your existing project with minimal changes. As of now, a subset of plugins is supported, and we're working on feature parity in the coming months. Zensical is fully Open Source, licensed under MIT, and can be used for any purpose, including for commercial use. We're also saying goodbye to our sponsorware model, replacing it with our new offering for professional users: Zensical Spark. This allows us to stay independent, maximizing user value, as we shape the future of Zensical together with you. You can subscribe to our newsletter to stay in the loop. This is the second article in a four-part series:\n- Transforming Material for MkDocs\n- Zensical \u2013 A modern static site generator built by the creators of Material for MkDocs. - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6",
    "- What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6 - What happens to the features in Insiders coming November 11, 2025\n- A path forward for our community coming November 18, 2025\nWhy Zensical?\u00b6\nSince its initial release in 2016, Material for MkDocs has helped tens of thousands of teams to publish and maintain reliable documentation. However, in recent years, it has become apparent that we were running up against limitations of our core dependency, MkDocs. These limitations proved impossible to overcome as they are deeply rooted in its architecture. We also mentioned in our update on our foundational work that MkDocs must be considered a supply chain risk, since it's unmaintained since August 2024. It has seen no releases in over a year and is accumulating unresolved issues and pull requests. These developments have forced us to cut our ties to MkDocs as a dependency.",
    "These developments have forced us to cut our ties to MkDocs as a dependency. In order to map out a path forward, we went back to the drawing board, talked to dozens of our professional users and thoroughly analyzed the MkDocs ecosystem. We didn't just want to create a fork or port of MkDocs, but decided to rethink static site generation from first principles. With Zensical, we are creating a modern static site generator, which is compatible with your content and customizations, and addresses MkDocs' limitations. While Material for MkDocs is built on top of MkDocs, Zensical consolidates both projects into one coherent stack, covering static site generation, theming, and customization. What you can expect today:\nAlthough we haven't reached full feature parity yet, you can already use Zensical to build your existing Material for MkDocs projects with minimal changes. You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6",
    "What you can expect\u00b6\nSolid foundation\u00b6 You can jump to the compatibility section to learn what is already supported. What you can expect\u00b6\nSolid foundation\u00b6\nOur goal with Zensical is to create a coherent and modern stack, vertically integrating all parts of the authoring experience (AX), developer experience (DX), and user experience (UX). This gives us a significant competitive advantage over solutions that overly rely on third-party frameworks and dependencies, helping us to create much more robust Open Source software. ZRX, our new differential build engine, creates a solid foundation for Zensical, and is an Open Source project of its own. It's a fresh take on making differential data flows easy to build and a joy to work with. Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster.",
    "Most engineering effort has gone into ZRX, as it forms the backbone of Zensical, and will allow us to ship features faster. Following the principle of architectural hoisting, we moved essential, reusable functionality into ZRX, which allows us to keep Zensical's core simple and focused on static site generation. ZRX handles the heavy lifting \u2013 differential builds, caching, and data flow orchestration. With the upcoming module system and component system, both of which are on our public roadmap, Zensical will gain more degrees of freedom in the coming months, allowing you to extend and customize Zensical in ways that were previously impossible with MkDocs. Modern design\u00b6\nZensical brings a fresh, modern design that breaks out of the Materal Design aesthetic, creating a visual foundation that is more easily brandable and adaptable to different use cases. The new design prioritizes clarity, simplicity, and usability, while having a more professional finish:",
    "The new design prioritizes clarity, simplicity, and usability, while having a more professional finish: Right now, the layout and site structure of Zensical match Material for MkDocs closely, as we're focusing on ensuring maximum compatibility. Once we finish work on our upcoming component system, we'll provide an alternative that is much more flexible and adaptable, and can be tailored to different use cases and branding requirements more easily. You can also keep the Material for MkDocs look and feel with a single line of configuration. Blazing-fast search\u00b6\nClient-side search isn't a compromise \u2013 for the vast majority of static sites, it's the best solution, since it's faster, involves zero maintenance, and doesn't require you to pay for a service. As covered in depth in the first part of this series, the current search implementation in Material for MkDocs has severe limitations, and is based on a now unmaintained library, which is why we decided to build a new search engine from scratch. It's based on the same goals as Zensical itself: performance, flexibility, and extensibility.",
    "It's based on the same goals as Zensical itself: performance, flexibility, and extensibility. Disco, our modular and blazing-fast client-side search engine, is exclusively available in Zensical. When you build your site with Zensical, your users will immediately benefit from Disco's improved ranking algorithm, as well as its filtering and aggregation capabilities:\nIn early 2026, we'll be releasing Disco as a standalone Open Source project. With the feedback of our professional users in Zensical Spark, we're going to evolve the search experience, turning Disco into a highly configurable and customizable search engine that adapts to your needs. You can subscribe to our newsletter to receive news about Disco. Authoring experience\u00b6\nSlow feedback loops can be a major pain point when writing documentation. Almost all of us know the feeling of waiting for the static site generator to finish building the site, just to see a small change reflected in the output. With Zensical, we're finally addressing this issue.",
    "With Zensical, we're finally addressing this issue. It's important to understand that we're not yet utilizing the differential capabilities of ZRX to the fullest extent, as we're forced to make several compromises to ensure maximum compatibility with Material for MkDocs at the moment. Markdown rendering needs to go through Python Markdown, which forces us to pay for extra marshalling costs. While the initial build can sometimes be slower than with MkDocs, repeated builds \u2013 especially when serving the site \u2013 are already 4 to 5x faster, as only changed files need to be rebuilt. We're also working on a new Markdown toolchain based on a CommonMark-compliant parser written in Rust, which will make Markdown processing significantly faster. We'll be tackling this as part of the upcoming component system, which we'll start working on in early 2026. Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content.",
    "Once our new Markdown toolchain is ready, we'll provide automated tools to translate between Python Markdown and CommonMark, so you don't need to manually migrate your content. Maximum compatibility\u00b6\nCompatibility with Material for MkDocs is our top priority. We understand that switching to a new static site generator can be challenging, especially for large projects with many customizations. Therefore, we've put significant effort into ensuring that Zensical understands mkdocs.yml\nconfiguration files, so that you can build your projects with minimal changes. This means your existing Markdown files, template overrides, CSS and JavaScript extensions don't need to be touched, primarily because we did not change the generated HTML, and rely on Python Markdown for processing your content. However, plugins are a different story. In MkDocs, practically all plugins have side effects, making it impossible to parallelize builds. We started from first principles and asked: what should extensibility look like in a modern static site generator? Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles:",
    "Our answer is the upcoming module system, which takes a fundamentally different approach based on four core principles: - Modules can inject, extend, and re-define functionality\n- Modules are deterministic through topological ordering\n- Modules foster reusability, with the possibility to remix them\n- Modules can cooperate through well-defined contracts\nWe're working on shipping essential functionality as provided by MkDocs plugins as built-in modules. In early 2026, we will open the module system to third-party developers, so they can start building their own modules, as we see Zensical as the heart of a thriving ecosystem. Zensical Spark\u00b6\nZensical Spark, our offering for professionals, is the result of countless calls with professional users of Material for MkDocs. From startups to large enterprises, we enable organizations to realize complex projects in diverse environments. For this, we've created Zensical Spark as a collaborative space. If you're a professional user, Zensical Spark is for you, since:\n-",
    "If you're a professional user, Zensical Spark is for you, since:\n- -\nYou can be confident that Zensical will continue to be developed and maintained in the long term as a set of interconnected and sustainable OSI-compliant Open Source projects. -\nYou can receive the support you need to successfully use, configure and customize Zensical in your organization, receiving first-class support from the Zensical team. -\nYou can influence the future development of Zensical by participating in our new approach to Open Source software development, helping us to build exactly what you need. Let's talk! If you're working in a professional context, reach out to contact@zensical.org to schedule a call and learn how Zensical Spark enables your team to transition to Zensical smoothly and have a voice in its continued development. You should also consider joining the waiting list, since seats are limited. We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical!",
    "We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! We're growing our team\u00b6\nWe're also excited to announce that we're growing our team:\nTimoth\u00e9e Mazzucotelli, also known as @pawamoy, is joining Zensical! At Zensical, Tim is focusing on providing the same seamless experience for generating API reference documentation from source code (via docstrings) as he has done with mkdocstrings, the second biggest project in the MkDocs ecosystem. With his expertise, and Zensical's new stack, we'll be pushing the boundaries of what's possible with API reference documentation. Goodbye, GitHub Sponsors\u00b6\nThank you! To all of you who have supported us over the years through GitHub Sponsors \u2013 we are incredibly grateful for your support. It has been invaluable in helping us to build, maintain and evolve Material for MkDocs, and we couldn't have done it without you. Seriously, thank you!",
    "Seriously, thank you! Material for MkDocs gave us something invaluable: experience building for tens of thousands of users, and the opportunity to build a team around Open Source software. It showed us that making a living from Open Source isn't just possible \u2013 we grew it into one of the largest sponsorware projects on GitHub and inspired others to pursue similar paths. Now we're breaking new ground. Zensical is our next chapter, and we're professionalizing how we approach Open Source development. Our vision is to make Zensical free for everyone to use while building a sustainable business around it through our new approach. This transition means saying goodbye to GitHub Sponsors. It has served us exceptionally well, but as we professionalize and scale, we're making the leap from personal project to company \u2013 building a business and team that can meet the growing demands of professional users while staying true to our values. We're doubling down on Open Source, developing software for everyone.",
    "We're doubling down on Open Source, developing software for everyone. We're doubling down on Open Source, developing software for everyone. If you want to continue supporting our work, please subscribe to our newsletter. We'll be providing new methods to support us in the coming months, with the possibility of getting exclusive goodies. Looking Ahead\u00b6\nMaterial for MkDocs grew organically in a pot that eventually became too small. With Zensical, we're building on solid foundations designed to grow with us \u2013 and with you. Material for MkDocs is now in maintenance mode\nWe want to be transparent about the risks of staying on Material for MkDocs. With MkDocs unmaintained and facing fundamental supply chain concerns, we cannot guarantee Material for MkDocs will continue working reliably in the future. We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical.",
    "We're aware that transitioning takes time, which is why we commit to support it at least for the next 12 months, fixing critical bugs and security vulnerabilities as needed, but the path forward is with Zensical. If documentation plays a critical role in your organization, and you're worried how this might affect your business, consider joining Zensical Spark, or feel free to schedule a call by reaching out at contact@zensical.org. Where we'll be in 12 months\u00b6\nOver the next 12 months, following our phased transition strategy, we'll reach Phase 2 and 3 \u2013 introducing our module system and component system, as well as CommonMark support. By replacing Python Markdown with a Rust-based Markdown parser, we'll unlock performance improvements and the modularity needed for flexible templating. This is where Zensical truly starts to unfold its capabilities. Zensical is already powering real projects due to extensive compatibility with Material for MkDocs. We're actively working on closing the gap to reach full feature parity. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help.",
    "If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. You can install Zensical now, and build your existing Material for MkDocs projects with it. If you run into a bug, please don't hesitate to open an issue \u2013 we're here to help. Connect with us\u00b6\nIf you have questions we haven't addressed, please reach out to us at contact@zensical.org. We're currently collecting questions from the community about Zensical, and will address them in an FAQ section as part of our documentation in the coming weeks. We're incredibly thankful that you have been part of our journey so far. With Zensical, we're embarking on a new chapter, and we couldn't be more excited to have you with us. You can subscribe to our newsletter to stay in the loop.",
    "I Am Mark Zuckerberg",
    "I Am Mark Zuckerberg Welcome to iammarkzuckerg.com\nNo, not THAT Mark Zuckerberg-this one's busy helping Hoosiers, not launching social networks. Relax, you haven't accidentally logged into Facebook or the Metaverse. You're on the site of Mark S. Zuckerberg, Indiana's original bearer of the name, proud bankruptcy attorney, and frequent recipient of confused emails from people seeking tech support or handouts of money. What I Really Do:\n- Help people obtain a fresh financial start (no passwords required)\n- Offer dependable, human-involved advice (my artificial intelligence is powered by coffee)\n- Answer local legal questions, not privacy scandals\nReal Zuckerberg Facts:\n- Shares a name, not fortune, with the Facebook founder\n- Gets mistaken daily for a tech billionaire\n- Has written zero social media apps, but plenty of court briefs\nFun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place!",
    "But if you need trustworthy bankruptcy help, you're in exactly the right place! Fun Fact:\nIn Indiana, saying \"I'm Mark Zuckerberg\" gets more laughs than likes. But if you need trustworthy bankruptcy help, you're in exactly the right place! Click around, get to know your (non-billionaire) local Mark, and remember: No login required. Click Here to See How Other\nWebsites Have Reacted to This\nInteresting Things That Have Happened to Me Because My Name is Mark Zuckerberg\nFor a complete list of things that have happened to Mark Zuckerberg click here\nLike I said, I don't wish Mark E. Zuckerberg any ill will at all. I hope the best for him, but let me tell you this: I will rule the search for \"Mark Zuckerberg bankruptcy\". And if he does fall upon difficult financial times, and happens to be in Indiana, I will gladly handle his case in honor of our eponymy.",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
    "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel Ironclad is a (partially) formally verified, real-time capable, UNIX-like operating system kernel for general-purpose and embedded uses. It is written in SPARK and Ada, and is comprised of 100% free software. Ironclad features a familiar POSIX-compatible interface, true simultaneous preemptive multitasking, Mandatory Access Control (MAC), and support for hard real-time scheduling. Ironclad is fully open source and distributed under the GPLv3, ensuring it remains free. No firmware blobs are needed or shipped with the kernel. Every piece of the stack is open source. SPARK's state of the art formal verification is employed for ensuring absence of errors and correctness of big portions of Ironclad, like cryptography, MAC, and user-facing facilities. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation.",
    "Dependency on only the GNU toolchain allows for easy cross-compilation. Ported to several platforms and boards, and designed to be easily portable to many more. Dependency on only the GNU toolchain allows for easy cross-compilation. Ironclad will always be free for use, study, and modification, so, to support the project, we rely on the use of donations and grants. Every contribution makes a difference and allows us to do more. This project is funded through NGI Zero Core, a fund established by NLnet with financial support from the European Commission's Next Generation Internet program. Learn more at the NLnet project page. Additionally, we would like to thank the following organizations:"
  ],
  "metadata": [
    {
      "article_id": "6468270047648745042",
      "chunk_id": 0,
      "title": "How AI generated code compounds technical debt",
      "topics": [
        "AI",
        "Technology"
      ],
      "source": "news",
      "url": "https://leaddev.com/technical-direction/how-ai-generated-code-accelerates-technical-debt",
      "added_at": "2025-11-09T12:01:24.411434"
    },
    {
      "article_id": "6468270047648745042",
      "chunk_id": 1,
      "title": "How AI generated code compounds technical debt",
      "topics": [
        "AI",
        "Technology"
      ],
      "source": "news",
      "url": "https://leaddev.com/technical-direction/how-ai-generated-code-accelerates-technical-debt",
      "added_at": "2025-11-09T12:01:24.820594"
    },
    {
      "article_id": "6468270047648745042",
      "chunk_id": 2,
      "title": "How AI generated code compounds technical debt",
      "topics": [
        "AI",
        "Technology"
      ],
      "source": "news",
      "url": "https://leaddev.com/technical-direction/how-ai-generated-code-accelerates-technical-debt",
      "added_at": "2025-11-09T12:01:25.126173"
    },
    {
      "article_id": "6468270047648745042",
      "chunk_id": 3,
      "title": "How AI generated code compounds technical debt",
      "topics": [
        "AI",
        "Technology"
      ],
      "source": "news",
      "url": "https://leaddev.com/technical-direction/how-ai-generated-code-accelerates-technical-debt",
      "added_at": "2025-11-09T12:01:26.150198"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 0,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:47.053167"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 1,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:47.361562"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 2,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:47.564209"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 3,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:47.872183"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 4,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:48.180725"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 5,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:48.486433"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 6,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:48.794964"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 7,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:49.101697"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 8,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:49.408001"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 9,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:49.715226"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 10,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:50.637616"
    },
    {
      "article_id": "9031711621983707363",
      "chunk_id": 11,
      "title": "Monorepo vs Multi-repo vs Git submodule vs Git Subtree",
      "topics": [
        "Technology",
        "Code",
        "Software",
        "Dev"
      ],
      "source": "news",
      "url": "https://levelup.gitconnected.com/monorepo-vs-multi-repo-vs-git-submodule-vs-git-subtree-a-complete-guide-for-developers-961535aa6d4c",
      "added_at": "2025-11-09T12:03:50.945173"
    },
    {
      "article_id": "8724880553008199896",
      "chunk_id": 0,
      "title": "Visualize FastAPI endpoints with FastAPI-Voyager",
      "topics": [
        "web development",
        "FastAPI",
        "data visualization",
        "user interaction",
        "frontend"
      ],
      "source": "news",
      "url": "https://www.newsyeah.fun/voyager/",
      "added_at": "2025-11-09T15:36:29.913359"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 0,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:32.880574"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 1,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:33.596963"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 2,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:33.902488"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 3,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:34.313742"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 4,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:34.721656"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 5,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:35.030281"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 6,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:35.335982"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 7,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:35.538746"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 8,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:36.155179"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 9,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:36.462339"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 10,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:36.770416"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 11,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:37.179200"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 12,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:37.487515"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 13,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:37.794019"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 14,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:38.102212"
    },
    {
      "article_id": "2394154087717237900",
      "chunk_id": 15,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "history",
        "technology",
        "investigation",
        "gaming",
        "packaging"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:36:38.408027"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 0,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:40.967907"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 1,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:41.276561"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 2,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:41.787231"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 3,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:42.545999"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 4,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:42.811064"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 5,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:43.118281"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 6,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:43.427143"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 7,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:43.674346"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 8,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:44.040173"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 9,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:44.347201"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 10,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:45.371162"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 11,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:45.679384"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 12,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:45.985464"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 13,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:46.395260"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 14,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:46.702274"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 15,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:47.009555"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 16,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:47.317173"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 17,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:47.621996"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 18,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:47.931456"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 19,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:48.238436"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 20,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:48.444757"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 21,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:48.694025"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 22,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:48.882331"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 23,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:49.314478"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 24,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:49.571012"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 25,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:49.980027"
    },
    {
      "article_id": "1869146048070042214",
      "chunk_id": 26,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "online security",
        "web activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:36:50.286282"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 0,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:53.972985"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 1,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:54.281184"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 2,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:54.587256"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 3,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:54.894903"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 4,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:55.201691"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 5,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:55.611450"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 6,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:55.918509"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 7,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:56.225614"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 8,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:56.533906"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 9,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:56.840202"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 10,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:57.147028"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 11,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:57.454537"
    },
    {
      "article_id": "8805740121820524253",
      "chunk_id": 12,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "Google Summer of Code",
        "2025",
        "Reports",
        "bubblewrap",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:36:57.761446"
    },
    {
      "article_id": "1826134407154398333",
      "chunk_id": 0,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "U.S.",
        "legal protection",
        "citizens' rights"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:36:59.912210"
    },
    {
      "article_id": "1826134407154398333",
      "chunk_id": 1,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "U.S.",
        "legal protection",
        "citizens' rights"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:37:00.219365"
    },
    {
      "article_id": "1826134407154398333",
      "chunk_id": 2,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "U.S.",
        "legal protection",
        "citizens' rights"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:37:00.526777"
    },
    {
      "article_id": "1826134407154398333",
      "chunk_id": 3,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "U.S.",
        "legal protection",
        "citizens' rights"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:37:00.833768"
    },
    {
      "article_id": "1826134407154398333",
      "chunk_id": 4,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "U.S.",
        "legal protection",
        "citizens' rights"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:37:01.142283"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 0,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:02.983926"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 1,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:03.292475"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 2,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:03.599345"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 3,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:03.906102"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 4,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:04.213934"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 5,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:04.520347"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 6,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:04.828361"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 7,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:05.441914"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 8,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:05.682919"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 9,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:05.954037"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 10,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:06.260946"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 11,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:06.569154"
    },
    {
      "article_id": "2132317040834157480",
      "chunk_id": 12,
      "title": "Show HN: Pipeflow-PHP \u2013 Automate anything with pipelines even non-devs can edit",
      "topics": [
        "PHP",
        "pipeline engine",
        "lightweight",
        "applications",
        "automation"
      ],
      "source": "news",
      "url": "https://github.com/marcosiino/pipeflow-php",
      "added_at": "2025-11-09T15:37:06.875220"
    },
    {
      "article_id": "1886671023262003124",
      "chunk_id": 0,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:37:09.128063"
    },
    {
      "article_id": "1886671023262003124",
      "chunk_id": 1,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:37:09.437193"
    },
    {
      "article_id": "1886671023262003124",
      "chunk_id": 2,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:37:09.643676"
    },
    {
      "article_id": "380610091005562212",
      "chunk_id": 0,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "UNIX-like",
        "operating system kernel"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:37:10.972442"
    },
    {
      "article_id": "380610091005562212",
      "chunk_id": 1,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "UNIX-like",
        "operating system kernel"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:37:11.278484"
    },
    {
      "article_id": "380610091005562212",
      "chunk_id": 2,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "UNIX-like",
        "operating system kernel"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:37:11.585637"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 0,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:12.610750"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 1,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:12.917657"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 2,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:13.327588"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 3,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:13.633798"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 4,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:14.045171"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 5,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:14.350797"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 6,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:14.658618"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 7,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:15.169760"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 8,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:15.477734"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 9,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:15.784142"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 10,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:16.092100"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 11,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:16.400023"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 12,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:16.705915"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 13,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:17.014011"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 14,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:17.320326"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 15,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:17.627273"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 16,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:17.935904"
    },
    {
      "article_id": "5192065637015062036",
      "chunk_id": 17,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "Zensical",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:37:18.856200"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 0,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:07.258153"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 1,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:07.562835"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 2,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:07.870367"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 3,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:08.186580"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 4,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:08.586165"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 5,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:08.786326"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 6,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:09.098650"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 7,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:09.609306"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 8,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:10.027339"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 9,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:10.330610"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 10,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:10.633167"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 11,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:10.940773"
    },
    {
      "article_id": "670343975519548186",
      "chunk_id": 12,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:48:11.247717"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 0,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:12.681080"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 1,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:13.092241"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 2,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:13.398122"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 3,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:13.706334"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 4,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:14.012671"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 5,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:14.319700"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 6,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:14.627981"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 7,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:14.933997"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 8,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:15.241201"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 9,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:15.548837"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 10,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:15.855972"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 11,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:16.777641"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 12,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:17.086279"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 13,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:17.285690"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 14,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:17.599754"
    },
    {
      "article_id": "5336453069579567915",
      "chunk_id": 15,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "technology",
        "detective work",
        "internet history"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:48:17.905143"
    },
    {
      "article_id": "7052011817839275389",
      "chunk_id": 0,
      "title": "Visualize FastAPI endpoints with FastAPI-Voyager",
      "topics": [
        "web development",
        "API",
        "data visualization",
        "interactive features"
      ],
      "source": "news",
      "url": "https://www.newsyeah.fun/voyager/",
      "added_at": "2025-11-09T15:48:20.874686"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 0,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:23.023992"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 1,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:23.332057"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 2,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:23.945685"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 3,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:24.252505"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 4,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:24.662950"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 5,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:25.071842"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 6,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:25.583717"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 7,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:25.890629"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 8,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:26.301976"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 9,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:26.710162"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 10,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:27.119980"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 11,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:27.427625"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 12,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:27.734191"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 13,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:28.041748"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 14,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:28.348561"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 15,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:28.656667"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 16,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:29.168115"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 17,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:29.577320"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 18,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:29.884499"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 19,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:30.192914"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 20,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:30.498985"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 21,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:30.807097"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 22,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:31.215740"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 23,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:31.523054"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 24,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:31.830493"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 25,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:32.137530"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 26,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:48:32.451952"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 0,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:34.463651"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 1,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:34.676493"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 2,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:34.900240"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 3,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:35.157095"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 4,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:35.618734"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 5,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:35.926184"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 6,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:36.335494"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 7,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:36.643173"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 8,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:36.951526"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 9,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:37.564476"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 10,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:37.808634"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 11,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:38.178692"
    },
    {
      "article_id": "3222392749056923458",
      "chunk_id": 12,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "software development",
        "Google Summer of Code",
        "NetBSD",
        "sandboxing",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:48:38.486036"
    },
    {
      "article_id": "2738018305339901175",
      "chunk_id": 0,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:48:41.149740"
    },
    {
      "article_id": "2738018305339901175",
      "chunk_id": 1,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:48:41.459665"
    },
    {
      "article_id": "2738018305339901175",
      "chunk_id": 2,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:48:41.762892"
    },
    {
      "article_id": "2738018305339901175",
      "chunk_id": 3,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:48:42.048367"
    },
    {
      "article_id": "2738018305339901175",
      "chunk_id": 4,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:48:42.274768"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 0,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:43.298818"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 1,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:43.504895"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 2,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:43.810809"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 3,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:44.118177"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 4,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:44.447645"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 5,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:44.732603"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 6,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:45.039473"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 7,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:45.346891"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 8,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:45.654470"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 9,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:45.962877"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 10,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:46.268964"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 11,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:46.575469"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 12,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:46.883093"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 13,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:47.190933"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 14,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:47.599745"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 15,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:47.908011"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 16,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:48.214288"
    },
    {
      "article_id": "5774591527350371752",
      "chunk_id": 17,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software development",
        "static site generator",
        "Material for MkDocs"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:48:48.521698"
    },
    {
      "article_id": "7664770114481337322",
      "chunk_id": 0,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "Hoosiers",
        "assistance"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:48:50.774302"
    },
    {
      "article_id": "7664770114481337322",
      "chunk_id": 1,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "Hoosiers",
        "assistance"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:48:51.081277"
    },
    {
      "article_id": "7664770114481337322",
      "chunk_id": 2,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "Hoosiers",
        "assistance"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:48:51.388601"
    },
    {
      "article_id": "6006956938719801033",
      "chunk_id": 0,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "operating system",
        "kernel",
        "UNIX-like"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:48:53.027826"
    },
    {
      "article_id": "6006956938719801033",
      "chunk_id": 1,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "operating system",
        "kernel",
        "UNIX-like"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:48:53.334088"
    },
    {
      "article_id": "6006956938719801033",
      "chunk_id": 2,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "formally verified",
        "real-time capable",
        "operating system",
        "kernel",
        "UNIX-like"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:48:53.643837"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 0,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:34.037466"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 1,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:34.758345"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 2,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:35.263729"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 3,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:35.776815"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 4,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:36.200654"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 5,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:36.805422"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 6,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:37.211836"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 7,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:37.532795"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 8,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:37.825106"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 9,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:38.132344"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 10,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:38.465413"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 11,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:39.003592"
    },
    {
      "article_id": "3962670354649777398",
      "chunk_id": 12,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "Numerical indexes",
        "Classification",
        "Bibliographic databases"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:55:39.489244"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 0,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:41.407448"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 1,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:41.717165"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 2,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:42.041162"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 3,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:42.335326"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 4,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:42.572814"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 5,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:43.306842"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 6,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:43.767679"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 7,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:44.286163"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 8,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:44.630143"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 9,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:44.914967"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 10,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:45.166191"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 11,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:45.606113"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 12,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:45.919970"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 13,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:46.248791"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 14,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:46.529148"
    },
    {
      "article_id": "295890002928905557",
      "chunk_id": 15,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "Internet history",
        "Game development",
        "Investigative journalism"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:55:46.835040"
    },
    {
      "article_id": "6860706918762962110",
      "chunk_id": 0,
      "title": "Visualize FastAPI endpoints with FastAPI-Voyager",
      "topics": [
        "web development",
        "FastAPI",
        "visualization",
        "user interaction",
        "data exploration"
      ],
      "source": "news",
      "url": "https://www.newsyeah.fun/voyager/",
      "added_at": "2025-11-09T15:55:49.197300"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 0,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:51.293817"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 1,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:51.552851"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 2,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:52.162280"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 3,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:52.468896"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 4,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:52.775400"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 5,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:53.081176"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 6,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:53.415058"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 7,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:53.802155"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 8,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:54.311536"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 9,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:54.618758"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 10,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:54.973823"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 11,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:55.232155"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 12,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:55.551175"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 13,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:56.280743"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 14,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:56.584463"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 15,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:57.037866"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 16,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:57.296840"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 17,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:57.590646"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 18,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:58.117823"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 19,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:58.421246"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 20,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:55:58.739500"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 21,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:00.079530"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 22,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:00.510040"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 23,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:00.848383"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 24,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:01.228016"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 25,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:01.451040"
    },
    {
      "article_id": "6984213789381143306",
      "chunk_id": 26,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "web activity",
        "control verification"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:56:01.794825"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 0,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:03.355195"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 1,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:03.569373"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 2,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:03.823223"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 3,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:04.080243"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 4,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:04.391098"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 5,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:04.646917"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 6,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:04.886758"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 7,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:05.093293"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 8,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:05.402593"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 9,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:05.739358"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 10,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:05.932708"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 11,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:06.170332"
    },
    {
      "article_id": "1596958057563952222",
      "chunk_id": 12,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:56:06.394229"
    },
    {
      "article_id": "5413222526982348809",
      "chunk_id": 0,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "legal protection",
        "Montana",
        "citizens' rights",
        "history"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:56:09.458169"
    },
    {
      "article_id": "5413222526982348809",
      "chunk_id": 1,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "legal protection",
        "Montana",
        "citizens' rights",
        "history"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:56:09.764346"
    },
    {
      "article_id": "5413222526982348809",
      "chunk_id": 2,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "legal protection",
        "Montana",
        "citizens' rights",
        "history"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:56:10.072464"
    },
    {
      "article_id": "5413222526982348809",
      "chunk_id": 3,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "legal protection",
        "Montana",
        "citizens' rights",
        "history"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:56:10.378684"
    },
    {
      "article_id": "5413222526982348809",
      "chunk_id": 4,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "legal protection",
        "Montana",
        "citizens' rights",
        "history"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:56:10.994340"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 0,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:11.812548"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 1,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:12.017030"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 2,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:12.429016"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 3,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:12.733918"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 4,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:13.041225"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 5,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:13.348530"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 6,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:13.655157"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 7,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:13.963827"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 8,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:14.269855"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 9,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:14.680591"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 10,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:15.089181"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 11,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:15.398173"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 12,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:15.703354"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 13,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:16.010780"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 14,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:16.651546"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 15,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:16.933411"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 16,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:17.239273"
    },
    {
      "article_id": "2742989716431158064",
      "chunk_id": 17,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "software",
        "static site generator",
        "modern"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:56:17.650062"
    },
    {
      "article_id": "2686732243144846519",
      "chunk_id": 0,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers",
        "identity"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:56:19.698133"
    },
    {
      "article_id": "2686732243144846519",
      "chunk_id": 1,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers",
        "identity"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:56:19.892334"
    },
    {
      "article_id": "2686732243144846519",
      "chunk_id": 2,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "website",
        "Mark Zuckerberg",
        "community service",
        "Hoosiers",
        "identity"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:56:20.107986"
    },
    {
      "article_id": "7814549832573697797",
      "chunk_id": 0,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "real-time",
        "operating system",
        "UNIX-like",
        "kernel",
        "formally verified"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:56:21.643686"
    },
    {
      "article_id": "7814549832573697797",
      "chunk_id": 1,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "real-time",
        "operating system",
        "UNIX-like",
        "kernel",
        "formally verified"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:56:21.949676"
    },
    {
      "article_id": "7814549832573697797",
      "chunk_id": 2,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "real-time",
        "operating system",
        "UNIX-like",
        "kernel",
        "formally verified"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:56:22.258244"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 0,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:35.067392"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 1,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:36.702337"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 2,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:37.623838"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 3,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:37.932132"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 4,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:38.750341"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 5,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:39.263183"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 6,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:39.568603"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 7,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:39.876217"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 8,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:40.184833"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 9,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:40.592144"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 10,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:40.901778"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 11,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:41.309680"
    },
    {
      "article_id": "7274642597512286300",
      "chunk_id": 12,
      "title": "The Manuscripts of Edsger W. Dijkstra",
      "topics": [
        "indexing",
        "numerical",
        "EWD",
        "BibT"
      ],
      "source": "news",
      "url": "https://www.cs.utexas.edu/~EWD/",
      "added_at": "2025-11-09T15:57:41.617988"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 0,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:44.176072"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 1,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:44.485239"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 2,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:44.857723"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 3,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:45.098095"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 4,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:45.404732"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 5,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:45.712170"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 6,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:46.021357"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 7,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:46.327260"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 8,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:46.634822"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 9,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:47.044325"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 10,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:47.351436"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 11,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:47.760657"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 12,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:48.171126"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 13,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:48.497786"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 14,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:48.888904"
    },
    {
      "article_id": "6597347448727373032",
      "chunk_id": 15,
      "title": "Reviving Classic Unix Games: A 20-Year Journey Through Software Archaeology",
      "topics": [
        "internet history",
        "research",
        "investigation",
        "technology",
        "gaming"
      ],
      "source": "news",
      "url": "https://vejeta.com/reviving-classic-unix-games-a-20-year-journey-through-software-archaeology/",
      "added_at": "2025-11-09T15:57:49.296114"
    },
    {
      "article_id": "7643618233456594432",
      "chunk_id": 0,
      "title": "Visualize FastAPI endpoints with FastAPI-Voyager",
      "topics": [
        "web development",
        "API",
        "FastAPI",
        "data visualization",
        "user interaction"
      ],
      "source": "news",
      "url": "https://www.newsyeah.fun/voyager/",
      "added_at": "2025-11-09T15:57:51.447729"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 0,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:54.802734"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 1,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:55.030816"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 2,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:55.544413"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 3,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:55.850489"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 4,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:56.157211"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 5,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:56.465035"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 6,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:56.771881"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 7,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:57.080643"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 8,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:57.386042"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 9,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:57.695112"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 10,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:58.001169"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 11,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:58.308218"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 12,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:58.514333"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 13,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:58.819276"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 14,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:59.024550"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 15,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:59.333035"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 16,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:59.638948"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 17,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:57:59.910139"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 18,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:00.154422"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 19,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:00.458281"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 20,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:00.766304"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 21,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:01.072993"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 22,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:01.379550"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 23,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:01.686343"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 24,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:01.994383"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 25,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:02.198979"
    },
    {
      "article_id": "1816113217470598566",
      "chunk_id": 26,
      "title": "Email verification protocol",
      "topics": [
        "email verification",
        "control verification",
        "online activity"
      ],
      "source": "news",
      "url": "https://github.com/WICG/email-verification-protocol",
      "added_at": "2025-11-09T15:58:02.505967"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 0,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:04.759119"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 1,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:05.107759"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 2,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:05.475425"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 3,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:05.987806"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 4,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:06.295001"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 5,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:06.602542"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 6,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:06.909843"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 7,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:07.216174"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 8,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:07.421543"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 9,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:07.729657"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 10,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:08.035370"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 11,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:08.254780"
    },
    {
      "article_id": "5523012415423943567",
      "chunk_id": 12,
      "title": "Using bubblewrap to add sandboxing to NetBSD",
      "topics": [
        "programming",
        "Google Summer of Code",
        "sandboxing",
        "NetBSD",
        "bubblewrap"
      ],
      "source": "news",
      "url": "https://blog.netbsd.org/tnf/entry/gsoc2025_bubblewrap_sandboxing",
      "added_at": "2025-11-09T15:58:08.547615"
    },
    {
      "article_id": "1733058420916330971",
      "chunk_id": 0,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:58:10.902686"
    },
    {
      "article_id": "1733058420916330971",
      "chunk_id": 1,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:58:11.212133"
    },
    {
      "article_id": "1733058420916330971",
      "chunk_id": 2,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:58:11.620142"
    },
    {
      "article_id": "1733058420916330971",
      "chunk_id": 3,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:58:11.927883"
    },
    {
      "article_id": "1733058420916330971",
      "chunk_id": 4,
      "title": "Montana Becomes First State to Enshrine 'Right to Compute' into Law",
      "topics": [
        "history",
        "Montana",
        "state law",
        "citizens' rights",
        "legal protection"
      ],
      "source": "news",
      "url": "https://montananewsroom.com/montana-becomes-first-state-to-enshrine-right-to-compute-into-law/",
      "added_at": "2025-11-09T15:58:12.141961"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 0,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:14.181222"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 1,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:14.589043"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 2,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:14.896961"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 3,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:15.204413"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 4,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:15.421834"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 5,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:15.717063"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 6,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:16.023249"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 7,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:16.330389"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 8,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:16.637052"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 9,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:16.945308"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 10,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:17.151767"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 11,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:17.342113"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 12,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:17.610110"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 13,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:17.867800"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 14,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:18.173535"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 15,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:18.584173"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 16,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:18.890014"
    },
    {
      "article_id": "3191832929143411579",
      "chunk_id": 17,
      "title": "Zensical \u2013 A modern static site generator built by the Material for MkDocs team",
      "topics": [
        "static site generator",
        "modern",
        "Zensical",
        "Material for MkDocs",
        "team"
      ],
      "source": "news",
      "url": "https://squidfunk.github.io/mkdocs-material/blog/2025/11/05/zensical/",
      "added_at": "2025-11-09T15:58:19.197815"
    },
    {
      "article_id": "1865792894362204715",
      "chunk_id": 0,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "personal branding",
        "activism",
        "Mark Zuckerberg",
        "website"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:58:21.615850"
    },
    {
      "article_id": "1865792894362204715",
      "chunk_id": 1,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "personal branding",
        "activism",
        "Mark Zuckerberg",
        "website"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:58:21.962190"
    },
    {
      "article_id": "1865792894362204715",
      "chunk_id": 2,
      "title": "I Am Mark Zuckerberg",
      "topics": [
        "personal branding",
        "activism",
        "Mark Zuckerberg",
        "website"
      ],
      "source": "news",
      "url": "https://iammarkzuckerberg.com/",
      "added_at": "2025-11-09T15:58:22.208467"
    },
    {
      "article_id": "2233081092756928513",
      "chunk_id": 0,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "operating system",
        "kernel",
        "real-time",
        "UNIX-like",
        "formal verification"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:58:23.454809"
    },
    {
      "article_id": "2233081092756928513",
      "chunk_id": 1,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "operating system",
        "kernel",
        "real-time",
        "UNIX-like",
        "formal verification"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:58:23.908794"
    },
    {
      "article_id": "2233081092756928513",
      "chunk_id": 2,
      "title": "Ironclad \u2013 formally verified, real-time capable, Unix-like OS kernel",
      "topics": [
        "operating system",
        "kernel",
        "real-time",
        "UNIX-like",
        "formal verification"
      ],
      "source": "news",
      "url": "https://ironclad-os.org/",
      "added_at": "2025-11-09T15:58:24.214953"
    }
  ],
  "last_updated": "2025-11-09T15:58:24.216153"
}