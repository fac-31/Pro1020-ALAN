{
  "documents": [
    "Alan is an AI assistant that helps with email management and provides intelligent responses using RAG technology.",
    "Why Solarpunk is already happening in Africa\n\n\nclimatedrift.substack.com\nWhy Solarpunk is already happening in Africa\nSkander Garroum\n16\u201320 minutes\n\n\ud83d\udc4b Welcome to Climate Drift: your cheat-sheet to climate. Each edition breaks down real solutions, hard numbers, and career moves for operators, founders, and investors who want impact. For more: Community | Accelerator | Open Climate Firesides | Deep Dives\n\nHey there! \ud83d\udc4b\nSkander here.\n\nYou know that feeling when you\u2019re waiting for the cable guy, and they said \u2018between 8am and 6pm, and you waste your entire day, and they never show up?\n\nNow imagine that, except the cable guy is \u2018electricity,\u2019 the day is \u201850 years,\u2019 and you\u2019re one of 600 million people. At some point, you stop waiting and figure it out yourself.\n\nWhat\u2019s happening across Sub-Saharan Africa right now is the most ambitious infrastructure project in human history, except it\u2019s not being built by governments or utilities or World Bank consortiums. It\u2019s being built by startups selling solar panels to farmers on payment plans. And it\u2019s working.\n\nOver 30 million solar products sold in 2024. 400,000 new solar installations every month across Africa. 50% market share captured by companies that didn\u2019t exist 15 years ago. Carbon credits subsidizing the cost. IoT chips in every device. 90%+ repayment rates on loans to people earning $2/day.\n\nAnd if you understand what\u2019s happening in Africa, you understand the template for how infrastructure will get built everywhere else for the next 50 years.\n\nToday we are looking into:\n\n    Why the grid will never come (and why that\u2019s actually good news)\n\n    How it takes three converging miracles (cheap hardware, zero-cost payments, and pay-as-you-go)\n\n    2 case studies on how it works on the ground\n\n    Whether this template works beyond Africa (spoiler: it already is)\n\n\ud83c\udf0a Let\u2019s dive in\n\nHere\u2019s a stat that should make you angry: 600 million people in Sub-Saharan Africa lack reliable electricity. Not because the technology doesn\u2019t exist. Not because they don\u2019t want it. But because the unit economics of grid extension to rural areas are completely, utterly, irredeemably fucked.\n\nThe traditional development playbook goes something like this: Chapter 1, build centralized power generation. Chapter 2, string transmission lines across hundreds of kilometers. Chapter 3, distribute to millions of homes. Chapter 4, collect payments. Chapter 5, maintain the whole thing forever.\n\nThis worked great if you were electrifying America in the 1930s, when labor was cheap, materials were subsidized, and the government could strong-arm right-of-way access. It works less great when you\u2019re trying to reach a farmer four hours from the nearest paved road who earns $600 per year.\n\nLet me show you the math:\n\n    Cost to connect one rural household to the grid: $266 to $2,000\n\n    Average rural household electricity spending: ~$10-20/month\n\n    Payback period: 13-200 months (if you can even collect payments)\n\n    Collection rate in rural areas: complicated\n\nSo utilities do what any rational actor would do: they stop building where the math stops working. Which is exactly where the people are.\n\nThis has been the development sector\u2019s dirty little secret for 50 years. \u201cWe\u2019re working on grid extension!\u201d Translation: we\u2019re not working on grid extension because the economics are impossible, but we need to say we\u2019re working on it so we keep getting donor money.\n\nMeanwhile, 1.5 billion people spend up to 10% of their income on kerosene, diesel, and other dirty fuels. They walk hours to charge their phones. They can\u2019t refrigerate medicine or food. Their kids can\u2019t study after dark. Women inhale cooking smoke equivalent to two packs of cigarettes daily.\n\nWhile everyone was arguing about feed-in tariffs and utility-scale solar, something wild happened to solar costs:\n\nSolar Panel Price History:\n\n    1980: $40/watt\n\n    2000: $5/watt\n\n    2010: $1.50/watt\n\n    2020: $0.30/wlltt\n\n    2025: $0.20/watt\n\nThat\u2019s a 99.5% decline in 45 years. Moore\u2019s Law except for sunshine.\n\n    Want to learn how solar got cheap? \n\nBut here\u2019s what\u2019s even crazier: the price of complete solar home systems:\n\nSolar Home System Evolution:\n\n    2008: $5,000 (affordable only for wealthy urban Kenyans)\n\n    2015: $800 (middle-class farmers)\n\n    2025: $120-$1,200 (true smallholders)\n\nBattery costs also collapsed 90%. Inverters got cheap. LED bulbs got efficient. Manufacturing in China got insanely good. Logistics in Africa got insanely better.\n\nAll of these trends converged around 2018-2020, and suddenly the economics of off-grid solar just... flipped. The hardware became a solved problem.\n\nBut there was still a massive, seemingly insurmountable barrier: $120 upfront might as well be $1 million when you earn $2/day.\n\nThis is where the story gets interesting.\n\nQuick history lesson: In 2007, Safaricom (Kenya\u2019s telco) launched M-PESA, a mobile money platform that let people transfer cash via SMS.\n\nEveryone thought it would fail. Why would anyone use their phone to send money?\n\nBy 2025: 70% of Kenyans use mobile money. Not in addition to banks. Instead of banks. Kenya processes more mobile money transactions per capita than any country on Earth.\n\nIt worked because it solved a real problem: Kenyans were already sending money through informal networks. M-PESA just made it cheaper and safer.\n\nHere\u2019s why this matters: M-PESA created a payment rail with near-zero transaction costs. Which means you can economically collect tiny payments. $0.21 per day payments.\n\nThis broke open a financing model that changes everything: Pay-As-You-Go.\n\nThis is the unlock. This is the thing that makes everything else possible.\n\nHere\u2019s the model:\n\n    A company (Sun King, SunCulture) installs a solar system in your home\n\n    You pay ~$100 down\n\n    Then $40-65/month over 24-30 months\n\n    The system has a GSM chip that calls home\n\n    No payment = remotely shut off\n\n    Keep paying = keep power\n\n    After 30 months = you own it, free power forever\n\nThe magic is this: You\u2019re not buying a $1,200 solar system. You\u2019re replacing $3-5/week kerosene spending with a $0.21/day solar subscription (so with $1.5 per week half the price of kerosene) that\u2019s cheaper AND gives you better light, phone charging, radio, and no respiratory disease.\n\nThe default rate? 90%+ of customers repay on time.\n\nWhy? Because the asset actually works. It delivers value every single day. The alternative is going back to kerosene lamps in the dark. Nobody wants that.\n\nThis is the \u201cinnovation\u201d that everyone missed. The hardware got cheap, but PAYG made it accessible. And mobile money made PAYG economically viable.\n\nNow let\u2019s talk about what happens when you combine these three things with 2 case studies.\n\n23 million solar products sold in 2023, serving 40 million customers in 42 countries, and targeting 50 million units by 2026.\nTheir product range spans from handheld solar lamps to multi-room home solar kits and clean LPG stoves\n\nProducts:\n\n    Handheld solar lamps ($50-120)\n\n    Multi-room home systems ($200-500)\n\n    LPG clean cookstoves (acquired PayGo Energy)\n\n    Phone charging, battery backup, lighting\n\n    Want to dive deeper? I got a casestudy for you\n\nEach turn of the wheel makes the next turn easier. This is a compounding moat.\n\nAnd here\u2019s what nobody outside Africa understands: Sun King has 50%+ market share in their category. They\u2019re not scrappy startup. They\u2019re a dominant infrastructure provider.\n\nThis would be like if one startup owned 50% of U.S. home solar. Except the impact and the TAM is bigger because there\u2019s no incumbent grid to compete with.\n\nIf Sun King is the lighting/household electrification play, SunCulture is the agriculture productivity play. And the numbers are even more insane.\n\nThe Problem:\n\n    95% of Sub-Saharan Africa\u2019s cropland is rain-dependent\n\n    Farmers spend $2B annually on diesel pumps\n\nThe SunCulture Solution:\n\n    Solar-powered irrigation pumps\n\n    IoT-enabled remote monitoring\n\n    PAYG financing ($100 down, $40-65/month)\n\n    Free installation, 10-year warranty\n\n    Drip irrigation included\n\nThe Results:\n\n    Crop yields increase 3-5\u00d7\n\n    Farmers go from $600/acre to $14,000/acre revenue\n\n    Zero marginal cost after payoff (no diesel!)\n\n    Year-round irrigation instead of seasonal\n\n    17 hours/week saved from manual water hauling\n\nThe Scale:\n\n    47,000+ systems deployed\n\n    40,000+ farmers served\n\n    50%+ market share in smallholder segment\n\n    6 countries (Kenya, Uganda, Ethiopia, Ivory Coast, Zambia, Togo)\n\nThat\u2019s not a charity. That\u2019s a fucking rocketship.\n\nOkay, this is where it gets really spicy.\n\nRemember that SunCulture solar pump displacing diesel? That\u2019s 2.9 tons of CO2 avoided per year. Per pump.\n\nMultiply by 47,000 pumps = 136,000 tons CO2/year. Over seven years = 3+ million tons cumulative.\n\n    Want to dive deeper? I got another casestudy for you\n\nNow here\u2019s the hack: Someone will pay for that.\n\nEnter carbon credits. SunCulture is the first African solar irrigation company with Verra-registered carbon credits. Each ton of avoided CO2 can be sold for $15-30 (high-quality agricultural credits, not sketchy forest offsets).\n\nLet\u2019s do the flywheel again, but this time turbocharged with carbon credits.\n\n    Install solar system\n\n    System displaces diesel (verified via IoT telemetry)\n\n    Displacement = carbon credits issued\n\n    Sell credits to companies needing offsets\n\n    Carbon revenue subsidizes upfront cost by 25-40%\n\n    Lower cost = 4-5\u00d7 larger addressable market\n\n    More systems deployed = more carbon credits\n\n    Repeat\n\nIt gets even better: there are people who will pay for credits beforehand.\nBritish International Investment (UK\u2019s DFI) pioneered this with SunCulture: they provided $6.6M in \u201ccarbon-backed equipment financing.\u201d They bear the carbon price risk, SunCulture gets upfront capital, farmers get 25-40% cheaper pumps.\n\nThis is how it should be: The climate impact that was an externality is now a revenue stream. The global North\u2019s carbon problem subsidizes the global South\u2019s energy access.\n\n    A quick note on MRV\n    Okay, so you might know I have\u2026 issues with the carbon credit world, especially MRV(monitoring, reporting, verification). Here monitoring is IoT-based, the MRV costs are near-zero. No expensive field audits. The telemetry data proves the pump is running = proves diesel is displaced = proves carbon is avoided.\n\nThe carbon credit mechanism turns climate infrastructure into an asset class. Which means you can finance it at scale.\n\n    Btw this is how the largest forest of the US is now being financed:\n\n    Chestnut Carbon buys degraded farmland across the Southeast, replants biodiverse native forests, verifies long-term carbon removal, and signs long-dated offtake deals with blue-chip buyers like Microsoft. The company has acquired more than 35,000 acres, planted over 17 million trees, and aims to restore 100,000+ acres by 2030 with an expected 100 million tons of CO2 removed over 50 years.\n\n    Learn more here: \n\nSo: what now?\n\nWhy is the market concentrated? Because the full-stack is really fucking hard.\n\nYou need:\n\n    Hardware manufacturing expertise\n\n    Supply chain across fragmented markets\n\n    Last-mile distribution (29,500 agents for Sun King)\n\n    Mobile money integration\n\n    Credit scoring models for the unbanked\n\n    IoT/telemetry systems\n\n    Customer service in 10+ languages\n\n    Financing (equity, debt, securitization)\n\n    Carbon market relationships\n\n    Regulatory navigation across 40+ countries\n\nMost companies can do 2-3 of these. The winners do all 10.\n\nThis creates massive barriers to entry and long-term moats. New entrants can\u2019t just show up with cheaper panels. The moat is the full-stack execution.\n\n\u200b\u200bLet\u2019s do the math on how big this can get.\n\n    600M people without reliable power in Sub-Saharan Africa\n\n    570M smallholder farming households in Africa\n\n    900M people in Africa use traditional cookstoves\n\nAnd that\u2019s just Africa. Add Asia (1 billion without electricity) and you\u2019re north of $300B-$500B.\n\nBut here\u2019s the thing: this massively understates the opportunity.\n\nThe solar system is the Trojan horse. The real business is the financial relationship with 40 million customers.\n\nBecause what you\u2019re really doing is creating a digital infrastructure layer that enables:\n\n    Consumer lending (smartphones, motorcycles, appliances)\n\n    Livestock/agriculture financing\n\n    Insurance products\n\n    Healthcare delivery\n\n    Education services\n\n    Payment processing\n\nSo the actual TAM? It\u2019s whatever the total consumer spending is for 600M people rising into the middle class.\n\nOkay, let\u2019s zoom out. What happens when 100M+ people get electrified through this model?\n\n    Kids study at night \u2192 higher test scores \u2192 better jobs\n\n    Adults work after dark \u2192 higher income\n\n    Farmers irrigate year-round \u2192 3-5x yields \u2192 food security\n\n    Phone charging \u2192 mobile money access \u2192 financial inclusion\n\n    Refrigeration \u2192 vaccine storage \u2192 disease prevention\n\n    Refrigeration \u2192 Keep milk/meat eatable \u2192 reduced food waste No kerosene smoke \u2192 respiratory disease drops\n\n    Clean cookstoves \u2192 600,000 fewer deaths/year from indoor pollution\n\n    Diesel displacement = cleaner air quality\n\nBut here\u2019s the meta-point: This is the template for building infrastructure in the 21st century.\n\nNot government-led. Not centralized. Not requiring 30-year megaprojects.\n\nInstead: modular, distributed, digitally-metered, remotely-monitored, PAYG-financed, carbon-subsidized infrastructure deployed by private companies in competitive markets.\n\nThe 20th century infrastructure model was:\n\n    Centralized generation\n\n    Government-led\n\n    Megaproject financing\n\n    30-year timelines\n\n    Monopolistic utilities\n\nThe 21st century infrastructure model is:\n\n    Distributed/modular\n\n    Private sector-led\n\n    PAYG financing\n\n    Deploy in days/weeks\n\n    Competitive markets\n\nThis is how things will get built going forward.\n\nSo what could go wrong?\n\nLet\u2019s start by making clear this is not a one size fits all solution:\nPAYG solar works for households and smallholders. Doesn\u2019t work for factories or heavy industry. This isn\u2019t a complete grid replacement.\n\n1. FX Risk Companies raise dollars, buy hardware in dollars, collect revenue in Naira/Shillings. Currency crashes can blow up unit economics overnight.\n\n2. Political/Regulatory Risk\nGovernments could impose lending restrictions, tariffs on solar imports, or subsidize grid/diesel to protect state utilities.\n\n3. Default Risk\n10% default rate is good but fragile. Economic shocks, droughts, or political instability could spike defaults.\n\n4. Maintenance Complexity\nPanels last 25 years, batteries 5 years, pumps break. Building service networks across rural Africa is expensive.\n\n5. Carbon Price Volatility\nCarbon credits crashed from $30/ton to $5/ton in 2024. If 25-40% of affordability comes from carbon revenue, price swings hurt.\n\n6. Competition from Grid\nWhat if governments actually build the grid? (Unlikely given economics, but possible with enough subsidy)\n\n7. Supply Chain Bottlenecks\n\nPort congestion, customs delays, tariff swings, China export controls, and last-mile logistics can delay installs, raise COGS, and tie up working capital.\n\n    Fun fact: Sun King is now producing their devices in Africa, cutting $300 Million in imports over the next years.\n\nOkay, the bear case is important. But let\u2019s talk about the scenarios where this doesn\u2019t just work: it goes \ud83c\udfd2.\n\nSolar panels dropped 99.5% in 45 years. What if we\u2019re only halfway through?\n\nCurrent situation:\n\n    China has 600+ GW of solar manufacturing capacity\n\n    Current global demand: ~400 GW/year\n\n    Overcapacity = price collapse incoming\n\nWhat happens next:\n\n    Solar: $0.20/watt \u2192 $0.10/watt by 2030\n\n    Batteries: Another 50% drop as sodium-ion scales\n\n    Complete solar home systems: $120-1,200 \u2192 $60-600\n\nA $60 entry-level system puts the addressable market at 2 billion people instead of 600 million. You\u2019re not just electrifying rural Africa. You\u2019re electrifying rural India, Bangladesh, Pakistan, Southeast Asia, Latin America.\n\nRight now, these companies finance at 12-18% interest rates. What if Development Finance Institutions (DFIs) actually do their job?\n\nThe scenario:\n\n    World Bank, IFC, British International Investment create dedicated facilities\n\n    \u201cDe-risk\u201d lending to proven operators like Sun King/SunCulture\n\n    Cost of capital drops from 15% \u2192 5-7%\n\nWhat this unlocks:\n\n    Monthly payments drop 30-40%\n\n    Addressable market expands by 200M+ people\n\n    Payback periods shrink from 30 months \u2192 18-24 months\n\n    Companies can deploy 3-5x faster with better unit economics\n\nThis is literally what happened with microfinance when Grameen Bank proved the model. Billions in cheap capital followed.\n\nHere\u2019s what nobody\u2019s pricing in: social proof at scale.\n\nThe flywheel:\n\n    Village A: 3 households get solar\n\n    Neighbors see: kids studying at night, no kerosene smell, phone always charged\n\n    Village A: 30 households get solar within 12 months\n\n    Next village over hears about it \u2192 sales agent swamped\n\n    Company expands distribution network to meet demand\n\nWhat the data shows:\n\n    Sun King\u2019s customer acquisition cost has dropped 60% since 2018\n\n    Why? Word of mouth. Referrals. \u201cMy cousin has one.\u201d\n\n    In mature markets (Kenya), 40%+ of sales come from referrals\n\nWhen 20-30% of a region has solar, it becomes the default. You\u2019re not an early adopter, you\u2019re behind. This is how mobile phones scaled in Africa. The tipping point creates exponential adoption curves.\n\nThe grid that never came turned out to be a blessing. While development experts spent 50 years debating how to extend 20th-century infrastructure to rural Africa, something more interesting happened: Africa built the 21st-century version instead.\n\nModular. Distributed. Digital. Financed by the people using it, subsidized by the carbon it avoids.\n\nThe solarpunk future isn\u2019t speculative fiction. It\u2019s 23 million solar systems, 40 million people, and a template for how infrastructure gets built when you\u2019re not stuck defending the past.\n\nThanks to Jarek Dmowski for first spotlighting the companies in our monthly Follow the Money and for his perspectives, and to Aaron Kruse for the conversations that shaped this essay.\n",
    "The state of SIMD in Rust in 2025\n\nThe state of SIMD in Rust in 2025\nSergey \"Shnatsel\" Davidoff\nSergey \"Shnatsel\" Davidoff\n7 min read\n\u00b7\n7 hours ago\n\nIf you\u2019re already familiar with SIMD, the table below is all you need.\n\nAnd if you\u2019re not, you will understand the table by the end of this article!\nPress enter or click to view image in full size\nWhat\u2019s SIMD? Why SIMD?\n\nHardware that does arithmetic is cheap, so any CPU made this century has plenty of it. But you still only have one instruction decoding block and it is hard to get it to go fast, so the arithmetic hardware is vastly underutilized.\n\nTo get around the instruction decoding bottleneck, you can feed the CPU a batch of numbers all at once for a single arithmetic operation like addition. Hence the name: \u201csingle instruction, multiple data,\u201d or SIMD for short.\n\nInstead of adding two numbers together, you can add two batches or \u201cvectors\u201d of numbers and it takes about the same amount of time.\n\nOn recent x86 chips these batches can be up to 512 bits in size, so in theory you can get an 8x speedup for math on u64 or a 64x speedup on u8!\nInstruction sets\n\nHistorically, SIMD instructions were added after the CPU architecture was already designed, so SIMD is an extension with its own marketing name on each architecture.\n\nARM calls theirs \u201cNEON\u201d, and all 64-bit ARM CPUs have it.\n\nWebAssembly doesn\u2019t have a marketing department, so they just call theirs \u201cWebAssembly 128-bit packed SIMD extension\u201d.\n\n64-bit x86 shipped with one called \u201cSSE2\u201d which has basic instructions for 128-bit vectors, but later they added a whole menagerie of extensions on top of that, with SSE 4.2 adding more operations, AVX and AVX2 adding 256-bit vectors and AVX-512 adding 512-bit vectors.\n\nThe word \u201clater\u201d in the above paragraph creates a problem.\nDoes this CPU have that instruction?\n\nIf you\u2019re running a program on an x86 CPU, it\u2019s not a given that the CPU has any particular SIMD extension. So by default the compiler isn\u2019t allowed to use instructions beyond SSE2 because that won\u2019t work on all x86 CPUs.\n\nThere are two ways around this problem.\n\nIf you work for a company that only ever runs their binaries on their own servers or on a public cloud, you can just assert that they\u2019re all recent enough to at least have AVX2 that was introduced over 10 years ago, and have the program crash or misbehave if it ever runs on anything without AVX2:\n\nRUSTFLAGS='-C target-cpu=x86\u201364-v3' cargo build --release\n\nHowever, if you are distributing the binaries for other people to run, that\u2019s not really an option.\n\nInstead you can do something called function multiversioning: compile the same function multiple times for different SIMD extensions, and when the program actually runs, check what features the CPU supports and select the appropriate version based on that.\n\nFortunately, this problem only exists on x86.\n\nARM made its NEON mandatory in all 64-bit CPUs and then didn\u2019t bother expanding the width beyond 128 bits. (Technically SVE exists, but in 2025 it is still mostly on paper, and Rust support for it is still in progress).\n\nWebAssembly makes you compile two different binaries, one with SIMD and one without, and use JavaScript to check if the browser supports SIMD.\nSolution space\n\nThere are four approaches to SIMD in Rust, in ascending order of effort:\n\n    Automatic vectorization\n    Fancy iterators\n    Portable SIMD abstractions\n    Raw intrinsics\n\nAutomatic vectorization\n\nThe easiest approach to SIMD is letting the compiler do it for you.\nBecome a member\n\nIt works surprisingly well, as long as you structure your code in a way that is amenable to vectorization. This article covers it:\nCan You Trust a Compiler to Optimize Your Code?\nMore or less the title this time, but first, a story about SIMD. There are three levels of understanding how SIMD works\u2026\n\nmatklad.github.io\n\nYou can check if it\u2019s working with cargo-show-asm or godbolt.org, but your benchmarks are the ultimate judge of the results.\n\nSadly there is a limit on the complexity of the code that the compiler will vectorize, and it may change between compiler versions. If something vectorizes today that doesn\u2019t necessarily mean it still will in a year from now.\n\nThe other drawback of this method is that the optimizer won\u2019t even touch anything involving floats (f32 and f64 types). It\u2019s not permitted to change any observable outputs of the program, and reordering float operations may alter the result due to precision loss. (There is a way to tell the compiler not to worry about precision loss, but it\u2019s currently nightly-only).\n\nSo right now, if you need to process floats, autovectorization is a no-go unless you can use nightly builds of the Rust compiler.\n\n(Floats are cursed even without SIMD. Something as simple as summing an array of them in a usable way turns out to be really hard).\n\nThere is no built-in way to multiversion functions, but the multiversion crate works great with autovectorization.\nFancy iterators\n\nJust like rayon lets you run your iterators in parallel by swapping .iter() with .par_iter(), there have been attempts to do the same for SIMD. After all, what is SIMD but another kind of parallelism?\n\nThis is the approach that the faster crate takes. That crate has been abandoned for years, and it doesn\u2019t look like this approach has panned out.\nPortable SIMD abstractions\n\nThe idea is to let you write your algorithm by explicitly operating on chunks of data, something like [f32; 8] but wrapped in a custom type, and then provide custom implementations of operations like + that compile down into SIMD instructions.\n\nstd::simd is exactly that. It supports all instruction sets LLVM supports, so its platform support is unparalleled. It pairs well with the multiversion crate. Sadly it\u2019s nightly-only and will remain such for the foreseeable future, so it\u2019s unusable in most situations.\n\nThe wide crate is a mature, established option. It supports NEON, WASM and all the x86 instruction sets. But it doesn\u2019t support multiversioning at all, save for very exotic and limited approaches like cargo-multivers.\n\nThe pulp crate is a great design with built-in multiversioning, and is quite mature and complete. It powers faer, so its performance is clearly proven. The drawbacks are that it doesn\u2019t support WASM, and that on x86 it only supports targeting AVX2 and AVX-512 but not the older extensions. But AVX2 was introduced in 2012 and in the Steam hardware survey 95% systems have it, so that might not be a big deal.\n\nThe macerator crate is a fork of pulp with vastly expanded instruction set support. It supports all x86 extensions, WASM, NEON, and even the LoongArch SIMD extensions. It\u2019s used only by burn-ndarray, and even there it\u2019s an optional dependency. It sounds great on paper, but it\u2019s oddly obscure and therefore unproven. I\u2019d probably write code using pulp, then replace it with macerator and see if everything still works and runs as fast as it should.\n\nThe fearless_simd crate is largely a copy of pulp\u2019s design made for use in vello. It\u2019s far less mature than pulp, but it\u2019s under active development. As of this writing it supports NEON, WASM and SSE4.2, but not the newer x86 extensions. Seems too immature just yet, but something to keep an eye on.\n\nsimdeez is a rather old crate that supports all instruction sets except AVX-512 and comes with built-in multiversioning. What gives me pause is that despite existing for many years, it\u2019s still barely used. Everyone else who needed SIMD built their own instead of using it. And its README says:\n\n    Currently things are well fleshed out for i32, i64, f32, and f64 types.\n\nSo I guess the other types aren\u2019t complete?\n\nTL;DR: use std::simd if you don\u2019t mind nightly, wide if you don\u2019t need multiversioning, and otherwise pulp or macerator.\n\nIf it\u2019s not 2025 when you\u2019re reading this, check out fearless_simd, because std::simd is still in nightly in your glorious future, isn\u2019t it?\nRaw intrinsics\n\nIf you want to get really close to the metal, there are always the raw intrinsics, just one step removed from the processor instructions.\n\nThe problem looming over any use of raw intrinsics is that you have to manually write them for every platform and instruction set you\u2019re targeting. Whereas std::simd or wide let you write your logic once and compile it down to the assembly automatically, with intrinsics you have to write a separate implementation for every single platform and instruction set (SSE, AVX, NEON\u2026) you care to support. That\u2019s a lot of code!\n\nIt\u2019s really not helped by the fact that they are all named something like _mm256_srli_epi32 and your code ends up as a long list of calls to these arcanely named functions. And wrappers that help readability introduce their own problems, such as clashes with multiversioning or unsafe code or arcane macros.\n\nYou also have to build your own multiversioning. Or rather, you have to manually dispatch to the dedicated implementation you have manually written for each instruction set. std::is_x86_feature_detected! macro takes care of the feature detection, but it is somewhat slow. In some cases it is beneficial to detect available features exactly once and then cache the results, but you have to implement that manually too.\n\nOn the bright side, this year writing intrinsics got markedly less awful. Most of them are no longer unsafe to call in Rust 1.86 and later, and the safe_unaligned_simd crate provides safe wrappers for the rest.\n\nSo at least this approach is no longer unsafe on top of all the other problems it has!\nWhich one is right for you?\n\nThe right tool for the job ultimately depends on the use case.\n\nWant zero dependencies and little up-front hassle? Autovectorization. Porting existing C code or targeting very specific hardware? Intrinsics. Anything else? Portable SIMD abstraction.\n\nAnd now that you made it this far, you can understand the table at the top of the article, which will help guide your decision!",
    "The state of SIMD in Rust in 2025\n\nWhat\u2019s SIMD? Why SIMD?\n\nHardware that does arithmetic is cheap, so any CPU made this century has plenty of it. But you still only have one instruction decoding block and it is hard to get it to go fast, so the arithmetic hardware is vastly underutilized.\n\nTo get around the instruction decoding bottleneck, you can feed the CPU a batch of numbers all at once for a single arithmetic operation like addition. Hence the name: \u201csingle instruction, multiple data,\u201d or SIMD for short.\n\nInstead of adding two numbers together, you can add two batches or \u201cvectors\u201d of numbers and it takes about the same amount of time.\n\nOn recent x86 chips these batches can be up to 512 bits in size, so in theory you can get an 8x speedup for math on u64 or a 64x speedup on u8!\nInstruction sets\n\nHistorically, SIMD instructions were added after the CPU architecture was already designed, so SIMD is an extension with its own marketing name on each architecture.\n\nARM calls theirs \u201cNEON\u201d, and all 64-bit ARM CPUs have it.\n\nWebAssembly doesn\u2019t have a marketing department, so they just call theirs \u201cWebAssembly 128-bit packed SIMD extension\u201d.\n\n64-bit x86 shipped with one called \u201cSSE2\u201d which has basic instructions for 128-bit vectors, but later they added a whole menagerie of extensions on top of that, with SSE 4.2 adding more operations, AVX and AVX2 adding 256-bit vectors and AVX-512 adding 512-bit vectors.\n\nThe word \u201clater\u201d in the above paragraph creates a problem.\nDoes this CPU have that instruction?\n\nIf you\u2019re running a program on an x86 CPU, it\u2019s not a given that the CPU has any particular SIMD extension. So by default the compiler isn\u2019t allowed to use instructions beyond SSE2 because that won\u2019t work on all x86 CPUs.\n\nThere are two ways around this problem.\n\nIf you work for a company that only ever runs their binaries on their own servers or on a public cloud, you can just assert that they\u2019re all recent enough to at least have AVX2 that was introduced over 10 years ago, and have the program crash or misbehave if it ever runs on anything without AVX2:\n\nRUSTFLAGS='-C target-cpu=x86\u201364-v3' cargo build --release\n\nHowever, if you are distributing the binaries for other people to run, that\u2019s not really an option.\n\nInstead you can do something called function multiversioning: compile the same function multiple times for different SIMD extensions, and when the program actually runs, check what features the CPU supports and select the appropriate version based on that.\n\nFortunately, this problem only exists on x86.\n\nARM made its NEON mandatory in all 64-bit CPUs and then didn\u2019t bother expanding the width beyond 128 bits. (Technically SVE exists, but in 2025 it is still mostly on paper, and Rust support for it is still in progress).\n\nWebAssembly makes you compile two different binaries, one with SIMD and one without, and use JavaScript to check if the browser supports SIMD.\nSolution space\n\nThere are four approaches to SIMD in Rust, in ascending order of effort:\n\n    Automatic vectorization\n    Fancy iterators\n    Portable SIMD abstractions\n    Raw intrinsics\n\nAutomatic vectorization\n\nThe easiest approach to SIMD is letting the compiler do it for you.\nBecome a member\n\nIt works surprisingly well, as long as you structure your code in a way that is amenable to vectorization. This article covers it:\nCan You Trust a Compiler to Optimize Your Code?\nMore or less the title this time, but first, a story about SIMD. There are three levels of understanding how SIMD works\u2026\n\nmatklad.github.io\n\nYou can check if it\u2019s working with cargo-show-asm or godbolt.org, but your benchmarks are the ultimate judge of the results.\n\nSadly there is a limit on the complexity of the code that the compiler will vectorize, and it may change between compiler versions. If something vectorizes today that doesn\u2019t necessarily mean it still will in a year from now.\n\nThe other drawback of this method is that the optimizer won\u2019t even touch anything involving floats (f32 and f64 types). It\u2019s not permitted to change any observable outputs of the program, and reordering float operations may alter the result due to precision loss. (There is a way to tell the compiler not to worry about precision loss, but it\u2019s currently nightly-only).\n\nSo right now, if you need to process floats, autovectorization is a no-go unless you can use nightly builds of the Rust compiler.\n\n(Floats are cursed even without SIMD. Something as simple as summing an array of them in a usable way turns out to be really hard).\n\nThere is no built-in way to multiversion functions, but the multiversion crate works great with autovectorization.\nFancy iterators\n\nJust like rayon lets you run your iterators in parallel by swapping .iter() with .par_iter(), there have been attempts to do the same for SIMD. After all, what is SIMD but another kind of parallelism?\n\nThis is the approach that the faster crate takes. That crate has been abandoned for years, and it doesn\u2019t look like this approach has panned out.\nPortable SIMD abstractions\n\nThe idea is to let you write your algorithm by explicitly operating on chunks of data, something like [f32; 8] but wrapped in a custom type, and then provide custom implementations of operations like + that compile down into SIMD instructions.\n\nstd::simd is exactly that. It supports all instruction sets LLVM supports, so its platform support is unparalleled. It pairs well with the multiversion crate. Sadly it\u2019s nightly-only and will remain such for the foreseeable future, so it\u2019s unusable in most situations.\n\nThe wide crate is a mature, established option. It supports NEON, WASM and all the x86 instruction sets. But it doesn\u2019t support multiversioning at all, save for very exotic and limited approaches like cargo-multivers.\n\nThe pulp crate is a great design with built-in multiversioning, and is quite mature and complete. It powers faer, so its performance is clearly proven. The drawbacks are that it doesn\u2019t support WASM, and that on x86 it only supports targeting AVX2 and AVX-512 but not the older extensions. But AVX2 was introduced in 2012 and in the Steam hardware survey 95% systems have it, so that might not be a big deal.\n\nThe macerator crate is a fork of pulp with vastly expanded instruction set support. It supports all x86 extensions, WASM, NEON, and even the LoongArch SIMD extensions. It\u2019s used only by burn-ndarray, and even there it\u2019s an optional dependency. It sounds great on paper, but it\u2019s oddly obscure and therefore unproven. I\u2019d probably write code using pulp, then replace it with macerator and see if everything still works and runs as fast as it should.\n\nThe fearless_simd crate is largely a copy of pulp\u2019s design made for use in vello. It\u2019s far less mature than pulp, but it\u2019s under active development. As of this writing it supports NEON, WASM and SSE4.2, but not the newer x86 extensions. Seems too immature just yet, but something to keep an eye on.\n\nsimdeez is a rather old crate that supports all instruction sets except AVX-512 and comes with built-in multiversioning. What gives me pause is that despite existing for many years, it\u2019s still barely used. Everyone else who needed SIMD built their own instead of using it. And its README says:\n\n    Currently things are well fleshed out for i32, i64, f32, and f64 types.\n\nSo I guess the other types aren\u2019t complete?\n\nTL;DR: use std::simd if you don\u2019t mind nightly, wide if you don\u2019t need multiversioning, and otherwise pulp or macerator.\n\nIf it\u2019s not 2025 when you\u2019re reading this, check out fearless_simd, because std::simd is still in nightly in your glorious future, isn\u2019t it?\nRaw intrinsics\n\nIf you want to get really close to the metal, there are always the raw intrinsics, just one step removed from the processor instructions.\n\nThe problem looming over any use of raw intrinsics is that you have to manually write them for every platform and instruction set you\u2019re targeting. Whereas std::simd or wide let you write your logic once and compile it down to the assembly automatically, with intrinsics you have to write a separate implementation for every single platform and instruction set (SSE, AVX, NEON\u2026) you care to support. That\u2019s a lot of code!\n\nIt\u2019s really not helped by the fact that they are all named something like _mm256_srli_epi32 and your code ends up as a long list of calls to these arcanely named functions. And wrappers that help readability introduce their own problems, such as clashes with multiversioning or unsafe code or arcane macros.\n\nYou also have to build your own multiversioning. Or rather, you have to manually dispatch to the dedicated implementation you have manually written for each instruction set. std::is_x86_feature_detected! macro takes care of the feature detection, but it is somewhat slow. In some cases it is beneficial to detect available features exactly once and then cache the results, but you have to implement that manually too.\n\nOn the bright side, this year writing intrinsics got markedly less awful. Most of them are no longer unsafe to call in Rust 1.86 and later, and the safe_unaligned_simd crate provides safe wrappers for the rest.\n\nSo at least this approach is no longer unsafe on top of all the other problems it has!\nWhich one is right for you?\n\nThe right tool for the job ultimately depends on the use case.\n\nWant zero dependencies and little up-front hassle? Autovectorization. Porting existing C code or targeting very specific hardware? Intrinsics. Anything else? Portable SIMD abstraction.\n\nAnd now that you made it this far, you can understand the table at the top of the article, which will help guide your decision!"
  ],
  "metadata": [
    {
      "type": "test_document",
      "title": "Alan AI Assistant",
      "topics": [
        "ai",
        "email",
        "assistant"
      ],
      "doc_id": "test_doc_1",
      "added_at": "2025-10-26T07:58:46.162201"
    },
    {
      "title": "Why Solarpunk is already happening in Africa",
      "topics": [
        "Technology"
      ],
      "source": "news",
      "url": "https://climatedrift.substack.com/p/why-solarpunk-is-already-happening",
      "added_at": "2025-11-05T20:50:38.700902"
    },
    {
      "title": "The state of SIMD in Rust in 2025",
      "topics": [
        ""
      ],
      "source": "news",
      "url": "https://shnatsel.medium.com/the-state-of-simd-in-rust-in-2025-32c263e5f53d",
      "added_at": "2025-11-05T22:41:04.431158"
    },
    {
      "title": "The state of SIMD in Rust in 2025",
      "topics": [
        "Technology",
        "SIMD"
      ],
      "source": "news",
      "url": "https://shnatsel.medium.com/the-state-of-simd-in-rust-in-2025-32c263e5f53d",
      "added_at": "2025-11-05T22:42:35.890065"
    }
  ],
  "last_updated": "2025-11-05T22:42:35.890566"
}